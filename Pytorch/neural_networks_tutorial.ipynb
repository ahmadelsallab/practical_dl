{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_networks_tutorial.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"gRBc642MH0WB","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U9-QIvsYH0WI","colab_type":"text"},"source":["\n","Neural Networks\n","===============\n","\n","Neural networks can be constructed using the ``torch.nn`` package.\n","\n","Now that you had a glimpse of ``autograd``, ``nn`` depends on\n","``autograd`` to define models and differentiate them.\n","An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n","returns the ``output``.\n","\n","For example, look at this network that classifies digit images:\n","\n",".. figure:: /_static/img/mnist.png\n","   :alt: convnet\n","\n","   convnet\n","\n","It is a simple feed-forward network. It takes the input, feeds it\n","through several layers one after the other, and then finally gives the\n","output.\n","\n","A typical training procedure for a neural network is as follows:\n","\n","- Define the neural network that has some learnable parameters (or\n","  weights)\n","- Iterate over a dataset of inputs\n","- Process input through the network\n","- Compute the loss (how far is the output from being correct)\n","- Propagate gradients back into the network’s parameters\n","- Update the weights of the network, typically using a simple update rule:\n","  ``weight = weight - learning_rate * gradient``\n","\n","Define the network\n","------------------\n","\n","Let’s define this network:\n","\n"]},{"cell_type":"code","metadata":{"id":"uwtF1HUUH0WK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"6c4c9992-6c7b-4d12-b68e-4ceb6bd87fad","executionInfo":{"status":"ok","timestamp":1557146078053,"user_tz":-120,"elapsed":939,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 5x5 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = Net()\n","print(net)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zVjrUfP_H0WN","colab_type":"text"},"source":["You just have to define the ``forward`` function, and the ``backward``\n","function (where gradients are computed) is automatically defined for you\n","using ``autograd``.\n","You can use any of the Tensor operations in the ``forward`` function.\n","\n","The learnable parameters of a model are returned by ``net.parameters()``\n","\n"]},{"cell_type":"code","metadata":{"id":"iDHy5ex6H0WP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":7673},"outputId":"7355c796-9780-471e-eb62-ffef5ca3e0c1","executionInfo":{"status":"ok","timestamp":1557146205045,"user_tz":-120,"elapsed":618,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1's .weight\n","params# gives the initial values of the weights"],"execution_count":4,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([6, 1, 5, 5])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[[[-0.1740, -0.0137,  0.1140, -0.1305,  0.1641],\n","           [-0.0564,  0.0959,  0.0843,  0.0112,  0.1026],\n","           [-0.1050,  0.0833, -0.1200,  0.1481, -0.1244],\n","           [-0.1125, -0.1012, -0.1872, -0.1182,  0.1776],\n","           [-0.0109,  0.0485,  0.1050, -0.0730,  0.1302]]],\n"," \n"," \n","         [[[-0.1112,  0.1512,  0.0814, -0.1995, -0.1323],\n","           [-0.0268,  0.1997,  0.0528, -0.0683, -0.1188],\n","           [ 0.0863,  0.1408, -0.0982, -0.1977,  0.1099],\n","           [-0.1618,  0.0540,  0.0524,  0.0435, -0.0618],\n","           [-0.0434, -0.1017,  0.1208,  0.0491,  0.1111]]],\n"," \n"," \n","         [[[ 0.0200,  0.1566, -0.1615,  0.1747,  0.1145],\n","           [-0.0849, -0.1709, -0.1497, -0.1773,  0.0272],\n","           [ 0.0920, -0.0302, -0.1648,  0.0460,  0.0138],\n","           [-0.0711, -0.1076, -0.1015,  0.1658,  0.0714],\n","           [ 0.1634,  0.1920, -0.0836, -0.1642,  0.1279]]],\n"," \n"," \n","         [[[-0.1614, -0.0141, -0.1458,  0.1920, -0.0514],\n","           [ 0.1668, -0.1447, -0.1180, -0.0407, -0.1266],\n","           [-0.1090,  0.0680, -0.0218,  0.0504,  0.1695],\n","           [-0.1183, -0.0303,  0.1028, -0.1419,  0.0890],\n","           [ 0.1469,  0.0227, -0.0808,  0.1226, -0.1856]]],\n"," \n"," \n","         [[[-0.0470, -0.1087, -0.1121,  0.1914,  0.1074],\n","           [-0.0562, -0.1186,  0.1237,  0.1438, -0.0102],\n","           [ 0.1903, -0.1052,  0.0622,  0.0330, -0.1957],\n","           [-0.0679, -0.1428,  0.1845, -0.1165,  0.0171],\n","           [-0.1906,  0.1181,  0.0338, -0.0713,  0.0646]]],\n"," \n"," \n","         [[[-0.1393,  0.1030,  0.1867,  0.1314,  0.1633],\n","           [ 0.0972, -0.0301,  0.0520, -0.0031,  0.0563],\n","           [ 0.0619, -0.1467, -0.0331, -0.1004, -0.0549],\n","           [ 0.1265,  0.0639, -0.1774,  0.1755,  0.1843],\n","           [ 0.1187,  0.0606,  0.0758,  0.1125,  0.0287]]]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.0610,  0.0096,  0.1243, -0.1664,  0.1856,  0.1529],\n","        requires_grad=True),\n"," Parameter containing:\n"," tensor([[[[-0.0440, -0.0443,  0.0353,  0.0539, -0.0362],\n","           [ 0.0134,  0.0674,  0.0717,  0.0779, -0.0361],\n","           [ 0.0411,  0.0677, -0.0808, -0.0723, -0.0584],\n","           [-0.0725, -0.0055, -0.0288,  0.0350, -0.0283],\n","           [ 0.0523, -0.0264,  0.0676, -0.0471,  0.0623]],\n"," \n","          [[-0.0625, -0.0528,  0.0185, -0.0159, -0.0558],\n","           [-0.0195, -0.0653,  0.0573,  0.0089, -0.0338],\n","           [ 0.0350, -0.0212,  0.0395,  0.0567, -0.0377],\n","           [-0.0029,  0.0195, -0.0050, -0.0193, -0.0223],\n","           [ 0.0414,  0.0600, -0.0772, -0.0771,  0.0138]],\n"," \n","          [[ 0.0241, -0.0118, -0.0147, -0.0172, -0.0195],\n","           [-0.0656, -0.0119,  0.0546, -0.0369,  0.0705],\n","           [-0.0167, -0.0249,  0.0668, -0.0797, -0.0341],\n","           [ 0.0588,  0.0275,  0.0419,  0.0449,  0.0193],\n","           [-0.0321,  0.0685,  0.0667, -0.0497,  0.0598]],\n"," \n","          [[ 0.0772, -0.0351, -0.0569,  0.0415, -0.0011],\n","           [ 0.0607, -0.0406,  0.0683,  0.0687,  0.0630],\n","           [-0.0402, -0.0424,  0.0111,  0.0122,  0.0317],\n","           [ 0.0237, -0.0109, -0.0111, -0.0797,  0.0435],\n","           [ 0.0665,  0.0550,  0.0707,  0.0686,  0.0795]],\n"," \n","          [[ 0.0620,  0.0707, -0.0537,  0.0194, -0.0602],\n","           [ 0.0679, -0.0410, -0.0583,  0.0626,  0.0377],\n","           [ 0.0339,  0.0709, -0.0051, -0.0600, -0.0291],\n","           [ 0.0468,  0.0391, -0.0214,  0.0629,  0.0410],\n","           [-0.0429, -0.0305, -0.0548, -0.0477,  0.0536]],\n"," \n","          [[ 0.0809,  0.0430, -0.0264, -0.0720,  0.0132],\n","           [ 0.0167,  0.0216,  0.0073,  0.0258, -0.0136],\n","           [ 0.0032,  0.0387,  0.0412,  0.0695, -0.0736],\n","           [ 0.0727, -0.0621, -0.0048,  0.0740,  0.0132],\n","           [-0.0073,  0.0779,  0.0489,  0.0797, -0.0457]]],\n"," \n"," \n","         [[[-0.0421,  0.0619,  0.0760, -0.0661,  0.0228],\n","           [-0.0166, -0.0153, -0.0651,  0.0032,  0.0469],\n","           [-0.0486,  0.0260,  0.0198, -0.0357, -0.0262],\n","           [-0.0135,  0.0706, -0.0504,  0.0641, -0.0715],\n","           [ 0.0724, -0.0061, -0.0524, -0.0303, -0.0725]],\n"," \n","          [[-0.0521,  0.0697, -0.0337, -0.0306,  0.0230],\n","           [-0.0257,  0.0788,  0.0093, -0.0060, -0.0355],\n","           [ 0.0009,  0.0800,  0.0419,  0.0230,  0.0816],\n","           [-0.0526, -0.0094, -0.0314, -0.0485, -0.0497],\n","           [-0.0011, -0.0118, -0.0600,  0.0153, -0.0286]],\n"," \n","          [[ 0.0402,  0.0160, -0.0774,  0.0522, -0.0136],\n","           [-0.0567, -0.0714, -0.0173,  0.0022,  0.0031],\n","           [-0.0569,  0.0749, -0.0119, -0.0720, -0.0621],\n","           [ 0.0677,  0.0355,  0.0478, -0.0660,  0.0138],\n","           [-0.0807, -0.0254, -0.0037, -0.0103, -0.0187]],\n"," \n","          [[ 0.0698, -0.0185,  0.0409,  0.0468,  0.0722],\n","           [ 0.0024, -0.0371,  0.0073,  0.0481, -0.0606],\n","           [-0.0472, -0.0003, -0.0592, -0.0454, -0.0687],\n","           [-0.0610,  0.0642,  0.0190, -0.0373,  0.0176],\n","           [ 0.0283,  0.0565,  0.0312, -0.0084, -0.0679]],\n"," \n","          [[ 0.0240,  0.0742,  0.0723, -0.0374,  0.0009],\n","           [-0.0312, -0.0233,  0.0498, -0.0234,  0.0009],\n","           [ 0.0323,  0.0056,  0.0380, -0.0132, -0.0813],\n","           [ 0.0396, -0.0141, -0.0420,  0.0133,  0.0214],\n","           [ 0.0784,  0.0243,  0.0188, -0.0715, -0.0672]],\n"," \n","          [[ 0.0370, -0.0188, -0.0416,  0.0355,  0.0015],\n","           [ 0.0103, -0.0390, -0.0029,  0.0457, -0.0145],\n","           [ 0.0422,  0.0815, -0.0689, -0.0421,  0.0039],\n","           [-0.0793,  0.0447, -0.0676, -0.0513,  0.0667],\n","           [ 0.0142, -0.0217,  0.0523,  0.0444, -0.0766]]],\n"," \n"," \n","         [[[-0.0487, -0.0089,  0.0158,  0.0214,  0.0242],\n","           [ 0.0655, -0.0514, -0.0484,  0.0669, -0.0100],\n","           [-0.0374,  0.0809,  0.0405, -0.0494,  0.0442],\n","           [-0.0095,  0.0188,  0.0503, -0.0673, -0.0707],\n","           [-0.0202, -0.0457,  0.0744,  0.0502, -0.0073]],\n"," \n","          [[ 0.0270,  0.0572,  0.0157, -0.0423,  0.0222],\n","           [-0.0341,  0.0392,  0.0440,  0.0768,  0.0427],\n","           [-0.0806, -0.0675,  0.0387, -0.0526, -0.0182],\n","           [-0.0368,  0.0100, -0.0293, -0.0789,  0.0640],\n","           [ 0.0721, -0.0018, -0.0678,  0.0701,  0.0562]],\n"," \n","          [[-0.0792,  0.0291,  0.0675, -0.0502,  0.0056],\n","           [ 0.0793,  0.0240,  0.0293,  0.0379,  0.0096],\n","           [-0.0060,  0.0575, -0.0163,  0.0497, -0.0789],\n","           [-0.0785, -0.0664,  0.0036, -0.0588,  0.0493],\n","           [ 0.0057, -0.0720, -0.0236,  0.0214, -0.0419]],\n"," \n","          [[ 0.0124, -0.0317, -0.0721,  0.0243, -0.0008],\n","           [ 0.0635, -0.0027, -0.0148,  0.0219, -0.0219],\n","           [ 0.0037,  0.0107, -0.0756,  0.0091,  0.0553],\n","           [ 0.0195, -0.0303, -0.0547, -0.0636, -0.0306],\n","           [ 0.0552, -0.0704, -0.0207, -0.0677,  0.0568]],\n"," \n","          [[-0.0704,  0.0123,  0.0290,  0.0699, -0.0068],\n","           [ 0.0620, -0.0486, -0.0097,  0.0241,  0.0447],\n","           [-0.0253,  0.0663,  0.0287, -0.0186, -0.0256],\n","           [-0.0026,  0.0806,  0.0063,  0.0003,  0.0392],\n","           [ 0.0476,  0.0221, -0.0670, -0.0386, -0.0664]],\n"," \n","          [[-0.0260,  0.0206,  0.0721,  0.0411, -0.0339],\n","           [-0.0622,  0.0065,  0.0714,  0.0672,  0.0190],\n","           [-0.0172,  0.0103, -0.0226, -0.0017,  0.0485],\n","           [-0.0362,  0.0455, -0.0251, -0.0438, -0.0593],\n","           [-0.0381, -0.0709,  0.0404, -0.0589,  0.0101]]],\n"," \n"," \n","         ...,\n"," \n"," \n","         [[[ 0.0337,  0.0313,  0.0117, -0.0288, -0.0798],\n","           [-0.0612,  0.0533,  0.0284, -0.0122,  0.0621],\n","           [ 0.0345, -0.0164, -0.0607, -0.0652, -0.0757],\n","           [-0.0490, -0.0532,  0.0726, -0.0249,  0.0526],\n","           [-0.0803, -0.0465, -0.0225, -0.0203, -0.0048]],\n"," \n","          [[ 0.0248,  0.0611, -0.0323,  0.0087, -0.0219],\n","           [ 0.0426, -0.0685,  0.0242, -0.0230, -0.0380],\n","           [ 0.0493, -0.0439,  0.0027,  0.0586,  0.0614],\n","           [-0.0345, -0.0778, -0.0731, -0.0213,  0.0412],\n","           [-0.0739,  0.0673,  0.0473,  0.0590,  0.0324]],\n"," \n","          [[-0.0291, -0.0134, -0.0427, -0.0251,  0.0377],\n","           [-0.0788, -0.0153,  0.0262, -0.0366, -0.0609],\n","           [ 0.0744, -0.0068,  0.0047, -0.0232,  0.0029],\n","           [ 0.0039,  0.0603,  0.0035, -0.0589, -0.0549],\n","           [ 0.0677, -0.0266,  0.0207,  0.0429,  0.0509]],\n"," \n","          [[ 0.0080,  0.0414,  0.0166,  0.0749, -0.0095],\n","           [ 0.0237,  0.0205,  0.0613,  0.0673, -0.0645],\n","           [ 0.0397, -0.0041,  0.0353,  0.0107, -0.0304],\n","           [ 0.0571, -0.0196, -0.0136,  0.0629, -0.0215],\n","           [ 0.0702,  0.0809,  0.0578,  0.0378, -0.0016]],\n"," \n","          [[-0.0809, -0.0135,  0.0523,  0.0618, -0.0291],\n","           [-0.0322,  0.0788, -0.0278,  0.0414, -0.0601],\n","           [ 0.0769,  0.0727,  0.0772,  0.0482,  0.0548],\n","           [ 0.0202, -0.0418,  0.0243, -0.0570,  0.0041],\n","           [-0.0098, -0.0096,  0.0384, -0.0340, -0.0085]],\n"," \n","          [[ 0.0090, -0.0489, -0.0138, -0.0216,  0.0775],\n","           [-0.0598, -0.0741,  0.0568, -0.0282,  0.0252],\n","           [-0.0178,  0.0069, -0.0521,  0.0262, -0.0483],\n","           [ 0.0586,  0.0258,  0.0328, -0.0556, -0.0574],\n","           [-0.0112, -0.0006,  0.0768, -0.0404, -0.0775]]],\n"," \n"," \n","         [[[-0.0658,  0.0262,  0.0076, -0.0652,  0.0802],\n","           [ 0.0516, -0.0580, -0.0443, -0.0375,  0.0413],\n","           [ 0.0379, -0.0406,  0.0260, -0.0123,  0.0517],\n","           [ 0.0104,  0.0549,  0.0071, -0.0744, -0.0042],\n","           [-0.0419,  0.0351,  0.0411, -0.0780, -0.0342]],\n"," \n","          [[-0.0348, -0.0393,  0.0198,  0.0136, -0.0374],\n","           [-0.0129,  0.0292, -0.0209, -0.0765,  0.0448],\n","           [ 0.0500, -0.0625,  0.0117,  0.0715, -0.0537],\n","           [ 0.0048, -0.0751,  0.0504,  0.0300, -0.0475],\n","           [ 0.0747, -0.0691,  0.0264,  0.0261,  0.0316]],\n"," \n","          [[-0.0475,  0.0288, -0.0205,  0.0484, -0.0687],\n","           [ 0.0034,  0.0421,  0.0671, -0.0536, -0.0274],\n","           [-0.0103, -0.0472, -0.0679,  0.0215, -0.0050],\n","           [ 0.0753, -0.0136,  0.0557, -0.0129,  0.0358],\n","           [-0.0250,  0.0006,  0.0037,  0.0642,  0.0627]],\n"," \n","          [[ 0.0509,  0.0277,  0.0772,  0.0416, -0.0625],\n","           [ 0.0410,  0.0699, -0.0397, -0.0270,  0.0316],\n","           [-0.0140, -0.0654, -0.0114,  0.0072, -0.0755],\n","           [ 0.0437,  0.0579, -0.0186,  0.0040,  0.0607],\n","           [ 0.0132, -0.0521, -0.0586,  0.0682, -0.0298]],\n"," \n","          [[ 0.0258,  0.0528, -0.0457,  0.0222, -0.0310],\n","           [-0.0295, -0.0319,  0.0063,  0.0179, -0.0098],\n","           [ 0.0362,  0.0331,  0.0512, -0.0375,  0.0560],\n","           [-0.0038, -0.0383,  0.0464,  0.0560, -0.0480],\n","           [-0.0136, -0.0587,  0.0162, -0.0044,  0.0320]],\n"," \n","          [[-0.0006, -0.0806,  0.0271, -0.0721, -0.0289],\n","           [-0.0473,  0.0456, -0.0365, -0.0680,  0.0004],\n","           [-0.0138, -0.0327, -0.0142, -0.0040, -0.0691],\n","           [ 0.0087, -0.0141,  0.0695,  0.0435, -0.0540],\n","           [-0.0399, -0.0409, -0.0013,  0.0399,  0.0645]]],\n"," \n"," \n","         [[[-0.0339,  0.0523, -0.0230,  0.0527, -0.0521],\n","           [-0.0514,  0.0800, -0.0450, -0.0614, -0.0113],\n","           [ 0.0578, -0.0739, -0.0420, -0.0592, -0.0241],\n","           [ 0.0440,  0.0142,  0.0815,  0.0611,  0.0330],\n","           [-0.0528,  0.0404, -0.0199, -0.0054, -0.0680]],\n"," \n","          [[-0.0050, -0.0424, -0.0070,  0.0476,  0.0811],\n","           [-0.0125, -0.0561,  0.0279, -0.0028, -0.0502],\n","           [-0.0346,  0.0636,  0.0218,  0.0083,  0.0219],\n","           [-0.0454, -0.0029, -0.0301, -0.0168,  0.0438],\n","           [-0.0319, -0.0676,  0.0109,  0.0606, -0.0386]],\n"," \n","          [[-0.0608, -0.0773,  0.0053, -0.0192,  0.0413],\n","           [-0.0478,  0.0618, -0.0225,  0.0192,  0.0124],\n","           [-0.0490,  0.0344,  0.0112, -0.0765, -0.0707],\n","           [-0.0008, -0.0027,  0.0251, -0.0211, -0.0614],\n","           [ 0.0133,  0.0671, -0.0458,  0.0018, -0.0566]],\n"," \n","          [[ 0.0147,  0.0266,  0.0741, -0.0789,  0.0748],\n","           [-0.0424, -0.0380, -0.0149,  0.0346,  0.0474],\n","           [-0.0378,  0.0142,  0.0526, -0.0092,  0.0099],\n","           [-0.0248,  0.0076,  0.0752, -0.0018,  0.0072],\n","           [ 0.0656,  0.0387,  0.0478,  0.0175,  0.0213]],\n"," \n","          [[-0.0538,  0.0776,  0.0049,  0.0654, -0.0368],\n","           [-0.0361,  0.0803,  0.0134,  0.0599, -0.0688],\n","           [ 0.0105, -0.0599, -0.0221, -0.0570, -0.0272],\n","           [ 0.0510,  0.0709,  0.0585, -0.0662, -0.0639],\n","           [-0.0519,  0.0722,  0.0279, -0.0308,  0.0580]],\n"," \n","          [[ 0.0259,  0.0689, -0.0540,  0.0735,  0.0457],\n","           [-0.0483, -0.0061, -0.0433, -0.0429, -0.0113],\n","           [-0.0460,  0.0086, -0.0161, -0.0045, -0.0271],\n","           [ 0.0142,  0.0415, -0.0111,  0.0316,  0.0205],\n","           [-0.0734, -0.0042,  0.0538, -0.0637,  0.0263]]]], requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.0662,  0.0124,  0.0730,  0.0566, -0.0438, -0.0089,  0.0103, -0.0524,\n","          0.0051,  0.0691,  0.0692, -0.0241,  0.0754,  0.0615,  0.0120,  0.0457],\n","        requires_grad=True),\n"," Parameter containing:\n"," tensor([[-0.0386,  0.0416,  0.0121,  ..., -0.0438,  0.0416, -0.0040],\n","         [ 0.0077,  0.0230, -0.0040,  ...,  0.0222,  0.0154, -0.0283],\n","         [-0.0044,  0.0059,  0.0153,  ...,  0.0397,  0.0127, -0.0141],\n","         ...,\n","         [ 0.0010,  0.0252,  0.0407,  ...,  0.0070,  0.0447, -0.0215],\n","         [-0.0245, -0.0240,  0.0143,  ..., -0.0438,  0.0377,  0.0232],\n","         [ 0.0218, -0.0150,  0.0492,  ..., -0.0281,  0.0308,  0.0491]],\n","        requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.0370,  0.0411, -0.0458, -0.0498, -0.0335,  0.0223, -0.0382,  0.0012,\n","          0.0319,  0.0409,  0.0406, -0.0347, -0.0478,  0.0387,  0.0342,  0.0249,\n","          0.0137,  0.0155,  0.0168, -0.0073, -0.0448,  0.0101,  0.0240,  0.0310,\n","         -0.0087, -0.0028, -0.0200, -0.0170,  0.0172, -0.0039, -0.0354,  0.0467,\n","         -0.0270, -0.0442, -0.0240,  0.0272,  0.0237, -0.0118,  0.0338, -0.0204,\n","          0.0341,  0.0332,  0.0139,  0.0094, -0.0078,  0.0300,  0.0417, -0.0096,\n","          0.0449, -0.0250, -0.0264,  0.0116,  0.0431,  0.0147, -0.0430,  0.0040,\n","          0.0338,  0.0288,  0.0220, -0.0229,  0.0117, -0.0369, -0.0190, -0.0085,\n","          0.0221, -0.0469, -0.0243, -0.0184,  0.0322, -0.0123, -0.0149,  0.0277,\n","         -0.0443,  0.0411, -0.0263,  0.0283,  0.0464, -0.0179, -0.0060,  0.0091,\n","         -0.0444, -0.0316,  0.0046, -0.0476,  0.0075,  0.0074, -0.0377,  0.0247,\n","          0.0028, -0.0335, -0.0421, -0.0025,  0.0200, -0.0194,  0.0395, -0.0496,\n","         -0.0409, -0.0337,  0.0294, -0.0096, -0.0266, -0.0118,  0.0411, -0.0356,\n","          0.0336, -0.0049,  0.0222,  0.0346,  0.0499,  0.0221,  0.0374, -0.0463,\n","         -0.0161,  0.0101, -0.0392, -0.0027, -0.0286, -0.0156,  0.0288,  0.0089],\n","        requires_grad=True),\n"," Parameter containing:\n"," tensor([[ 0.0908,  0.0097, -0.0771,  ...,  0.0558, -0.0770,  0.0044],\n","         [-0.0143, -0.0768,  0.0324,  ...,  0.0040, -0.0341,  0.0790],\n","         [ 0.0102, -0.0351,  0.0128,  ..., -0.0168,  0.0312,  0.0397],\n","         ...,\n","         [-0.0332, -0.0890, -0.0785,  ...,  0.0698,  0.0552, -0.0168],\n","         [-0.0134, -0.0631, -0.0268,  ...,  0.0898, -0.0787, -0.0262],\n","         [ 0.0904,  0.0370,  0.0399,  ...,  0.0691, -0.0670, -0.0051]],\n","        requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.0310, -0.0141,  0.0545,  0.0409,  0.0598,  0.0509,  0.0402, -0.0754,\n","         -0.0511, -0.0450, -0.0303, -0.0405,  0.0490, -0.0602,  0.0588,  0.0504,\n","          0.0776,  0.0828, -0.0539,  0.0232,  0.0231, -0.0179,  0.0332,  0.0388,\n","          0.0355, -0.0032, -0.0441, -0.0440,  0.0369,  0.0708, -0.0409,  0.0573,\n","          0.0184, -0.0223, -0.0383,  0.0862, -0.0800, -0.0530, -0.0245,  0.0477,\n","          0.0761,  0.0016, -0.0190,  0.0243, -0.0484,  0.0813,  0.0273,  0.0148,\n","          0.0833,  0.0800, -0.0606, -0.0299,  0.0327,  0.0799,  0.0220, -0.0649,\n","          0.0750, -0.0777, -0.0347,  0.0817,  0.0857, -0.0387,  0.0682, -0.0266,\n","         -0.0421,  0.0901, -0.0780,  0.0414, -0.0899,  0.0047,  0.0909,  0.0784,\n","         -0.0242,  0.0300, -0.0610, -0.0495, -0.0145,  0.0727, -0.0882, -0.0568,\n","         -0.0471, -0.0488, -0.0229, -0.0245], requires_grad=True),\n"," Parameter containing:\n"," tensor([[ 0.0395, -0.0544, -0.1058, -0.0174, -0.0906, -0.0363,  0.0494,  0.0468,\n","           0.0971, -0.0558,  0.0385,  0.0829,  0.0580, -0.0759, -0.0872,  0.0548,\n","          -0.0273, -0.0702, -0.0489,  0.0544, -0.0804,  0.0016,  0.0745,  0.0483,\n","           0.0966,  0.0973, -0.0686, -0.0654,  0.0245, -0.0595, -0.1087, -0.0462,\n","          -0.0738, -0.0432,  0.0888, -0.0256, -0.0131, -0.0485, -0.0459,  0.0533,\n","          -0.0458, -0.0045, -0.0540, -0.0042,  0.0937,  0.0501, -0.0087, -0.0299,\n","           0.1036, -0.0037, -0.0490,  0.0025, -0.0743, -0.0717,  0.0802,  0.0386,\n","           0.0676, -0.1082,  0.0280,  0.0120, -0.0819,  0.0061, -0.0711, -0.0758,\n","          -0.0271, -0.0657,  0.0847, -0.0198,  0.0454, -0.0252,  0.0671, -0.0896,\n","          -0.0568, -0.0255,  0.0579,  0.0441,  0.1056,  0.0123,  0.0255, -0.0855,\n","          -0.0411,  0.0646,  0.0888,  0.0573],\n","         [-0.0850, -0.0672,  0.0473,  0.0778,  0.0126,  0.0707,  0.0170, -0.0678,\n","          -0.0439,  0.0464,  0.1004, -0.0364,  0.0090,  0.0314, -0.0336,  0.0167,\n","           0.0474, -0.0263,  0.0988, -0.0596, -0.0569,  0.0191, -0.0003, -0.0512,\n","          -0.0678,  0.0370,  0.0131,  0.0857,  0.0098,  0.0855,  0.0724,  0.0938,\n","          -0.0044,  0.0920,  0.0371, -0.0578, -0.0228,  0.0826,  0.0719,  0.0407,\n","           0.0139, -0.0647, -0.0204, -0.0524, -0.0642, -0.0961, -0.0979, -0.0351,\n","           0.0322, -0.1033, -0.1091,  0.0843,  0.1014,  0.0012, -0.0773, -0.0073,\n","           0.1001, -0.0627, -0.0097,  0.0392,  0.0142, -0.1068,  0.0371, -0.0947,\n","          -0.0851, -0.0731, -0.0939,  0.0763,  0.0830, -0.0142,  0.0051,  0.0991,\n","           0.0509, -0.0382, -0.0598,  0.0556,  0.1009, -0.0473,  0.0780, -0.0587,\n","          -0.0356,  0.0449, -0.0059, -0.0572],\n","         [-0.0505,  0.0953,  0.0691, -0.0494,  0.1079, -0.0890, -0.0897,  0.0826,\n","          -0.0948,  0.1080,  0.0404,  0.0291,  0.0394,  0.0079,  0.0477,  0.0186,\n","           0.0606,  0.0003,  0.0889, -0.0173, -0.0132,  0.0156, -0.0909,  0.0764,\n","           0.0727, -0.0926, -0.0301,  0.0793,  0.0214,  0.0326, -0.0904,  0.0807,\n","           0.1004,  0.0937, -0.0210, -0.0593,  0.0364,  0.0296,  0.0115, -0.0863,\n","          -0.0042,  0.0282,  0.0879,  0.0556, -0.0437, -0.0358, -0.0878,  0.0457,\n","          -0.0569, -0.0210,  0.0049,  0.0380, -0.0977, -0.0699, -0.1014, -0.0576,\n","          -0.0696, -0.0857,  0.1044,  0.0747, -0.0894,  0.0143, -0.0497, -0.0262,\n","           0.1066,  0.0771, -0.0638, -0.0808,  0.0993, -0.0174,  0.0702,  0.0585,\n","           0.0447,  0.0978,  0.0212, -0.0197,  0.0180, -0.0703,  0.0395, -0.1038,\n","          -0.0456,  0.0250,  0.0691,  0.0534],\n","         [ 0.0593,  0.0078, -0.0879, -0.0763, -0.0172,  0.0103, -0.0435,  0.1064,\n","           0.0317,  0.0436,  0.1018,  0.0033, -0.0119, -0.0863,  0.0975, -0.0402,\n","           0.0321, -0.0481, -0.1066,  0.0941, -0.0886, -0.0188,  0.0171,  0.0854,\n","          -0.0207,  0.0824,  0.0723, -0.0206,  0.0215, -0.0347,  0.0857,  0.0462,\n","           0.0885,  0.0817,  0.0014, -0.0131,  0.0805,  0.0874,  0.0558, -0.0225,\n","          -0.0598,  0.0317,  0.0765, -0.0103,  0.0271, -0.0775, -0.0250, -0.0180,\n","          -0.0397,  0.0231, -0.0962, -0.0999, -0.0580,  0.0111, -0.0228, -0.0983,\n","          -0.0146, -0.0924,  0.0293, -0.1077,  0.0063,  0.0084, -0.1035,  0.0858,\n","          -0.0400,  0.0049,  0.1057,  0.0596, -0.0378,  0.0386,  0.0499, -0.0887,\n","           0.0820, -0.0330, -0.0523, -0.0384,  0.0854, -0.0699,  0.0806, -0.0816,\n","          -0.0950,  0.0046,  0.0574, -0.0197],\n","         [ 0.0394, -0.0251, -0.0187, -0.0113, -0.0977,  0.0190,  0.0853, -0.0551,\n","           0.1082,  0.0351,  0.0154,  0.0804, -0.0010, -0.0726,  0.0048,  0.0515,\n","          -0.0699,  0.0042, -0.0579,  0.0785, -0.0021,  0.0507, -0.0986, -0.0691,\n","          -0.0454,  0.0016, -0.0607, -0.0960,  0.0988,  0.0617,  0.0993,  0.0881,\n","           0.0398,  0.0267,  0.0514, -0.0663, -0.0249,  0.0398,  0.1003, -0.1073,\n","           0.0058,  0.0457,  0.0054, -0.1038, -0.1067, -0.0977, -0.0998, -0.0923,\n","          -0.0582, -0.1077,  0.0713,  0.0213,  0.0844, -0.0136,  0.0212,  0.0753,\n","          -0.0448,  0.0795,  0.1058, -0.1067, -0.0369, -0.0902,  0.0864,  0.0477,\n","           0.0139,  0.0181,  0.0402,  0.0774, -0.0751, -0.0882,  0.0727,  0.0756,\n","           0.0394, -0.0134,  0.0514, -0.0189,  0.0833,  0.0430,  0.0058, -0.0225,\n","           0.0333, -0.0795, -0.0192, -0.0240],\n","         [-0.0803, -0.0110,  0.0494,  0.1077, -0.0785,  0.0306, -0.1024,  0.0786,\n","           0.0216, -0.0464,  0.0853, -0.0498,  0.0949, -0.1005,  0.0456, -0.0167,\n","          -0.0738, -0.0675,  0.0577,  0.0686,  0.1020,  0.0902, -0.0481, -0.0231,\n","          -0.0271,  0.0129,  0.0121, -0.0367, -0.1008, -0.0841, -0.0685, -0.1091,\n","          -0.0650, -0.0148,  0.0390, -0.0246, -0.0228, -0.0483, -0.0872, -0.0515,\n","          -0.0974,  0.0034,  0.0755, -0.1053, -0.0951,  0.0910, -0.0664, -0.0348,\n","          -0.0811, -0.1019,  0.0890,  0.0267, -0.0338, -0.0795, -0.0291, -0.0323,\n","           0.0351,  0.0943,  0.0841,  0.0101, -0.0113, -0.0672,  0.0944,  0.0262,\n","           0.0016, -0.0034, -0.0998,  0.0913,  0.0541, -0.0100,  0.0145,  0.0658,\n","           0.0171,  0.0106, -0.0283,  0.0640, -0.0695, -0.0892, -0.0823,  0.0612,\n","          -0.0464, -0.0471,  0.0513,  0.0099],\n","         [-0.0399, -0.0453, -0.0549,  0.0166,  0.0994,  0.0650,  0.0828, -0.0439,\n","          -0.0707, -0.0660, -0.0470, -0.0568,  0.0106, -0.0134, -0.0676, -0.0280,\n","           0.1058,  0.0639, -0.0374,  0.0211,  0.0467, -0.0060, -0.0711,  0.0659,\n","          -0.0506,  0.0427,  0.0112, -0.0375, -0.0427, -0.0994,  0.0645,  0.0997,\n","          -0.0553,  0.0917, -0.0704,  0.0393, -0.0867, -0.1031, -0.0543,  0.0778,\n","           0.0390, -0.1085, -0.0676, -0.0186,  0.0830,  0.0679,  0.0097, -0.0014,\n","          -0.0363,  0.0190,  0.0121, -0.0837, -0.0993,  0.0969,  0.0888, -0.0511,\n","          -0.0773, -0.0310, -0.0040,  0.0299, -0.1040,  0.1073,  0.1059,  0.0137,\n","          -0.1036,  0.0464,  0.0276,  0.1069,  0.0073, -0.0831, -0.0570,  0.1073,\n","          -0.0499,  0.0689, -0.0046,  0.0870, -0.0347, -0.0739, -0.0512,  0.0316,\n","           0.0071,  0.0079, -0.0857,  0.0293],\n","         [-0.0426, -0.0620,  0.0334,  0.0536, -0.0393, -0.0544,  0.0895,  0.1046,\n","           0.0904,  0.0466, -0.1067, -0.0846, -0.0600,  0.0008, -0.0290,  0.0411,\n","           0.0417,  0.0042, -0.0857, -0.0747,  0.0053, -0.0122, -0.0194,  0.0511,\n","           0.1075, -0.0038,  0.0511, -0.0986,  0.1006, -0.0950,  0.0977, -0.0416,\n","           0.0336,  0.0652,  0.0667,  0.0665, -0.1009,  0.0790, -0.0286, -0.1009,\n","           0.0760,  0.0525, -0.0786, -0.0600,  0.0883, -0.0243, -0.0312, -0.0295,\n","          -0.0573, -0.1022,  0.0860,  0.0966,  0.0299,  0.0317, -0.0377,  0.0563,\n","           0.0756, -0.0017, -0.0679,  0.0075,  0.0820, -0.0406, -0.0354,  0.0861,\n","          -0.0097,  0.0627,  0.0025, -0.0074,  0.0204,  0.0036,  0.0238,  0.0182,\n","           0.0046,  0.0713, -0.0587, -0.0304,  0.0490, -0.0490, -0.0707, -0.0809,\n","          -0.0372, -0.1086,  0.0604, -0.0383],\n","         [-0.0286,  0.0354,  0.0942, -0.0209,  0.0078, -0.0391,  0.1006, -0.0568,\n","          -0.0435,  0.1048,  0.0002, -0.0330, -0.1062,  0.0770,  0.0913,  0.0476,\n","          -0.0120,  0.0911, -0.0049,  0.0919, -0.0345, -0.0428, -0.0263,  0.0654,\n","           0.1052,  0.0421,  0.0722,  0.0902,  0.0961, -0.0405, -0.0605,  0.0243,\n","           0.0154,  0.0483,  0.0176, -0.0172,  0.0915,  0.0801,  0.0013, -0.0724,\n","          -0.0898, -0.0116,  0.0933, -0.0192,  0.0223,  0.0515, -0.0522,  0.0608,\n","          -0.0931,  0.0437, -0.0928, -0.0162,  0.0933, -0.0277, -0.0427, -0.0663,\n","          -0.0644,  0.1033, -0.0416,  0.0616, -0.1071, -0.0536,  0.0197, -0.0465,\n","          -0.0655,  0.1087, -0.1074,  0.1058,  0.0704,  0.0057, -0.0974, -0.0614,\n","          -0.0271,  0.0761, -0.0215,  0.0045,  0.0935,  0.0683, -0.0844, -0.0177,\n","           0.0714, -0.0885,  0.0094,  0.1047],\n","         [-0.0006,  0.0462,  0.0071,  0.0786, -0.0418,  0.0841,  0.0561,  0.0173,\n","           0.1027, -0.0011, -0.0176,  0.0803, -0.0884, -0.0988, -0.0464, -0.0180,\n","          -0.0788, -0.0226, -0.0084,  0.0996, -0.0505, -0.0487,  0.0988, -0.0831,\n","           0.0328, -0.0630, -0.0592,  0.0220, -0.0132,  0.1034, -0.1054,  0.0407,\n","          -0.0338, -0.0150, -0.0121,  0.0457,  0.0763,  0.0050, -0.0144, -0.0066,\n","          -0.1059,  0.1037,  0.1064,  0.0219, -0.0126, -0.0058, -0.0150,  0.0669,\n","          -0.0493,  0.0906,  0.0187,  0.0360, -0.0021,  0.0977,  0.0360, -0.0411,\n","          -0.0505, -0.0258,  0.1012,  0.0246, -0.0122,  0.0141,  0.0332, -0.1028,\n","           0.0383, -0.0730,  0.0785,  0.0021, -0.0956, -0.0258,  0.0742, -0.0971,\n","          -0.0918,  0.0097,  0.0856, -0.0079,  0.0808,  0.0549, -0.0988,  0.0431,\n","           0.0353, -0.1050,  0.0848, -0.0932]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.0286,  0.0726,  0.0672,  0.0837,  0.0385,  0.1085, -0.0286, -0.0761,\n","         -0.0307,  0.0647], requires_grad=True)]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"poyvhBHqH0WX","colab_type":"text"},"source":["Let try a random 32x32 input.\n","Note: expected input size of this net (LeNet) is 32x32. To use this net on\n","MNIST dataset, please resize the images from the dataset to 32x32.\n","\n"]},{"cell_type":"code","metadata":{"id":"g1H3hohFH0WZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"31407e1e-2491-42e6-9347-6b7cf90c559e","executionInfo":{"status":"ok","timestamp":1557146255456,"user_tz":-120,"elapsed":707,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[-0.0576,  0.1099,  0.0334,  0.0445,  0.0173,  0.0152, -0.0554, -0.1353,\n","         -0.0010,  0.1712]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iwUwZsumH0Wf","colab_type":"text"},"source":["Zero the gradient buffers of all parameters (needed every epoch) and backprops with random\n","gradients:\n","\n"]},{"cell_type":"code","metadata":{"id":"gQBwtk_pH0Wg","colab_type":"code","colab":{}},"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dyOq24v1H0Wj","colab_type":"text"},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n","    package only supports inputs that are a mini-batch of samples, and not\n","    a single sample.\n","\n","    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n","    ``nSamples x nChannels x Height x Width``.\n","\n","    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n","    a fake batch dimension.</p></div>\n","\n","Before proceeding further, let's recap all the classes you’ve seen so far.\n","\n","**Recap:**\n","  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n","     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n","     tensor.\n","  -  ``nn.Module`` - Neural network module. *Convenient way of\n","     encapsulating parameters*, with helpers for moving them to GPU,\n","     exporting, loading, etc.\n","  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n","     registered as a parameter when assigned as an attribute (self.) to a*\n","     ``Module``.\n","  -  ``autograd.Function`` - Implements *forward and backward definitions\n","     of an autograd operation*. Every ``Tensor`` operation creates at\n","     least a single ``Function`` node that connects to functions that\n","     created a ``Tensor`` and *encodes its history*.\n","\n","**At this point, we covered:**\n","  -  Defining a neural network\n","  -  Processing inputs and calling backward\n","\n","**Still Left:**\n","  -  Computing the loss\n","  -  Updating the weights of the network\n","\n","Loss Function\n","-------------\n","A loss function takes the (output, target) pair of inputs, and computes a\n","value that estimates how far away the output is from the target.\n","\n","There are several different\n","`loss functions <https://pytorch.org/docs/nn.html#loss-functions>`_ under the\n","nn package .\n","A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n","between the input and the target.\n","\n","For example:\n","\n"]},{"cell_type":"code","metadata":{"id":"kOth1rh-H0Wl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"769eaa82-aa58-47fd-95c3-7393194ba3c4","executionInfo":{"status":"ok","timestamp":1557146530584,"user_tz":-120,"elapsed":689,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["output = net(input)\n","target = torch.randn(10)  # a dummy target, for example\n","target = target.view(1, -1)  # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor(1.2816, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I_NcRX59H0Wn","colab_type":"text"},"source":["Now, if you follow ``loss`` in the backward direction, using its\n","``.grad_fn`` attribute, you will see a graph of computations that looks\n","like this:\n","\n","::\n","\n","    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n","          -> view -> linear -> relu -> linear -> relu -> linear\n","          -> MSELoss\n","          -> loss\n","\n","So, when we call ``loss.backward()``, the whole graph is differentiated\n","w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n","will have their ``.grad`` Tensor accumulated with the gradient.\n","\n","For illustration, let us follow a few steps backward:\n","\n"]},{"cell_type":"code","metadata":{"id":"EZVoaiNAH0Wo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"a49284cf-49d9-47e3-e96f-47787a4f8603","executionInfo":{"status":"ok","timestamp":1557146532191,"user_tz":-120,"elapsed":674,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7f833f44fba8>\n","<AddmmBackward object at 0x7f833f44fb00>\n","<AccumulateGrad object at 0x7f833f44fba8>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SJPQ2ZwzH0Wu","colab_type":"text"},"source":["Backprop\n","--------\n","To backpropagate the error all we have to do is to ``loss.backward()``.\n","You need to clear the existing gradients though, else gradients will be\n","accumulated to existing gradients.\n","\n","\n","Now we shall call ``loss.backward()``, and have a look at conv1's bias\n","gradients before and after the backward.\n","\n"]},{"cell_type":"code","metadata":{"id":"Xdxd78TYH0Ww","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"a0eeb8b3-472a-42b4-de5b-bef24b3e08ff","executionInfo":{"status":"ok","timestamp":1557146614029,"user_tz":-120,"elapsed":955,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward\n","tensor([ 0.0142,  0.0079,  0.0195, -0.0178,  0.0143,  0.0156])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tu7VsZo5H0Wy","colab_type":"text"},"source":["Now, we have seen how to use loss functions.\n","\n","**Read Later:**\n","\n","  The neural network package contains various modules and loss functions\n","  that form the building blocks of deep neural networks. A full list with\n","  documentation is `here <https://pytorch.org/docs/nn>`_.\n","\n","**The only thing left to learn is:**\n","\n","  - Updating the weights of the network\n","\n","Update the weights\n","------------------\n","The simplest update rule used in practice is the Stochastic Gradient\n","Descent (SGD):\n","\n","     ``weight = weight - learning_rate * gradient``\n","\n","We can implement this using simple python code:\n","\n",".. code:: python\n","\n","    learning_rate = 0.01\n","    for f in net.parameters():\n","        f.data.sub_(f.grad.data * learning_rate)\n","\n","However, as you use neural networks, you want to use various different\n","update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n","To enable this, we built a small package: ``torch.optim`` that\n","implements all these methods. Using it is very simple:\n","\n"]},{"cell_type":"code","metadata":{"id":"iEmaAS3SH0Wz","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# create your optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# in your training loop:\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rai-tHdoH0W2","colab_type":"text"},"source":[".. Note::\n","\n","      Observe how gradient buffers had to be manually set to zero using\n","      ``optimizer.zero_grad()``. This is because gradients are accumulated\n","      as explained in `Backprop`_ section.\n","\n"]}]}