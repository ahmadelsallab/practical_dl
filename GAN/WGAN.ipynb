{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WGAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lCUIhJFuF6na","colab_type":"text"},"source":["# How to Implement Wasserstein Loss for Generative Adversarial Networks\n","\n","[Theory](https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nlagdn87VowD"},"source":["The Wasserstein Generative Adversarial Network, or Wasserstein GAN, is an extension to the generative adversarial network that both improves the __stability__ when training the model and provides a __loss function that correlates with the quality of generated images__.\n","\n","It is an important extension to the GAN model and requires a __conceptual shift away from a discriminator__ that predicts the probability of a generated image being “real” and toward the idea of a critic model that scores the “realness” of a given image.\n","\n","This conceptual shift is motivated mathematically using the __earth mover distance__, or __Wasserstein distance__, to train the GAN that measures the ___distance between the data distribution observed in the training dataset and the distribution observed in the generated examples___.\n","\n","In this post, you will discover how to implement Wasserstein loss for Generative Adversarial Networks.\n","\n","After reading this post, you will know:\n","\n","- The conceptual shift in the WGAN from discriminator predicting a probability to a critic predicting a score.\n","- The implementation details for the WGAN as minor changes to the standard deep convolutional GAN.\n","- The intuition behind the Wasserstein loss function and how implement it from scratch."]},{"cell_type":"markdown","metadata":{"id":"AfoITKlRXtcX","colab_type":"text"},"source":["# Overview\n","This tutorial is divided into five parts; they are:\n","\n","1. GAN Stability and the Discriminator\n","2. What is a Wasserstein GAN?\n","3. Implementation Details of the Wasserstein GAN\n","4. How to Implement Wasserstein Loss\n","5. Common Point of Confusion With Expected Labels\n","\n","GAN Stability and the Discriminator Generative Adversarial Networks, or GANs, are challenging to train.\n","\n","The discriminator model must classify a given input image as real (from the dataset) or fake (generated), and the generator model must generate new and plausible images.\n","\n","The reason GANs are __difficult__ to train is that the architecture involves the __simultaneous training of a generator and a discriminator model in a zero-sum game__. Stable training requires finding and maintaining an __equilibrium__ between the capabilities of the two models.\n","\n","The __discriminator__ model is a neural network that learns a binary classification problem, using a __sigmoid__ activation function in the output layer, and is fit using a __binary cross entropy__ loss function. As such, the model predicts a probability that a given input is real (or fake as 1 minus the predicted) as a value between 0 and 1.\n","\n","The loss function has the effect of __penalizing__ the model ___proportionally to how far the predicted probability distribution differs from the expected probability distribution for a given image___. This provides the basis for the error that is back propagated through the discriminator and the generator in order to perform better on the next batch.\n","\n","The WGAN relaxes the role of the discriminator when training a GAN and proposes the alternative of a ___critic___.\n","\n","# What is a Wasserstein GAN?\n","The Wasserstein GAN, or WGAN for short, was introduced by Martin Arjovsky, et al. in their 2017 paper titled “Wasserstein GAN.”\n","\n","It is an extension of the GAN that seeks an alternate way of training the generator model to better approximate the distribution of data observed in a given training dataset.\n","\n","Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a ___critic___ that ___scores the realness or fakeness of a given image___.\n","\n","This change is motivated by a mathematical argument that training the ___generator___ should seek a __minimization__ of the __distance__ between the distribution of the data observed in the ___training__ dataset and the distribution observed in ___generated___ examples. The argument contrasts different distribution distance measures, such as ___Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Earth-Mover (EM) distance, referred to as Wasserstein distance.___\n","\n","_The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions._\n","\n","— Wasserstein GAN, 2017.\n","\n","They demonstrate that a __critic__ neural network can be trained to __approximate the Wasserstein distance__, and, in turn, used to effectively train a generator model.\n","\n","_… we define a form of GAN called Wasserstein-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound._\n","\n","— Wasserstein GAN, 2017.\n","\n","Importantly, the __Wasserstein distance__ has the properties that it is __continuous__ and __differentiable__ and _continues to provide a linear gradient, even after the critic is well trained._\n","\n","_The fact that the EM distance is continuous and differentiable a.e. means that we can (and should) ___train the critic till optimality.___ […] _the more we train the critic, the more reliable gradient of the Wasserstein we get_, which is actually useful by the fact that Wasserstein is differentiable almost everywhere._\n","\n","— Wasserstein GAN, 2017.\n","\n","___This is unlike the discriminator model that, once trained, may fail to provide useful gradient information for updating the generator model.___\n","\n","_The __discriminator__ learns ___very quickly___ to distinguish between fake and real, and as expected ___provides no reliable gradient information___. The __critic__, however, ___can’t saturate___ and converges to a linear function that gives remarkably clean gradients everywhere._\n","\n","— Wasserstein GAN, 2017.\n","\n","The benefit of the __WGAN__ is that the training process is more stable and ___less sensitive to model architecture and choice of hyperparameter configurations.___\n","\n","_… training WGANs ___does not___ require maintaining a ___careful balance___ in training of the discriminator and the generator, and does not require a careful design of the network architecture either. The __mode dropping phenomenon__ that is typical in GANs is also ___drastically reduced___._\n","\n","— Wasserstein GAN, 2017.\n","\n","Perhaps most importantly, the ___loss of the discriminator appears to relate to the quality of images created by the generator.___\n","\n","Specifically, ___the lower___ the __loss of the critic__ when evaluating generated images, ___the higher___ the expected __quality of the generated images__. This is important as __unlike other GANs__ that seek stability in terms of finding an __equilibrium__ between two models, the __WGAN__ seeks __convergence__, lowering generator loss.\n","\n","__Way to evaluate__\n","\n","_To our knowledge, this is the first time in GAN literature that such a property is shown, where the __loss of the GAN shows properties of convergence__. This property is extremely useful when doing research in adversarial networks as __one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others__._\n","\n","— Wasserstein GAN, 2017."]},{"cell_type":"markdown","metadata":{"id":"u3Zd-TRFcTrr","colab_type":"text"},"source":["# Implementation Details of the Wasserstein GAN\n","Although the theoretical grounding for the WGAN is dense, the implementation of a WGAN requires a _few minor_ changes to the standard deep convolutional GAN, or DCGAN.\n","\n","Those changes are as follows:\n","\n","- Use a linear activation function in the output layer of the critic model (instead of sigmoid).\n","\n","- Use __Wasserstein loss__ to train the critic and generator models that promote larger difference between scores for real and generated images.\n","\n","- Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).\n","\n","_In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [−0.01, 0.01]) after each gradient update._\n","\n","— Wasserstein GAN, 2017.\n","\n","- Update the critic model more times than the generator each iteration (e.g. 5).\n","\n","- Use the RMSProp version of gradient descent with small learning rate and no momentum (e.g. 0.00005).\n","\n","_… we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam […] We therefore switched to RMSProp …_\n","\n","— Wasserstein GAN, 2017.\n","\n","The image below provides a summary of the main training loop for training a WGAN, taken from the paper. Note the listing of recommended hyperparameters used in the model.\n","\n","![WGAN](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Algorithm-for-the-Wasserstein-Generative-Adversarial-Networks.png)"]},{"cell_type":"markdown","metadata":{"id":"DKHlvB3tdGGp","colab_type":"text"},"source":["# How to Implement Wasserstein Loss\n","The Wasserstein loss function seeks to increase the gap between the scores for real and generated images. Objective is to minimize this loss, or min the gap.\n","\n","We can summarize the function as it is described in the paper as follows:\n","\n","- Critic Loss = [average critic score on real images] – [average critic score on fake images] --> to minimize the distance between real and fake distributions.\n","- Generator Loss = -[average critic score on fake images] -->maximize critic score on fake image (so the negative sign, or the maximization of critic score on fake images).\n","\n","When we had a discriminator, we used to feed fake generated samples, with \"real\" labels \"=1\", so that if the discriminator says it's fake \"=0\", it gets more penalized, so to encourage the generator to fool the discriminator. This is the same case here, where we feed fake generated samples, and we penalize the high critic score (by the negative sign), so the generator gets updated to fool the critic to give low score and get fooled (lower distance means same as real distribution)\n","\n","Where the average scores are calculated across a mini-batch of samples.\n","\n","This is precisely how the loss is implemented for graph-based deep learning frameworks such as PyTorch and TensorFlow.\n","\n","The calculations are straightforward to interpret once we recall that stochastic gradient descent seeks to minimize loss.\n","\n","In the case of the generator, a larger score from the critic will result in a smaller loss for the generator, encouraging the critic to output larger scores for fake images. For example, an average score of 10 becomes -10, an average score of 50 becomes -50, which is smaller, and so on.\n","\n","In the case of the critic, a larger score for real images results in a larger resulting loss for the critic, penalizing the model. This encourages the critic to output smaller scores for real images. For example, an average score of 20 for real images and 50 for fake images results in a loss of -30; an average score of 10 for real images and 50 for fake images results in a loss of -40, which is better, and so on.\n","\n","The sign of the loss does not matter in this case, as long as loss for real images is a small number and the loss for fake images is a large number. The Wasserstein loss encourages the critic to separate these numbers.\n","\n","We can also reverse the situation and encourage the critic to output a large score for real images and a small score for fake images and achieve the same result. Some implementations make this change.\n","\n","# Keras as high level API framework\n","In the Keras deep learning library (and some others), we cannot implement the Wasserstein loss function directly as described in the paper and as implemented in PyTorch and TensorFlow. Instead, we can achieve the same effect without having the calculation of the loss for the critic dependent upon the loss calculated for real and fake images.\n","\n","A good way to think about this is a __negative score for real images__ and a __positive score for fake images__, although this negative/positive split of scores learned during training is not required; __just larger and smaller is sufficient.__\n","\n","- Small Critic Score (e.g.< 0): Real – Large Critic Score (e.g. >0): Fake\n","\n","We can multiply the __average predicted score by -1__ in the case of __fake__ images so that larger averages become smaller averages and the gradient is in the correct direction, i.e. minimizing loss. For example, average scores on fake images of [0.5, 0.8, and 1.0] across three batches of fake images would become [-0.5, -0.8, and -1.0] when calculating weight updates.\n","\n","- Loss For Fake Images = -1 * Average Critic Score\n","\n","_No change is needed for the case of real scores, as we want to encourage smaller average scores for real images._\n","\n","- Loss For Real Images = Average Critic Score\n","\n","This can be implemented consistently by assigning an ___expected outcome target of -1 for fake images and 1 for real images___ and implementing the __loss__ function as the ___expected label multiplied by the average score___. The -1 label will be multiplied by the average score for fake images and encourage a larger predicted average, and the +1 label will be multiplied by the average score for real images and have no effect, encouraging a smaller predicted average.\n","\n","Wasserstein Loss = Label * Average Critic Score\n","Or\n","\n","- Wasserstein Loss(Real Images) = 1 * Average Predicted Score\n","- Wasserstein Loss(Fake Images) = -1 * Average Predicted Score\n","\n","__We can implement this in Keras by assigning the expected labels of -1 and 1 for fake and real images respectively.__ \n","\n","The __inverse labels could be used to the same effect__, e.g. -1 for real and +1 for fake to encourage small scores for fake images and large scores for real images. Some developers do implement the WGAN in this alternate way, which is just as correct.\n","\n","___The loss function can be implemented by multiplying the expected label for each sample by the predicted score (element wise), then calculating the mean.___\n","\n","```\n","def wasserstein_loss(y_true, y_pred):\n","\treturn mean(y_true * y_pred)\n","```\n","\n","The above function is the elegant way to implement the loss function; an alternative, less-elegant implementation that might be more intuitive is as follows:\n","\n","```\n","def wasserstein_loss(y_true, y_pred):\n"," \treturn mean(y_true) * mean(y_pred)\n","```\n","\n","In Keras, the mean function can be implemented using the Keras __backend__ API to ensure the mean is calculated across samples in the provided __tensors__; for example:\n","\n","```\n","from keras import backend\n","\n","# implementation of wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","\treturn backend.mean(y_true * y_pred)\n","```\n","\n","To understand it better, recall the WGAN algo above, in the loss, any real input is a positive term, while any fake is a negative term. The loss is just the mean of all the critic predictions then.\n","\n","Now that we know how to implement the Wasserstein loss function in Keras, let’s clarify one common point of misunderstanding."]},{"cell_type":"markdown","metadata":{"id":"aoohYfcejHtU","colab_type":"text"},"source":["# Common Point of Confusion With Expected Labels\n","Recall we are using the expected labels of -1 for fake images and +1 for real images.\n","\n","A common point of confusion is that a perfect critic model will output -1 for every fake image and +1 for every real image.\n","\n","___This is incorrect.___\n","\n","Again, recall we are using stochastic gradient descent to find the set of weights in the critic (and generator) models that minimize the loss function.\n","\n","We have established that we want the critic model to output larger scores on average for fake images and smaller scores on average for real images. We then designed a loss function to encourage this outcome.\n","\n","This is the key point about loss functions used to train neural network models. They encourage a desired model behavior, and they do not have to achieve this by providing the expected outcomes. In this case, we defined our Wasserstein loss function to interpret the average score predicted by the critic model and used labels for the real and fake cases to help with this interpretation.\n","\n","__So what is a good loss for real and fake images under Wasserstein loss?__\n","\n","Wasserstein is not an absolute and comparable loss for comparing across GAN models. Instead, it is relative and depends on your model configuration and dataset. What is important is that it is consistent for a given critic model and ___convergence of the generator (better loss) does correlate with better generated image quality.___\n","\n","It could be negative scores for real images and positive scores for fake images, but this is not required. All scores could be positive or all scores could be negative.\n","\n","_The loss function only encourages a separation between scores for fake and real images as larger and smaller, not necessarily positive and negative._"]},{"cell_type":"markdown","metadata":{"id":"ZaPCtGBBVZQf","colab_type":"text"},"source":["# How to Develop a Wasserstein Generative Adversarial Network (WGAN) From Scratch\n","[Source](https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/)\n","\n","To summarize, the differences in implementation for the WGAN are as follows:\n","\n","- Use a linear activation function in the output layer of the critic model (instead of sigmoid).\n","- Use -1 labels for real images and 1 labels for fake images (instead of 1 and 0).\n","- Use Wasserstein loss to train the critic and generator models.\n","- Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).\n","- Update the critic model more times than the generator each iteration (e.g. 5).\n","- Use the RMSProp version of gradient descent with a small learning rate and no momentum (e.g. 0.00005).\n","- Using the standard DCGAN model as a starting point, let’s take a look at each of these implementation details in turn."]},{"cell_type":"markdown","metadata":{"id":"HPnK9nXmk5dd","colab_type":"text"},"source":["## Linear Activation in Critic Output Layer\n","The DCGAN uses the sigmoid activation function in the output layer of the discriminator to predict the likelihood of a given image being real.\n","\n","In the WGAN, the critic model requires a linear activation to predict the score of “realness” for a given image.\n","\n","This can be achieved by setting the ‘activation‘ argument to ‘linear‘ in the output layer of the critic model.\n","```\n","# define output layer of the critic model\n","...\n","model.add(Dense(1, activation='linear'))\n","```\n","\n","The linear activation is the default activation for a layer, so we can, in fact, leave the activation unspecified to achieve the same result.\n","\n","```\n","# define output layer of the critic model\n","...\n","model.add(Dense(1))\n","```"]},{"cell_type":"markdown","metadata":{"id":"yztWfLTLlIr2","colab_type":"text"},"source":["## Class Labels for Real and Fake Images\n","The DCGAN uses the class 0 for fake images and class 1 for real images, and these class labels are used to train the GAN.\n","\n","In the DCGAN, these are precise labels that the discriminator is expected to achieve. \n","___The WGAN does not have precise labels for the critic.___ \n","\n","__Instead, it encourages the critic to output scores that are different for real and fake images.__\n","\n","This is achieved via the Wasserstein function that cleverly makes use of positive and negative class labels.\n","\n","The WGAN can be implemented where -1 class labels are used for real images and +1 class labels are used for fake or generated images.\n","\n","This can be achieved using the ones() NumPy function.\n","\n","For example:\n","\n","```\n","...\n","# generate class labels, -1 for 'real'\n","y = -ones((n_samples, 1))\n","...\n","# create class labels with 1.0 for 'fake'\n","y = ones((n_samples, 1))\n","```"]},{"cell_type":"markdown","metadata":{"id":"BE3K_nvZltKT","colab_type":"text"},"source":["# Wasserstein Loss Function\n","The DCGAN trains the discriminator as a binary classification model to predict the probability that a given image is real.\n","\n","To train this model, the discriminator is optimized using the binary cross entropy loss function. The same loss function is used to update the generator model.\n","\n","The primary contribution of the WGAN model is the use of a new loss function that encourages the discriminator to predict a score of ___how real or fake a given input looks___. This transforms the role of the discriminator from a classifier into a critic for scoring the realness or fakeness of images, where the difference between the scores is as large as possible.\n","\n","We can implement the Wasserstein loss as a custom function in Keras that calculates the average score for real or fake images.\n","\n","The score is maximizing for real examples and minimizing for fake examples. Given that stochastic gradient descent is a minimization algorithm, we can multiply the class label by the mean score (e.g. -1 for real and 1 for fake which as no effect), which ensures that the loss for real and fake images is minimizing to the network.\n","\n","An efficient implementation of this loss function for Keras is listed below."]},{"cell_type":"code","metadata":{"id":"c6WapkCvFxVY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"36db27d7-3807-43d1-a84b-b726e922214f","executionInfo":{"status":"ok","timestamp":1567085902497,"user_tz":-120,"elapsed":2508,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["from keras import backend\n","\n","# implementation of wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","\treturn backend.mean(y_true * y_pred)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bJ6rhTrXmTwy","colab_type":"text"},"source":["This loss function can be used to train a Keras model by specifying the function name when compiling the model.\n","\n","For example:\n","```\n","...\n","# compile the model\n","model.compile(loss=wasserstein_loss, ...)\n","```"]},{"cell_type":"markdown","metadata":{"id":"ac8dKWzMmRzg","colab_type":"text"},"source":["# Critic Weight Clipping\n","The DCGAN does not use any gradient clipping, although the WGAN requires gradient clipping for the critic model.\n","\n","We can implement weight clipping as a __Keras constraint.__\n","\n","This is a class that must extend the Constraint class and define an implementation of the __call__() function for applying the operation and the get_config() function for returning any configuration.\n","\n","We can also define an __init__() function to set the configuration, in this case, the symmetrical size of the bounding box for the weight hypercube, e.g. 0.01.\n","\n","The ClipConstraint class is defined below.\n","\n","```\n","# clip model weights to a given hypercube\n","class ClipConstraint(Constraint):\n","\t# set clip value when initialized\n","\tdef __init__(self, clip_value):\n","\t\tself.clip_value = clip_value\n","\n","\t# clip model weights to hypercube\n","\tdef __call__(self, weights):\n","\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n","\n","\t# get the config\n","\tdef get_config(self):\n","\t\treturn {'clip_value': self.clip_value}\n","```\n","\n","To use the constraint, the class can be constructed, then used in a layer by setting the ___```kernel_constraint```___ argument; for example:\n","\n","```\n","...\n","# define the constraint\n","const = ClipConstraint(0.01)\n","...\n","# use the constraint in a layer\n","model.add(Conv2D(..., kernel_constraint=const))\n","```\n","\n","The constraint is only required when updating the critic model."]},{"cell_type":"markdown","metadata":{"id":"_wNd15KPnIOm","colab_type":"text"},"source":["# Update Critic More Than Generator\n","In the DCGAN, the generator and the discriminator model must be updated in equal amounts.\n","\n","Specifically, the discriminator is updated with a half batch of real and a half batch of fake samples each iteration, whereas the generator is updated with a single batch of generated samples.\n","\n","For example:\n","```\n","...\n","# main gan training loop\n","for i in range(n_steps):\n","\n","\t# update the discriminator\n","\n","\t# get randomly selected 'real' samples\n","\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t# update critic model weights\n","\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t# generate 'fake' examples\n","\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t# update critic model weights\n","\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\n","\t# update generator\n","\n","\t# prepare points in latent space as input for the generator\n","\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t# create inverted labels for the fake samples\n","\ty_gan = ones((n_batch, 1))\n","\t# update the generator via the critic's error\n","\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","```"]},{"cell_type":"markdown","metadata":{"id":"1HBZnQw5nXVm","colab_type":"text"},"source":["In the WGAN model, the critic model must be updated more than the generator model.\n","\n","Specifically, a new hyperparameter is defined to control the number of times that the critic is updated for each update to the generator model, called n_critic, and is set to 5.\n","\n","This can be implemented as a new loop within the main GAN update loop; for example:\n","```\n","...\n","# main gan training loop\n","for i in range(n_steps):\n","\n","\t# update the critic\n","\tfor _ in range(n_critic):\n","\t\t# get randomly selected 'real' samples\n","\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t\t# update critic model weights\n","\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t\t# generate 'fake' examples\n","\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t# update critic model weights\n","\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\n","\t# update generator\n","\n","\t# prepare points in latent space as input for the generator\n","\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t# create inverted labels for the fake samples\n","\ty_gan = ones((n_batch, 1))\n","\t# update the generator via the critic's error\n","\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","```"]},{"cell_type":"markdown","metadata":{"id":"HNAmly9AnhEh","colab_type":"text"},"source":["# Use RMSProp Stochastic Gradient Descent\n","The DCGAN uses the Adam version of stochastic gradient descent with a small learning rate and modest momentum.\n","\n","The WGAN recommends the use of RMSProp instead, with a small learning rate of 0.00005.\n","\n","This can be implemented in Keras when the model is compiled. For example:\n","```\n","...\n","# compile model\n","opt = RMSprop(lr=0.00005)\n","model.compile(loss=wasserstein_loss, optimizer=opt)\n","```"]},{"cell_type":"markdown","metadata":{"id":"T7As44gannBb","colab_type":"text"},"source":["# How to Train a Wasserstein GAN Model\n","Now that we know the specific implementation details for the WGAN, we can implement the model for image generation.\n","\n","## MNIST\n","In this section, we will develop a WGAN to generate a single handwritten digit (‘7’) from the MNIST dataset. This is a good test problem for the WGAN as it is a small dataset requiring a modest mode that is quick to train.\n","\n","The first step is to define the models.\n","\n","## Critic\n","The critic model takes as input one 28×28 grayscale image and outputs a score for the realness or fakeness of the image. It is implemented as a modest convolutional neural network using best practices for DCGAN design such as using the LeakyReLU activation function with a slope of 0.2, batch normalization, and using a 2×2 stride to downsample.\n","\n","The critic model makes use of the new ClipConstraint weight constraint to clip model weights after mini-batch updates and is optimized using the custom wasserstein_loss() function, the RMSProp version of stochastic gradient descent with a learning rate of 0.00005.\n","\n","The `define_critic()` function below implements this, defining and compiling the critic model and returning it. The input shape of the image is parameterized as a default function argument to make it clear."]},{"cell_type":"code","metadata":{"id":"P0Fa7GKgn19x","colab_type":"code","colab":{}},"source":["# define the standalone critic model\n","def define_critic(in_shape=(28,28,1)):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# weight constraint\n","\tconst = ClipConstraint(0.01)\n","\t# define model\n","\tmodel = Sequential()\n","\t# downsample to 14x14\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# downsample to 7x7\n","\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# scoring, linear activation\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(1))\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ax3JaB6mn4ll","colab_type":"text"},"source":["# Generator\n","The generator model takes as input a point in the latent space and outputs a single 28×28 grayscale image.\n","\n","This is achieved by using a fully connected layer to interpret the point in the latent space and provide sufficient activations that can be reshaped into many copies (in this case, 128) of a low-resolution version of the output image (e.g. 7×7). This is then upsampled two times, doubling the size and quadrupling the area of the activations each time using transpose convolutional layers.\n","\n","The model uses best practices such as the LeakyReLU activation, a kernel size that is a factor of the stride size, and a hyperbolic tangent (tanh) activation function in the output layer.\n","\n","The `define_generator()` function below defines the __generator model__ but intentionally __does not compile__ it as it is not trained directly, then returns the model. The size of the latent space is parameterized as a function argument."]},{"cell_type":"code","metadata":{"id":"agdvv7Uyn8_s","colab_type":"code","colab":{}},"source":["# define the standalone generator model\n","def define_generator(latent_dim):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# define model\n","\tmodel = Sequential()\n","\t# foundation for 7x7 image\n","\tn_nodes = 128 * 7 * 7\n","\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\tmodel.add(Reshape((7, 7, 128)))\n","\t# upsample to 14x14\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# upsample to 28x28\n","\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(LeakyReLU(alpha=0.2))\n","\t# output 28x28x1\n","\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n","\treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsumHJCAn-Cb","colab_type":"text"},"source":["# WGAN\n","Next, a GAN model can be defined that combines both the generator model and the critic model into one larger model.\n","\n","This larger model will be used to train the model weights in the generator, using the output and error calculated by the critic model. The __critic model__ is trained separately, and as such, the model weights are marked as __not trainable__ in this larger GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the critic weights only has an effect when training the combined GAN model, not when training the critic standalone.\n","\n","This larger GAN model takes as input a point in the latent space, uses the generator model to generate an image, which is fed as input to the critic model, then output scored as real or fake. The model is fit using RMSProp with the custom `wasserstein_loss()` function.\n","\n","The `define_gan()` function below implements this, taking the already defined generator and critic models as input."]},{"cell_type":"code","metadata":{"id":"SyzQtQqFohDS","colab_type":"code","colab":{}},"source":["# define the combined generator and critic model, for updating the generator\n","def define_gan(generator, critic):\n","\t# make weights in the critic not trainable\n","\tcritic.trainable = False\n","\t# connect them\n","\tmodel = Sequential()\n","\t# add generator\n","\tmodel.add(generator)\n","\t# add the critic\n","\tmodel.add(critic)\n","\t# compile model\n","\topt = RMSprop(lr=0.00005)\n","\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n","\treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-VcVq9YonWb","colab_type":"text"},"source":["Now that we have defined the GAN model, we need to train it. But, before we can train the model, we require input data.\n","\n","## Data preparation\n","The first step is to load and scale the MNIST dataset. The whole dataset is loaded via a call to the load_data() Keras function, then a __subset__ of the images is selected (about 5,000) that belongs to __class 7__, e.g. are a handwritten depiction of the number seven. Then the pixel values must be scaled to the range [-1,1] to match the output of the generator model.\n","\n","The `load_real_samples()` function below implements this, returning the loaded and scaled subset of the MNIST training dataset ready for modeling."]},{"cell_type":"code","metadata":{"id":"2tlqWAbhoxAc","colab_type":"code","colab":{}},"source":["# load images\n","def load_real_samples():\n","\t# load dataset\n","\t(trainX, trainy), (_, _) = load_data()\n","\t# select all of the examples for a given class\n","\tselected_ix = trainy == 7\n","\tX = trainX[selected_ix]\n","\t# expand to 3d, e.g. add channels\n","\tX = expand_dims(X, axis=-1)\n","\t# convert from ints to floats\n","\tX = X.astype('float32')\n","\t# scale from [0,255] to [-1,1]\n","\tX = (X - 127.5) / 127.5\n","\treturn X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6C6oymrOozMA","colab_type":"text"},"source":["We will require one batch (or a half) batch of real images from the dataset each update to the GAN model. A simple way to achieve this is to __select a random sample of images from the dataset each time__.\n","\n","The `generate_real_samples()` function below implements this, taking the prepared dataset as an argument, selecting and returning a random sample of images and their corresponding label for the critic, specifically target=-1 indicating that they are real images."]},{"cell_type":"code","metadata":{"id":"KqJNDXPCo2Tr","colab_type":"code","colab":{}},"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# select images\n","\tX = dataset[ix]\n","\t# generate class labels, -1 for 'real'\n","\ty = -ones((n_samples, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MuB3fE-So6jR","colab_type":"text"},"source":["Next, we need inputs for the generator model. These are random points from the latent space, specifically Gaussian distributed random variables.\n","\n","The `generate_latent_points()` function implements this, taking the size of the latent space as an argument and the number of points required, and returning them as a batch of input samples for the generator model."]},{"cell_type":"code","metadata":{"id":"oGqBsVGgo9aY","colab_type":"code","colab":{}},"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tx_input = x_input.reshape(n_samples, latent_dim)\n","\treturn x_input"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_aK0Wj_pDHr","colab_type":"text"},"source":["Next, we need to use the points in the latent space as input to the generator in order to generate new images.\n","\n","The `generate_fake_samples()` function below implements this, taking the generator model and size of the latent space as arguments, then generating points in the latent space and using them as input to the generator model.\n","\n","The function returns the generated images and their __corresponding label for the critic model__, specifically target=1 to indicate they are fake or generated."]},{"cell_type":"code","metadata":{"id":"Iuc1pCUnpKWU","colab_type":"code","colab":{}},"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tx_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\tX = generator.predict(x_input)\n","\t# create class labels with 1.0 for 'fake'\n","\ty = ones((n_samples, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vNpdk9ppRgG","colab_type":"text"},"source":["We need to record the performance of the model. Perhaps the most reliable way to evaluate the performance of a GAN is to use the generator to generate images, and then review and subjectively evaluate them.\n","\n","The `summarize_performance()` function below takes the generator model at a given point during training and uses it to generate 100 images in a 10×10 grid, that are then plotted and saved to file. The model is also saved to file at this time, in case we would like to use it later to generate more images."]},{"cell_type":"code","metadata":{"id":"FXRcT1YZpVKt","colab_type":"code","colab":{}},"source":["# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, latent_dim, n_samples=100):\n","\t# prepare fake examples\n","\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n","\t# scale from [-1,1] to [0,1]\n","\tX = (X + 1) / 2.0\n","\t# plot images\n","\tfor i in range(10 * 10):\n","\t\t# define subplot\n","\t\tpyplot.subplot(10, 10, 1 + i)\n","\t\t# turn off axis\n","\t\tpyplot.axis('off')\n","\t\t# plot raw pixel data\n","\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n","\t# save plot to file\n","\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()\n","\t# save the generator model\n","\tfilename2 = 'model_%04d.h5' % (step+1)\n","\tg_model.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GBpkA4AhpXWo","colab_type":"text"},"source":["In addition to image quality, it is a good idea to keep track of the loss and accuracy of the model over time.\n","\n","The loss for the critic for real and fake samples can be tracked for each model update, as can the loss for the generator for each update. These can then be used to create line plots of loss at the end of the training run. The `plot_history()` function below implements this and saves the results to file."]},{"cell_type":"code","metadata":{"id":"wH4gRonJpbBU","colab_type":"code","colab":{}},"source":["# create a line plot of loss for the gan and save to file\n","def plot_history(d1_hist, d2_hist, g_hist):\n","\t# plot history\n","\tpyplot.plot(d1_hist, label='crit_real')\n","\tpyplot.plot(d2_hist, label='crit_fake')\n","\tpyplot.plot(g_hist, label='gen')\n","\tpyplot.legend()\n","\tpyplot.savefig('plot_line_plot_loss.png')\n","\tpyplot.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qUXXFvVUphWp","colab_type":"text"},"source":["We are now ready to fit the GAN model.\n","\n","The model is fit for 10 training epochs, which is arbitrary, as the model begins generating plausible number-7 digits after perhaps the first few epochs. A batch size of 64 samples is used, and each training epoch involves 6,265/64, or about 97, batches of real and fake samples and updates to the model. The model is therefore trained for 10 epochs of 97 batches, or 970 iterations.\n","\n","First, the critic model is updated for a half batch of real samples, then a half batch of fake samples, together forming one batch of weight updates. This is then repeated n_critic (5) times as required by the WGAN algorithm.\n","\n","__Inverse labels for the composite model__\n","\n","The generator is then updated via the composite GAN model. __Importantly, the target label is set to -1 or real for the generated samples__. \n","\n","___This has the effect of updating the generator toward getting better at generating real samples on the next batch.___\n","\n","___Note that___\n","This might seem the opposite to what stated in the WGAN algo above, since the when we update the generator, we used the \"fake\" sign (-1 in that case) not the \"real\" one. But take care, in the algo above we used to maximize the objective (- sign in the gradient update), while here in the `train` function, we minimize it.\n","The point is, in both cases, we want to update the generator such that it is penalized when the critic is NOT fooled. So we feed the critic the generated samples, and mark them as \"real\" (-1 in our case), and if the critic gives low score then it's fooled and no need to update the generator, while if it gives high score, then it's not fooled, and then the generator needs to be updated.\n","\n","The `train()` function below implements this, taking the defined models, dataset, and size of the latent dimension as arguments and parameterizing the number of epochs and batch size with default arguments. The generator model is saved at the end of training.\n","\n","The performance of the critic and generator models is reported each iteration. Sample images are generated and saved every epoch, and line plots of model performance are created and saved at the end of the run."]},{"cell_type":"code","metadata":{"id":"O_SUBhOzqdKs","colab_type":"code","colab":{}},"source":["# train the generator and critic\n","def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(dataset.shape[0] / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","\t# calculate the size of half a batch of samples\n","\thalf_batch = int(n_batch / 2)\n","\t# lists for keeping track of loss\n","\tc1_hist, c2_hist, g_hist = list(), list(), list()\n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# update the critic more than the generator\n","\t\tc1_tmp, c2_tmp = list(), list()\n","\t\tfor _ in range(n_critic):\n","\t\t\t# get randomly selected 'real' samples\n","\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n","\t\t\tc1_tmp.append(c_loss1)\n","\t\t\t# generate 'fake' examples\n","\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","\t\t\t# update critic model weights\n","\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n","\t\t\tc2_tmp.append(c_loss2)\n","\t\t# store critic loss\n","\t\tc1_hist.append(mean(c1_tmp))\n","\t\tc2_hist.append(mean(c2_tmp))\n","\t\t# prepare points in latent space as input for the generator\n","\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n","\t\t# create inverted labels for the fake samples\n","\t\ty_gan = -ones((n_batch, 1))\n","\t\t# update the generator via the critic's error\n","\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n","\t\tg_hist.append(g_loss)\n","\t\t# summarize loss on this batch\n","\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n","\t\t# evaluate the model performance every 'epoch'\n","\t\tif (i+1) % bat_per_epo == 0:\n","\t\t\tsummarize_performance(i, g_model, latent_dim)\n","\t# line plots of loss\n","\tplot_history(c1_hist, c2_hist, g_hist)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgKK2jLvqrKk","colab_type":"text"},"source":["Now that all of the functions have been defined, we can create the models, load the dataset, and begin the training process."]},{"cell_type":"code","metadata":{"id":"Rh40_ayzqrm2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3c22d564-2feb-4ba8-eb9d-f9505de2bed4","executionInfo":{"status":"ok","timestamp":1567087261812,"user_tz":-120,"elapsed":166047,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["# example of a wgan for generating handwritten digits\n","from numpy import expand_dims\n","from numpy import mean\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.mnist import load_data\n","from keras import backend\n","from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.initializers import RandomNormal\n","from keras.constraints import Constraint\n","from matplotlib import pyplot\n","\n","# clip model weights to a given hypercube\n","class ClipConstraint(Constraint):\n","\t# set clip value when initialized\n","\tdef __init__(self, clip_value):\n","\t\tself.clip_value = clip_value\n","\n","\t# clip model weights to hypercube\n","\tdef __call__(self, weights):\n","\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n","\n","\t# get the config\n","\tdef get_config(self):\n","\t\treturn {'clip_value': self.clip_value}\n","\n","# calculate wasserstein loss\n","def wasserstein_loss(y_true, y_pred):\n","\treturn backend.mean(y_true * y_pred)\n","\n","# size of the latent space\n","latent_dim = 50\n","# create the critic\n","critic = define_critic()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, critic)\n","# load image data\n","dataset = load_real_samples()\n","print(dataset.shape)\n","# train model\n","train(generator, critic, gan_model, dataset, latent_dim)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0829 13:58:15.825289 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0829 13:58:15.880085 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0829 13:58:15.887164 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","W0829 13:58:15.933376 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0829 13:58:15.934643 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0829 13:58:19.522133 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0829 13:58:19.735710 140646808881024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n","(6265, 28, 28, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":[">1, c1=-3.189, c2=-0.014 g=0.140\n",">2, c1=-7.878, c2=0.014 g=0.971\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":[">3, c1=-10.536, c2=-0.585 g=1.019\n",">4, c1=-11.809, c2=-0.310 g=0.948\n",">5, c1=-13.267, c2=-0.282 g=0.711\n",">6, c1=-14.437, c2=-0.132 g=0.629\n",">7, c1=-15.796, c2=-0.484 g=0.368\n",">8, c1=-16.586, c2=-0.259 g=0.398\n",">9, c1=-17.957, c2=-0.335 g=-0.049\n",">10, c1=-18.284, c2=-0.352 g=-0.024\n",">11, c1=-18.366, c2=-0.345 g=-0.529\n",">12, c1=-20.076, c2=-0.185 g=-1.085\n",">13, c1=-19.624, c2=-0.138 g=-1.957\n",">14, c1=-20.588, c2=-0.266 g=-2.530\n",">15, c1=-21.053, c2=0.053 g=-3.142\n",">16, c1=-21.228, c2=-0.123 g=-4.056\n",">17, c1=-21.307, c2=-0.393 g=-4.903\n",">18, c1=-21.727, c2=-0.039 g=-5.082\n",">19, c1=-21.183, c2=-0.038 g=-5.261\n",">20, c1=-21.732, c2=-0.112 g=-4.768\n",">21, c1=-21.059, c2=-0.701 g=-4.229\n",">22, c1=-21.265, c2=-0.786 g=-3.548\n",">23, c1=-21.614, c2=-1.351 g=-2.527\n",">24, c1=-21.805, c2=-1.977 g=-1.012\n",">25, c1=-21.255, c2=-2.822 g=-0.010\n",">26, c1=-21.056, c2=-3.216 g=1.462\n",">27, c1=-21.540, c2=-4.062 g=2.640\n",">28, c1=-21.432, c2=-4.982 g=4.141\n",">29, c1=-20.803, c2=-5.984 g=5.275\n",">30, c1=-21.647, c2=-6.434 g=6.460\n",">31, c1=-21.140, c2=-6.958 g=7.601\n",">32, c1=-21.280, c2=-7.192 g=8.332\n",">33, c1=-22.199, c2=-7.628 g=9.108\n",">34, c1=-21.682, c2=-7.581 g=9.786\n",">35, c1=-21.945, c2=-7.637 g=9.839\n",">36, c1=-22.251, c2=-7.968 g=9.992\n",">37, c1=-22.498, c2=-7.857 g=9.821\n",">38, c1=-22.685, c2=-7.727 g=9.045\n",">39, c1=-23.070, c2=-7.925 g=8.981\n",">40, c1=-22.816, c2=-8.203 g=8.450\n",">41, c1=-22.679, c2=-8.768 g=7.485\n",">42, c1=-22.711, c2=-8.534 g=6.684\n",">43, c1=-22.819, c2=-9.613 g=5.776\n",">44, c1=-23.443, c2=-9.701 g=5.025\n",">45, c1=-23.126, c2=-9.996 g=4.813\n",">46, c1=-23.448, c2=-9.739 g=5.151\n",">47, c1=-23.480, c2=-10.093 g=5.993\n",">48, c1=-24.701, c2=-10.304 g=6.672\n",">49, c1=-23.135, c2=-10.387 g=7.175\n",">50, c1=-24.237, c2=-11.200 g=7.858\n",">51, c1=-24.431, c2=-10.899 g=9.225\n",">52, c1=-23.641, c2=-11.770 g=10.495\n",">53, c1=-23.917, c2=-11.698 g=11.702\n",">54, c1=-25.128, c2=-12.202 g=12.479\n",">55, c1=-24.177, c2=-12.507 g=12.927\n",">56, c1=-24.739, c2=-12.717 g=12.785\n",">57, c1=-24.957, c2=-12.684 g=12.494\n",">58, c1=-25.277, c2=-13.289 g=12.306\n",">59, c1=-24.657, c2=-13.843 g=11.513\n",">60, c1=-26.174, c2=-14.094 g=10.966\n",">61, c1=-26.250, c2=-13.978 g=10.210\n",">62, c1=-25.503, c2=-13.364 g=10.572\n",">63, c1=-25.664, c2=-14.746 g=11.536\n",">64, c1=-24.628, c2=-13.037 g=11.164\n",">65, c1=-25.800, c2=-16.886 g=11.643\n",">66, c1=-25.362, c2=-12.845 g=11.473\n",">67, c1=-27.366, c2=-14.626 g=11.487\n",">68, c1=-26.579, c2=-15.037 g=10.238\n",">69, c1=-26.261, c2=-15.733 g=9.790\n",">70, c1=-27.887, c2=-18.559 g=9.338\n",">71, c1=-27.307, c2=-15.369 g=9.053\n",">72, c1=-26.680, c2=-15.512 g=10.331\n",">73, c1=-27.350, c2=-16.214 g=9.495\n",">74, c1=-27.337, c2=-15.830 g=9.843\n",">75, c1=-26.041, c2=-14.267 g=9.981\n",">76, c1=-28.315, c2=-16.582 g=11.572\n",">77, c1=-27.990, c2=-15.886 g=10.694\n",">78, c1=-28.236, c2=-17.846 g=9.749\n",">79, c1=-28.481, c2=-18.423 g=9.405\n",">80, c1=-28.995, c2=-18.790 g=9.226\n",">81, c1=-29.396, c2=-17.018 g=9.359\n",">82, c1=-28.705, c2=-15.956 g=10.293\n",">83, c1=-28.146, c2=-15.854 g=9.592\n",">84, c1=-29.969, c2=-18.606 g=9.970\n",">85, c1=-28.444, c2=-15.753 g=11.043\n",">86, c1=-28.498, c2=-15.908 g=11.705\n",">87, c1=-27.152, c2=-16.827 g=11.519\n",">88, c1=-30.778, c2=-17.193 g=12.816\n",">89, c1=-28.817, c2=-15.532 g=13.308\n",">90, c1=-28.591, c2=-15.703 g=12.399\n",">91, c1=-28.698, c2=-17.364 g=14.723\n",">92, c1=-29.888, c2=-16.959 g=13.953\n",">93, c1=-29.643, c2=-18.353 g=15.651\n",">94, c1=-28.891, c2=-16.755 g=15.608\n",">95, c1=-28.011, c2=-19.808 g=13.492\n",">96, c1=-28.749, c2=-17.523 g=15.956\n",">97, c1=-30.524, c2=-17.174 g=14.995\n",">Saved: generated_plot_0097.png and model_0097.h5\n",">98, c1=-31.138, c2=-19.122 g=18.047\n",">99, c1=-30.466, c2=-18.459 g=18.078\n",">100, c1=-29.975, c2=-17.679 g=19.168\n",">101, c1=-28.760, c2=-18.423 g=17.770\n",">102, c1=-29.165, c2=-19.600 g=18.488\n",">103, c1=-30.760, c2=-19.888 g=19.355\n",">104, c1=-29.884, c2=-18.647 g=19.162\n",">105, c1=-29.878, c2=-19.257 g=20.306\n",">106, c1=-28.326, c2=-18.402 g=20.440\n",">107, c1=-30.852, c2=-19.086 g=21.613\n",">108, c1=-29.192, c2=-18.118 g=20.952\n",">109, c1=-31.000, c2=-19.743 g=21.918\n",">110, c1=-29.666, c2=-19.476 g=23.343\n",">111, c1=-30.939, c2=-19.813 g=23.398\n",">112, c1=-29.376, c2=-20.138 g=24.057\n",">113, c1=-31.091, c2=-20.121 g=23.852\n",">114, c1=-29.119, c2=-20.720 g=23.617\n",">115, c1=-29.255, c2=-20.377 g=24.758\n",">116, c1=-28.603, c2=-19.923 g=23.351\n",">117, c1=-29.783, c2=-20.820 g=24.480\n",">118, c1=-31.342, c2=-19.393 g=27.523\n",">119, c1=-29.360, c2=-19.755 g=25.649\n",">120, c1=-29.018, c2=-19.839 g=28.639\n",">121, c1=-29.305, c2=-20.785 g=27.578\n",">122, c1=-30.537, c2=-20.871 g=24.867\n",">123, c1=-28.348, c2=-20.067 g=26.200\n",">124, c1=-30.425, c2=-21.905 g=26.412\n",">125, c1=-30.423, c2=-20.017 g=27.351\n",">126, c1=-29.662, c2=-20.895 g=25.158\n",">127, c1=-29.142, c2=-20.382 g=26.804\n",">128, c1=-28.618, c2=-18.657 g=26.402\n",">129, c1=-29.892, c2=-18.730 g=26.350\n",">130, c1=-29.286, c2=-19.897 g=27.738\n",">131, c1=-28.944, c2=-19.388 g=25.231\n",">132, c1=-30.270, c2=-19.481 g=27.838\n",">133, c1=-28.297, c2=-20.269 g=23.358\n",">134, c1=-31.052, c2=-19.984 g=25.981\n",">135, c1=-29.344, c2=-19.190 g=27.294\n",">136, c1=-27.131, c2=-17.169 g=26.561\n",">137, c1=-30.453, c2=-19.478 g=27.395\n",">138, c1=-28.846, c2=-18.213 g=22.811\n",">139, c1=-28.284, c2=-16.859 g=24.902\n",">140, c1=-29.764, c2=-16.843 g=24.808\n",">141, c1=-29.822, c2=-15.880 g=25.835\n",">142, c1=-28.949, c2=-18.790 g=24.675\n",">143, c1=-29.409, c2=-17.323 g=18.625\n",">144, c1=-29.216, c2=-18.546 g=25.696\n",">145, c1=-30.116, c2=-13.531 g=22.638\n",">146, c1=-29.545, c2=-16.361 g=23.290\n",">147, c1=-30.221, c2=-18.255 g=24.711\n",">148, c1=-30.673, c2=-15.348 g=23.167\n",">149, c1=-29.698, c2=-19.373 g=23.785\n",">150, c1=-28.369, c2=-15.860 g=20.743\n",">151, c1=-29.744, c2=-13.019 g=16.287\n",">152, c1=-29.743, c2=-16.648 g=18.393\n",">153, c1=-32.182, c2=-17.008 g=19.811\n",">154, c1=-32.051, c2=-16.518 g=17.332\n",">155, c1=-32.602, c2=-16.294 g=17.074\n",">156, c1=-30.857, c2=-14.703 g=16.669\n",">157, c1=-31.771, c2=-20.924 g=14.154\n",">158, c1=-33.798, c2=-16.586 g=10.881\n",">159, c1=-32.187, c2=-16.954 g=8.630\n",">160, c1=-33.032, c2=-13.624 g=14.969\n",">161, c1=-32.223, c2=-17.924 g=12.032\n",">162, c1=-32.145, c2=-17.307 g=3.505\n",">163, c1=-33.880, c2=-20.814 g=1.670\n",">164, c1=-34.930, c2=-19.119 g=0.688\n",">165, c1=-36.735, c2=-16.866 g=0.466\n",">166, c1=-33.046, c2=-16.432 g=6.299\n",">167, c1=-34.499, c2=-11.173 g=3.812\n",">168, c1=-34.870, c2=-21.730 g=2.169\n",">169, c1=-36.824, c2=-16.182 g=0.510\n",">170, c1=-34.848, c2=-19.034 g=0.324\n",">171, c1=-35.447, c2=-17.062 g=-6.533\n",">172, c1=-34.575, c2=-18.881 g=-8.047\n",">173, c1=-36.669, c2=-18.082 g=-5.490\n",">174, c1=-35.204, c2=-17.431 g=-8.749\n",">175, c1=-36.763, c2=-16.717 g=-6.851\n",">176, c1=-37.582, c2=-20.585 g=-8.383\n",">177, c1=-37.239, c2=-18.978 g=-13.172\n",">178, c1=-36.844, c2=-17.224 g=-14.861\n",">179, c1=-37.196, c2=-17.575 g=-9.220\n",">180, c1=-36.463, c2=-13.763 g=-11.913\n",">181, c1=-38.522, c2=-20.640 g=-13.534\n",">182, c1=-37.974, c2=-20.308 g=-17.328\n",">183, c1=-40.283, c2=-21.406 g=-20.330\n",">184, c1=-38.089, c2=-16.859 g=-17.434\n",">185, c1=-38.641, c2=-15.084 g=-15.434\n",">186, c1=-38.833, c2=-22.277 g=-18.666\n",">187, c1=-36.536, c2=-19.781 g=-19.176\n",">188, c1=-42.286, c2=-26.367 g=-24.841\n",">189, c1=-40.724, c2=-21.068 g=-23.529\n",">190, c1=-39.181, c2=-17.413 g=-22.328\n",">191, c1=-41.586, c2=-22.190 g=-25.501\n",">192, c1=-41.635, c2=-17.591 g=-24.759\n",">193, c1=-39.564, c2=-23.059 g=-28.973\n",">194, c1=-44.356, c2=-22.720 g=-32.464\n",">Saved: generated_plot_0194.png and model_0194.h5\n",">195, c1=-42.997, c2=-22.776 g=-29.552\n",">196, c1=-42.102, c2=-27.207 g=-32.828\n",">197, c1=-45.283, c2=-17.472 g=-36.894\n",">198, c1=-45.022, c2=-24.079 g=-33.577\n",">199, c1=-42.176, c2=-23.307 g=-32.573\n",">200, c1=-40.534, c2=-19.469 g=-35.787\n",">201, c1=-39.890, c2=-20.759 g=-34.387\n",">202, c1=-40.848, c2=-15.288 g=-36.452\n",">203, c1=-38.668, c2=-15.840 g=-35.526\n",">204, c1=-41.244, c2=-11.300 g=-32.707\n",">205, c1=-40.173, c2=-13.740 g=-35.582\n",">206, c1=-41.563, c2=-16.365 g=-34.015\n",">207, c1=-41.084, c2=-14.298 g=-34.413\n",">208, c1=-38.158, c2=-13.141 g=-41.340\n",">209, c1=-41.594, c2=-17.493 g=-39.194\n",">210, c1=-36.347, c2=-14.217 g=-36.235\n",">211, c1=-39.220, c2=-11.008 g=-37.701\n",">212, c1=-38.296, c2=-11.233 g=-37.485\n",">213, c1=-38.041, c2=-11.175 g=-35.356\n",">214, c1=-38.901, c2=-11.112 g=-42.250\n",">215, c1=-38.109, c2=-6.283 g=-38.988\n",">216, c1=-36.182, c2=3.331 g=-40.541\n",">217, c1=-40.210, c2=-8.229 g=-38.612\n",">218, c1=-39.360, c2=1.951 g=-34.587\n",">219, c1=-36.000, c2=-1.191 g=-40.786\n",">220, c1=-37.806, c2=2.130 g=-37.913\n",">221, c1=-36.951, c2=-3.783 g=-36.177\n",">222, c1=-40.241, c2=7.115 g=-42.197\n",">223, c1=-36.214, c2=5.770 g=-37.184\n",">224, c1=-40.800, c2=7.459 g=-39.917\n",">225, c1=-37.988, c2=-0.412 g=-34.991\n",">226, c1=-38.260, c2=-2.391 g=-38.859\n",">227, c1=-38.704, c2=-2.102 g=-39.081\n",">228, c1=-37.794, c2=7.130 g=-38.818\n",">229, c1=-38.164, c2=7.987 g=-37.036\n",">230, c1=-38.919, c2=12.352 g=-41.478\n",">231, c1=-40.456, c2=8.310 g=-39.627\n",">232, c1=-39.344, c2=15.878 g=-36.832\n",">233, c1=-40.114, c2=11.433 g=-37.684\n",">234, c1=-40.702, c2=15.809 g=-34.406\n",">235, c1=-40.450, c2=10.664 g=-36.585\n",">236, c1=-42.356, c2=19.520 g=-36.110\n",">237, c1=-37.942, c2=12.416 g=-34.890\n",">238, c1=-39.368, c2=17.091 g=-33.115\n",">239, c1=-41.814, c2=17.558 g=-28.255\n",">240, c1=-41.163, c2=20.313 g=-30.953\n",">241, c1=-40.724, c2=17.048 g=-27.810\n",">242, c1=-40.510, c2=17.421 g=-30.821\n",">243, c1=-42.078, c2=18.418 g=-28.298\n",">244, c1=-42.903, c2=15.165 g=-30.206\n",">245, c1=-41.736, c2=16.811 g=-25.388\n",">246, c1=-40.508, c2=11.267 g=-28.729\n",">247, c1=-42.084, c2=9.687 g=-32.200\n",">248, c1=-40.075, c2=12.458 g=-32.750\n",">249, c1=-44.160, c2=12.376 g=-30.706\n",">250, c1=-43.707, c2=15.728 g=-26.552\n",">251, c1=-39.322, c2=5.271 g=-30.370\n",">252, c1=-40.285, c2=7.870 g=-27.877\n",">253, c1=-43.391, c2=9.324 g=-28.934\n",">254, c1=-43.633, c2=8.724 g=-26.822\n",">255, c1=-45.189, c2=13.683 g=-24.640\n",">256, c1=-42.962, c2=6.908 g=-25.203\n",">257, c1=-45.241, c2=9.180 g=-21.276\n",">258, c1=-46.078, c2=13.211 g=-14.786\n",">259, c1=-46.790, c2=11.147 g=-18.689\n",">260, c1=-46.247, c2=12.329 g=-16.036\n",">261, c1=-47.787, c2=12.579 g=-14.329\n",">262, c1=-48.543, c2=14.639 g=-13.425\n",">263, c1=-49.931, c2=15.888 g=-12.651\n",">264, c1=-49.055, c2=19.651 g=-13.786\n",">265, c1=-53.890, c2=17.154 g=-7.425\n",">266, c1=-51.443, c2=17.603 g=-12.252\n",">267, c1=-55.715, c2=18.810 g=-9.440\n",">268, c1=-55.483, c2=17.053 g=-7.381\n",">269, c1=-57.597, c2=15.361 g=-3.940\n",">270, c1=-57.441, c2=15.956 g=-3.472\n",">271, c1=-60.565, c2=14.755 g=-2.648\n",">272, c1=-60.868, c2=14.993 g=-1.518\n",">273, c1=-62.772, c2=14.277 g=1.351\n",">274, c1=-61.835, c2=11.194 g=2.724\n",">275, c1=-64.134, c2=5.302 g=4.548\n",">276, c1=-65.143, c2=2.417 g=8.933\n",">277, c1=-65.244, c2=0.218 g=9.953\n",">278, c1=-68.152, c2=-4.977 g=12.561\n",">279, c1=-68.997, c2=-11.979 g=15.819\n",">280, c1=-71.715, c2=-16.595 g=19.835\n",">281, c1=-72.399, c2=-22.049 g=23.840\n",">282, c1=-76.027, c2=-30.169 g=28.767\n",">283, c1=-76.720, c2=-37.362 g=36.114\n",">284, c1=-79.575, c2=-42.906 g=41.774\n",">285, c1=-82.594, c2=-48.979 g=48.841\n",">286, c1=-84.366, c2=-55.350 g=53.625\n",">287, c1=-84.755, c2=-60.000 g=60.276\n",">288, c1=-86.966, c2=-65.561 g=64.725\n",">289, c1=-88.923, c2=-69.353 g=68.180\n",">290, c1=-89.678, c2=-71.644 g=74.201\n",">291, c1=-91.796, c2=-75.459 g=76.945\n",">Saved: generated_plot_0291.png and model_0291.h5\n",">292, c1=-93.030, c2=-78.649 g=82.993\n",">293, c1=-95.129, c2=-81.569 g=84.750\n",">294, c1=-94.267, c2=-84.108 g=86.937\n",">295, c1=-96.476, c2=-87.378 g=90.116\n",">296, c1=-97.643, c2=-89.357 g=91.142\n",">297, c1=-98.517, c2=-91.718 g=93.384\n",">298, c1=-100.821, c2=-94.566 g=93.887\n",">299, c1=-102.164, c2=-95.601 g=95.741\n",">300, c1=-102.231, c2=-95.836 g=93.932\n",">301, c1=-103.620, c2=-97.487 g=94.716\n",">302, c1=-104.569, c2=-97.869 g=96.351\n",">303, c1=-104.740, c2=-101.755 g=97.134\n",">304, c1=-105.963, c2=-101.910 g=97.383\n",">305, c1=-107.357, c2=-104.351 g=98.322\n",">306, c1=-108.640, c2=-105.823 g=99.026\n",">307, c1=-109.736, c2=-106.511 g=100.658\n",">308, c1=-110.548, c2=-107.771 g=100.417\n",">309, c1=-110.269, c2=-112.161 g=102.760\n",">310, c1=-112.998, c2=-113.483 g=104.927\n",">311, c1=-113.466, c2=-115.506 g=103.511\n",">312, c1=-113.909, c2=-115.214 g=103.723\n",">313, c1=-114.062, c2=-117.256 g=105.343\n",">314, c1=-115.445, c2=-120.411 g=107.377\n",">315, c1=-117.642, c2=-120.257 g=107.250\n",">316, c1=-116.320, c2=-121.078 g=107.895\n",">317, c1=-119.557, c2=-123.785 g=108.981\n",">318, c1=-119.541, c2=-124.617 g=110.866\n",">319, c1=-119.964, c2=-126.952 g=110.645\n",">320, c1=-120.659, c2=-127.820 g=112.437\n",">321, c1=-122.185, c2=-129.970 g=113.549\n",">322, c1=-123.584, c2=-129.230 g=114.441\n",">323, c1=-125.185, c2=-132.356 g=116.998\n",">324, c1=-125.062, c2=-133.423 g=117.512\n",">325, c1=-126.521, c2=-134.827 g=118.684\n",">326, c1=-127.483, c2=-135.648 g=120.494\n",">327, c1=-127.762, c2=-137.196 g=120.459\n",">328, c1=-130.255, c2=-138.619 g=123.439\n",">329, c1=-131.179, c2=-139.264 g=122.148\n",">330, c1=-131.515, c2=-140.962 g=125.130\n",">331, c1=-133.130, c2=-142.087 g=128.150\n",">332, c1=-134.100, c2=-143.125 g=128.370\n",">333, c1=-135.261, c2=-145.074 g=132.950\n",">334, c1=-137.094, c2=-146.171 g=133.284\n",">335, c1=-137.575, c2=-146.832 g=135.073\n",">336, c1=-138.640, c2=-148.386 g=138.116\n",">337, c1=-139.632, c2=-148.913 g=140.613\n",">338, c1=-142.083, c2=-150.599 g=142.536\n",">339, c1=-142.020, c2=-152.347 g=144.790\n",">340, c1=-143.642, c2=-153.542 g=145.955\n",">341, c1=-144.633, c2=-154.969 g=148.270\n",">342, c1=-146.119, c2=-155.224 g=150.756\n",">343, c1=-147.680, c2=-157.373 g=152.544\n",">344, c1=-148.535, c2=-158.393 g=154.979\n",">345, c1=-149.604, c2=-160.688 g=157.020\n",">346, c1=-151.194, c2=-161.205 g=159.456\n",">347, c1=-152.323, c2=-162.607 g=161.922\n",">348, c1=-153.600, c2=-164.256 g=163.729\n",">349, c1=-155.019, c2=-165.590 g=165.674\n",">350, c1=-157.179, c2=-167.359 g=168.338\n",">351, c1=-156.308, c2=-168.515 g=169.323\n",">352, c1=-159.351, c2=-169.377 g=171.119\n",">353, c1=-160.110, c2=-171.242 g=172.442\n",">354, c1=-161.960, c2=-173.018 g=174.231\n",">355, c1=-162.012, c2=-173.908 g=175.463\n",">356, c1=-163.934, c2=-174.494 g=176.846\n",">357, c1=-164.151, c2=-175.393 g=177.007\n",">358, c1=-166.177, c2=-175.977 g=178.472\n",">359, c1=-167.370, c2=-176.644 g=179.214\n",">360, c1=-168.330, c2=-176.252 g=179.443\n",">361, c1=-167.610, c2=-176.580 g=180.127\n",">362, c1=-168.372, c2=-173.952 g=179.264\n",">363, c1=-168.604, c2=-173.786 g=179.879\n",">364, c1=-169.868, c2=-174.783 g=180.426\n",">365, c1=-169.948, c2=-173.340 g=180.295\n",">366, c1=-170.712, c2=-172.914 g=179.639\n",">367, c1=-170.428, c2=-172.360 g=179.784\n",">368, c1=-170.969, c2=-172.885 g=180.882\n",">369, c1=-172.876, c2=-176.650 g=181.763\n",">370, c1=-172.767, c2=-175.946 g=181.951\n",">371, c1=-174.054, c2=-174.251 g=181.247\n",">372, c1=-173.875, c2=-175.213 g=180.311\n",">373, c1=-175.820, c2=-173.176 g=177.445\n",">374, c1=-176.426, c2=-172.838 g=177.963\n",">375, c1=-177.656, c2=-173.480 g=177.285\n",">376, c1=-177.354, c2=-175.189 g=177.005\n",">377, c1=-176.604, c2=-178.780 g=179.696\n",">378, c1=-177.617, c2=-173.831 g=178.166\n",">379, c1=-176.895, c2=-167.669 g=177.507\n",">380, c1=-175.494, c2=-169.303 g=178.177\n",">381, c1=-175.657, c2=-166.666 g=178.834\n",">382, c1=-174.925, c2=-165.928 g=180.103\n",">383, c1=-175.817, c2=-173.094 g=181.301\n",">384, c1=-176.117, c2=-176.216 g=182.351\n",">385, c1=-176.379, c2=-178.208 g=185.558\n",">386, c1=-178.317, c2=-180.979 g=187.429\n",">387, c1=-177.700, c2=-183.040 g=189.550\n",">388, c1=-179.709, c2=-185.256 g=191.237\n",">Saved: generated_plot_0388.png and model_0388.h5\n",">389, c1=-180.615, c2=-186.796 g=192.602\n",">390, c1=-181.746, c2=-189.288 g=195.415\n",">391, c1=-182.801, c2=-189.787 g=194.932\n",">392, c1=-184.332, c2=-191.267 g=195.890\n",">393, c1=-184.766, c2=-192.114 g=197.665\n",">394, c1=-186.522, c2=-194.579 g=197.945\n",">395, c1=-186.531, c2=-195.959 g=199.347\n",">396, c1=-189.356, c2=-197.050 g=199.969\n",">397, c1=-189.581, c2=-199.060 g=203.296\n",">398, c1=-191.201, c2=-199.778 g=203.717\n",">399, c1=-191.546, c2=-201.258 g=204.448\n",">400, c1=-192.533, c2=-202.114 g=205.234\n",">401, c1=-194.468, c2=-202.390 g=205.170\n",">402, c1=-194.227, c2=-203.978 g=206.025\n",">403, c1=-195.548, c2=-204.260 g=207.080\n",">404, c1=-196.850, c2=-204.552 g=207.654\n",">405, c1=-197.060, c2=-204.048 g=207.807\n",">406, c1=-197.814, c2=-204.977 g=208.670\n",">407, c1=-199.222, c2=-204.389 g=208.698\n",">408, c1=-198.188, c2=-203.447 g=208.755\n",">409, c1=-199.785, c2=-204.010 g=209.211\n",">410, c1=-201.001, c2=-205.510 g=211.066\n",">411, c1=-202.435, c2=-208.180 g=211.341\n",">412, c1=-202.802, c2=-208.551 g=212.511\n",">413, c1=-202.419, c2=-209.690 g=213.101\n",">414, c1=-202.960, c2=-210.054 g=214.928\n",">415, c1=-206.046, c2=-212.940 g=215.817\n",">416, c1=-205.458, c2=-213.867 g=216.643\n",">417, c1=-205.957, c2=-215.820 g=219.220\n",">418, c1=-207.426, c2=-215.193 g=218.033\n",">419, c1=-207.298, c2=-215.614 g=220.972\n",">420, c1=-208.165, c2=-215.961 g=221.418\n",">421, c1=-208.529, c2=-215.008 g=221.600\n",">422, c1=-210.330, c2=-217.634 g=222.283\n",">423, c1=-210.646, c2=-217.212 g=223.065\n",">424, c1=-210.389, c2=-215.234 g=224.009\n",">425, c1=-212.417, c2=-213.711 g=222.865\n",">426, c1=-211.441, c2=-212.411 g=224.645\n",">427, c1=-212.622, c2=-211.070 g=226.529\n",">428, c1=-211.552, c2=-209.229 g=226.439\n",">429, c1=-212.038, c2=-208.716 g=224.875\n",">430, c1=-213.238, c2=-209.329 g=227.231\n",">431, c1=-212.127, c2=-205.061 g=225.668\n",">432, c1=-210.562, c2=-204.716 g=223.567\n",">433, c1=-211.353, c2=-204.443 g=222.097\n",">434, c1=-213.252, c2=-207.417 g=222.072\n",">435, c1=-213.539, c2=-212.879 g=225.125\n",">436, c1=-215.359, c2=-212.480 g=224.792\n",">437, c1=-215.866, c2=-216.758 g=229.189\n",">438, c1=-215.086, c2=-222.018 g=229.715\n",">439, c1=-218.030, c2=-221.375 g=231.571\n",">440, c1=-218.335, c2=-221.720 g=232.547\n",">441, c1=-217.102, c2=-221.690 g=233.551\n",">442, c1=-218.358, c2=-217.578 g=232.958\n",">443, c1=-215.946, c2=-212.825 g=229.468\n",">444, c1=-217.111, c2=-203.591 g=230.025\n",">445, c1=-215.412, c2=-198.992 g=225.324\n",">446, c1=-214.119, c2=-191.212 g=218.836\n",">447, c1=-215.248, c2=-182.639 g=214.189\n",">448, c1=-215.029, c2=-173.565 g=206.430\n",">449, c1=-213.353, c2=-168.942 g=198.498\n",">450, c1=-212.273, c2=-171.821 g=192.650\n",">451, c1=-214.762, c2=-172.347 g=190.331\n",">452, c1=-216.646, c2=-172.905 g=188.448\n",">453, c1=-215.337, c2=-175.576 g=190.204\n",">454, c1=-218.536, c2=-179.580 g=192.019\n",">455, c1=-219.175, c2=-184.626 g=194.435\n",">456, c1=-221.939, c2=-188.284 g=197.523\n",">457, c1=-222.852, c2=-191.372 g=200.460\n",">458, c1=-224.708, c2=-197.512 g=203.035\n",">459, c1=-226.600, c2=-202.515 g=207.553\n",">460, c1=-228.865, c2=-206.087 g=212.353\n",">461, c1=-230.787, c2=-212.226 g=218.412\n",">462, c1=-232.503, c2=-216.927 g=220.534\n",">463, c1=-234.649, c2=-221.311 g=225.425\n",">464, c1=-233.846, c2=-225.033 g=229.387\n",">465, c1=-236.114, c2=-228.767 g=234.261\n",">466, c1=-238.978, c2=-231.868 g=238.651\n",">467, c1=-239.593, c2=-235.769 g=240.417\n",">468, c1=-240.949, c2=-239.803 g=243.925\n",">469, c1=-243.049, c2=-242.853 g=245.429\n",">470, c1=-242.473, c2=-245.145 g=249.895\n",">471, c1=-243.888, c2=-247.651 g=251.528\n",">472, c1=-246.022, c2=-249.826 g=254.429\n",">473, c1=-248.179, c2=-251.997 g=255.410\n",">474, c1=-247.893, c2=-251.650 g=256.939\n",">475, c1=-248.404, c2=-254.447 g=259.517\n",">476, c1=-249.354, c2=-254.909 g=260.138\n",">477, c1=-249.780, c2=-256.476 g=260.919\n",">478, c1=-252.278, c2=-256.363 g=261.658\n",">479, c1=-252.269, c2=-254.642 g=262.215\n",">480, c1=-252.113, c2=-249.222 g=260.155\n",">481, c1=-252.619, c2=-242.734 g=258.643\n",">482, c1=-250.699, c2=-230.072 g=252.526\n",">483, c1=-247.214, c2=-213.045 g=245.099\n",">484, c1=-242.103, c2=-202.635 g=235.994\n",">485, c1=-239.318, c2=-187.599 g=224.528\n",">Saved: generated_plot_0485.png and model_0485.h5\n",">486, c1=-235.031, c2=-187.196 g=218.155\n",">487, c1=-231.251, c2=-187.955 g=216.257\n",">488, c1=-231.508, c2=-182.433 g=210.173\n",">489, c1=-233.399, c2=-187.017 g=210.955\n",">490, c1=-235.195, c2=-199.403 g=217.691\n",">491, c1=-239.799, c2=-209.344 g=224.215\n",">492, c1=-236.866, c2=-213.057 g=224.199\n",">493, c1=-237.507, c2=-209.566 g=220.149\n",">494, c1=-233.905, c2=-188.746 g=198.894\n",">495, c1=-221.748, c2=-131.524 g=152.530\n",">496, c1=-208.471, c2=21.760 g=25.419\n",">497, c1=-199.335, c2=98.236 g=-81.948\n",">498, c1=-189.929, c2=111.682 g=-88.610\n",">499, c1=-185.493, c2=106.310 g=-102.731\n",">500, c1=-180.688, c2=95.897 g=-107.446\n",">501, c1=-172.728, c2=87.618 g=-106.653\n",">502, c1=-170.769, c2=85.383 g=-99.199\n",">503, c1=-162.357, c2=63.231 g=-91.921\n",">504, c1=-158.007, c2=56.062 g=-83.106\n",">505, c1=-157.089, c2=43.999 g=-82.980\n",">506, c1=-149.119, c2=39.365 g=-82.179\n",">507, c1=-149.204, c2=36.030 g=-75.076\n",">508, c1=-141.911, c2=17.605 g=-85.018\n",">509, c1=-136.884, c2=8.459 g=-90.442\n",">510, c1=-132.239, c2=-21.164 g=-103.597\n",">511, c1=-142.703, c2=0.836 g=-100.291\n",">512, c1=-129.533, c2=15.845 g=-104.065\n",">513, c1=-130.573, c2=18.456 g=-102.578\n",">514, c1=-132.243, c2=26.880 g=-97.584\n",">515, c1=-126.405, c2=14.443 g=-102.478\n",">516, c1=-123.352, c2=32.342 g=-93.836\n",">517, c1=-124.740, c2=8.945 g=-83.105\n",">518, c1=-126.899, c2=33.475 g=-82.908\n",">519, c1=-125.317, c2=13.692 g=-83.251\n",">520, c1=-128.771, c2=57.588 g=-84.286\n",">521, c1=-128.972, c2=37.444 g=-82.726\n",">522, c1=-121.722, c2=30.221 g=-82.042\n",">523, c1=-121.313, c2=21.679 g=-90.229\n",">524, c1=-122.561, c2=0.782 g=-89.634\n",">525, c1=-126.185, c2=11.130 g=-85.705\n",">526, c1=-117.425, c2=15.012 g=-93.135\n",">527, c1=-119.262, c2=13.848 g=-92.837\n",">528, c1=-118.641, c2=23.096 g=-88.683\n",">529, c1=-116.265, c2=4.926 g=-91.430\n",">530, c1=-123.715, c2=6.124 g=-93.143\n",">531, c1=-107.491, c2=19.089 g=-97.108\n",">532, c1=-113.405, c2=-9.826 g=-99.724\n",">533, c1=-117.490, c2=12.120 g=-86.539\n",">534, c1=-115.693, c2=-5.126 g=-86.360\n",">535, c1=-121.332, c2=40.607 g=-86.334\n",">536, c1=-116.275, c2=29.770 g=-84.328\n",">537, c1=-115.797, c2=36.027 g=-80.567\n",">538, c1=-112.442, c2=-1.858 g=-87.538\n",">539, c1=-114.253, c2=17.168 g=-91.076\n",">540, c1=-106.547, c2=-21.266 g=-101.199\n",">541, c1=-99.341, c2=-18.098 g=-103.941\n",">542, c1=-112.782, c2=-21.097 g=-102.407\n",">543, c1=-103.763, c2=-26.211 g=-106.577\n",">544, c1=-105.520, c2=-7.373 g=-109.167\n",">545, c1=-108.929, c2=-7.515 g=-101.111\n",">546, c1=-97.258, c2=21.870 g=-95.458\n",">547, c1=-110.871, c2=-6.692 g=-95.290\n",">548, c1=-104.872, c2=-1.549 g=-103.888\n",">549, c1=-96.440, c2=13.271 g=-104.791\n",">550, c1=-110.031, c2=11.245 g=-94.143\n",">551, c1=-93.849, c2=-11.597 g=-88.498\n",">552, c1=-104.357, c2=-16.246 g=-93.573\n",">553, c1=-94.491, c2=-28.744 g=-91.522\n",">554, c1=-102.177, c2=-4.191 g=-83.312\n",">555, c1=-99.781, c2=0.982 g=-90.057\n",">556, c1=-102.858, c2=28.349 g=-81.649\n",">557, c1=-105.537, c2=9.438 g=-79.469\n",">558, c1=-96.658, c2=32.642 g=-75.507\n",">559, c1=-91.016, c2=-23.286 g=-86.351\n",">560, c1=-96.562, c2=-7.974 g=-85.911\n",">561, c1=-107.361, c2=-49.309 g=-87.060\n",">562, c1=-92.725, c2=-25.970 g=-88.996\n",">563, c1=-97.280, c2=-40.972 g=-86.104\n",">564, c1=-95.629, c2=-10.549 g=-90.740\n",">565, c1=-96.624, c2=-31.213 g=-80.466\n",">566, c1=-87.944, c2=6.462 g=-79.762\n",">567, c1=-103.909, c2=19.065 g=-69.734\n",">568, c1=-96.191, c2=27.258 g=-77.737\n",">569, c1=-83.082, c2=-16.744 g=-70.949\n",">570, c1=-93.445, c2=-15.150 g=-79.491\n",">571, c1=-72.989, c2=-8.808 g=-73.783\n",">572, c1=-82.009, c2=-36.307 g=-72.437\n",">573, c1=-95.804, c2=-15.837 g=-69.838\n",">574, c1=-76.940, c2=-20.661 g=-68.447\n",">575, c1=-83.533, c2=30.353 g=-56.851\n",">576, c1=-96.186, c2=-0.520 g=-51.991\n",">577, c1=-88.438, c2=32.333 g=-62.561\n",">578, c1=-89.199, c2=7.196 g=-64.061\n",">579, c1=-75.173, c2=31.517 g=-54.655\n",">580, c1=-75.467, c2=1.378 g=-43.167\n",">581, c1=-86.169, c2=29.014 g=-57.513\n",">582, c1=-85.339, c2=16.261 g=-59.818\n",">Saved: generated_plot_0582.png and model_0582.h5\n",">583, c1=-81.273, c2=-10.240 g=-55.797\n",">584, c1=-82.193, c2=0.291 g=-56.446\n",">585, c1=-88.978, c2=14.663 g=-62.423\n",">586, c1=-85.538, c2=-6.472 g=-53.585\n",">587, c1=-77.493, c2=28.120 g=-40.151\n",">588, c1=-79.074, c2=13.762 g=-42.681\n",">589, c1=-83.630, c2=32.325 g=-45.427\n",">590, c1=-85.718, c2=14.129 g=-43.555\n",">591, c1=-78.554, c2=44.589 g=-38.728\n",">592, c1=-82.350, c2=18.119 g=-45.808\n",">593, c1=-80.920, c2=39.465 g=-42.755\n",">594, c1=-76.226, c2=44.814 g=-29.261\n",">595, c1=-79.230, c2=40.967 g=-37.479\n",">596, c1=-72.768, c2=7.209 g=-17.898\n",">597, c1=-75.714, c2=26.346 g=-9.167\n",">598, c1=-82.586, c2=33.668 g=-17.124\n",">599, c1=-72.149, c2=37.014 g=-10.331\n",">600, c1=-68.342, c2=35.782 g=-9.860\n",">601, c1=-75.935, c2=38.928 g=-11.145\n",">602, c1=-72.990, c2=37.015 g=-10.820\n",">603, c1=-63.705, c2=28.901 g=-8.724\n",">604, c1=-62.767, c2=34.486 g=-1.145\n",">605, c1=-64.847, c2=30.956 g=4.279\n",">606, c1=-58.868, c2=26.774 g=3.819\n",">607, c1=-62.753, c2=18.988 g=-7.635\n",">608, c1=-56.364, c2=12.046 g=11.533\n",">609, c1=-62.535, c2=15.682 g=1.271\n",">610, c1=-61.897, c2=23.593 g=8.581\n",">611, c1=-58.830, c2=15.431 g=3.565\n",">612, c1=-59.060, c2=10.287 g=4.011\n",">613, c1=-49.477, c2=16.788 g=-0.197\n",">614, c1=-58.475, c2=21.416 g=12.381\n",">615, c1=-62.045, c2=11.595 g=2.068\n",">616, c1=-51.363, c2=23.979 g=12.929\n",">617, c1=-53.126, c2=21.679 g=8.603\n",">618, c1=-51.084, c2=15.518 g=11.347\n",">619, c1=-50.247, c2=17.254 g=17.407\n",">620, c1=-46.395, c2=16.329 g=22.451\n",">621, c1=-46.589, c2=12.368 g=24.153\n",">622, c1=-45.344, c2=13.015 g=27.116\n",">623, c1=-40.087, c2=9.299 g=34.992\n",">624, c1=-38.196, c2=3.154 g=28.544\n",">625, c1=-40.815, c2=11.282 g=31.668\n",">626, c1=-37.070, c2=2.177 g=29.386\n",">627, c1=-32.932, c2=7.615 g=31.109\n",">628, c1=-32.332, c2=1.797 g=28.854\n",">629, c1=-30.891, c2=0.429 g=34.481\n",">630, c1=-33.307, c2=-1.650 g=30.037\n",">631, c1=-33.212, c2=1.945 g=27.594\n",">632, c1=-33.697, c2=4.738 g=32.709\n",">633, c1=-31.405, c2=0.168 g=27.666\n",">634, c1=-31.400, c2=1.690 g=27.511\n",">635, c1=-30.728, c2=0.875 g=28.652\n",">636, c1=-27.378, c2=-0.814 g=31.826\n",">637, c1=-32.369, c2=-0.449 g=33.156\n",">638, c1=-25.788, c2=-0.306 g=37.903\n",">639, c1=-28.415, c2=-5.685 g=30.423\n",">640, c1=-26.087, c2=1.453 g=35.888\n",">641, c1=-24.300, c2=-3.138 g=28.794\n",">642, c1=-23.189, c2=-0.533 g=35.740\n",">643, c1=-27.907, c2=-4.707 g=29.841\n",">644, c1=-28.442, c2=2.397 g=25.433\n",">645, c1=-27.506, c2=0.747 g=28.871\n",">646, c1=-30.876, c2=0.650 g=26.868\n",">647, c1=-27.026, c2=-1.963 g=38.301\n",">648, c1=-25.356, c2=-2.598 g=34.153\n",">649, c1=-22.025, c2=-6.077 g=37.964\n",">650, c1=-25.233, c2=-2.802 g=32.600\n",">651, c1=-22.717, c2=-1.507 g=32.798\n",">652, c1=-26.442, c2=-1.153 g=24.173\n",">653, c1=-20.797, c2=-5.218 g=32.116\n",">654, c1=-20.698, c2=-5.224 g=33.033\n",">655, c1=-19.900, c2=-8.985 g=33.966\n",">656, c1=-19.071, c2=-7.284 g=29.988\n",">657, c1=-19.307, c2=-6.408 g=34.927\n",">658, c1=-18.450, c2=-5.748 g=31.111\n",">659, c1=-20.489, c2=-9.257 g=32.690\n",">660, c1=-20.003, c2=-4.835 g=33.527\n",">661, c1=-20.498, c2=-8.078 g=31.065\n",">662, c1=-16.334, c2=-10.309 g=24.867\n",">663, c1=-19.062, c2=-7.495 g=35.764\n",">664, c1=-22.195, c2=-10.746 g=33.802\n",">665, c1=-19.528, c2=-6.525 g=40.224\n",">666, c1=-10.815, c2=-8.133 g=34.475\n",">667, c1=-13.824, c2=-8.122 g=37.809\n",">668, c1=-12.118, c2=-7.352 g=33.612\n",">669, c1=-17.919, c2=-10.849 g=41.066\n",">670, c1=-16.543, c2=-10.077 g=26.712\n",">671, c1=-11.932, c2=-10.930 g=36.523\n",">672, c1=-11.117, c2=-12.387 g=38.619\n",">673, c1=-15.415, c2=-11.259 g=36.121\n",">674, c1=-13.277, c2=-9.097 g=35.917\n",">675, c1=-9.676, c2=-11.933 g=32.721\n",">676, c1=-16.023, c2=-11.694 g=32.362\n",">677, c1=-8.369, c2=-11.416 g=37.918\n",">678, c1=-13.281, c2=-12.130 g=31.524\n",">679, c1=-13.344, c2=-12.267 g=31.536\n",">Saved: generated_plot_0679.png and model_0679.h5\n",">680, c1=-11.449, c2=-13.168 g=36.663\n",">681, c1=-8.089, c2=-17.455 g=38.425\n",">682, c1=-6.660, c2=-21.325 g=42.816\n",">683, c1=-6.300, c2=-16.973 g=36.142\n",">684, c1=-5.872, c2=-18.756 g=43.848\n",">685, c1=-8.508, c2=-15.317 g=42.109\n",">686, c1=-8.472, c2=-23.298 g=45.350\n",">687, c1=-10.806, c2=-18.081 g=40.472\n",">688, c1=-10.424, c2=-20.724 g=42.614\n",">689, c1=-5.578, c2=-20.148 g=38.090\n",">690, c1=-9.176, c2=-24.280 g=38.090\n",">691, c1=-2.639, c2=-26.702 g=42.586\n",">692, c1=-4.190, c2=-30.067 g=42.814\n",">693, c1=-2.449, c2=-19.164 g=41.343\n",">694, c1=-3.894, c2=-23.672 g=42.049\n",">695, c1=-1.619, c2=-22.097 g=44.017\n",">696, c1=-5.795, c2=-25.567 g=46.964\n",">697, c1=-1.571, c2=-21.129 g=41.224\n",">698, c1=-2.064, c2=-27.724 g=42.319\n",">699, c1=-2.087, c2=-23.790 g=44.636\n",">700, c1=-1.341, c2=-26.458 g=47.633\n",">701, c1=0.426, c2=-19.559 g=44.459\n",">702, c1=-1.465, c2=-24.811 g=45.560\n",">703, c1=-1.937, c2=-23.699 g=39.255\n",">704, c1=-1.739, c2=-31.090 g=44.750\n",">705, c1=2.282, c2=-28.595 g=46.476\n",">706, c1=2.927, c2=-28.625 g=48.145\n",">707, c1=3.865, c2=-27.459 g=44.552\n",">708, c1=0.979, c2=-30.981 g=46.285\n",">709, c1=4.688, c2=-29.772 g=44.145\n",">710, c1=4.406, c2=-29.618 g=48.565\n",">711, c1=4.113, c2=-26.847 g=46.683\n",">712, c1=4.081, c2=-28.799 g=54.212\n",">713, c1=4.384, c2=-29.752 g=50.029\n",">714, c1=6.196, c2=-31.642 g=54.982\n",">715, c1=4.493, c2=-26.025 g=50.059\n",">716, c1=3.635, c2=-29.515 g=52.497\n",">717, c1=3.308, c2=-27.529 g=49.080\n",">718, c1=5.927, c2=-27.672 g=49.869\n",">719, c1=2.906, c2=-33.337 g=49.832\n",">720, c1=2.306, c2=-27.655 g=49.522\n",">721, c1=7.615, c2=-29.416 g=48.758\n",">722, c1=4.383, c2=-29.987 g=52.682\n",">723, c1=6.348, c2=-30.274 g=47.108\n",">724, c1=1.956, c2=-33.308 g=49.774\n",">725, c1=-0.033, c2=-31.395 g=50.077\n",">726, c1=6.089, c2=-31.469 g=49.980\n",">727, c1=8.752, c2=-34.290 g=51.540\n",">728, c1=5.154, c2=-29.322 g=46.505\n",">729, c1=2.390, c2=-33.580 g=48.550\n",">730, c1=6.617, c2=-33.571 g=49.335\n",">731, c1=6.104, c2=-32.404 g=51.718\n",">732, c1=5.032, c2=-30.115 g=46.187\n",">733, c1=3.779, c2=-31.318 g=52.770\n",">734, c1=6.565, c2=-26.297 g=44.373\n",">735, c1=5.189, c2=-33.091 g=53.540\n",">736, c1=6.575, c2=-29.802 g=46.412\n",">737, c1=7.829, c2=-33.119 g=48.690\n",">738, c1=4.978, c2=-28.305 g=51.677\n",">739, c1=4.015, c2=-31.935 g=50.042\n",">740, c1=5.722, c2=-30.553 g=44.319\n",">741, c1=4.268, c2=-32.378 g=50.478\n",">742, c1=5.594, c2=-28.580 g=48.546\n",">743, c1=6.011, c2=-31.148 g=51.845\n",">744, c1=7.786, c2=-28.613 g=47.786\n",">745, c1=6.684, c2=-29.913 g=51.289\n",">746, c1=5.659, c2=-26.606 g=43.411\n",">747, c1=3.336, c2=-29.266 g=50.530\n",">748, c1=1.708, c2=-29.578 g=44.543\n",">749, c1=5.219, c2=-28.236 g=46.822\n",">750, c1=1.930, c2=-29.443 g=48.712\n",">751, c1=3.837, c2=-26.869 g=45.714\n",">752, c1=6.046, c2=-29.076 g=48.008\n",">753, c1=4.986, c2=-31.313 g=45.583\n",">754, c1=8.129, c2=-33.643 g=50.621\n",">755, c1=7.833, c2=-31.348 g=43.100\n",">756, c1=10.931, c2=-28.991 g=43.652\n",">757, c1=10.263, c2=-31.316 g=50.230\n",">758, c1=9.427, c2=-34.703 g=62.022\n",">759, c1=12.477, c2=-31.795 g=50.017\n",">760, c1=8.038, c2=-30.451 g=49.160\n",">761, c1=5.519, c2=-27.298 g=48.265\n",">762, c1=1.360, c2=-27.354 g=42.344\n",">763, c1=2.481, c2=-23.010 g=46.029\n",">764, c1=2.300, c2=-28.388 g=48.611\n",">765, c1=2.141, c2=-26.278 g=47.730\n",">766, c1=8.220, c2=-28.370 g=41.779\n",">767, c1=1.945, c2=-28.148 g=47.427\n",">768, c1=4.665, c2=-30.707 g=46.044\n",">769, c1=6.323, c2=-30.146 g=53.866\n",">770, c1=7.843, c2=-29.148 g=41.249\n",">771, c1=8.878, c2=-33.233 g=48.746\n",">772, c1=5.477, c2=-26.947 g=44.937\n",">773, c1=7.097, c2=-28.953 g=47.591\n",">774, c1=3.988, c2=-30.210 g=45.734\n",">775, c1=6.394, c2=-28.506 g=46.023\n",">776, c1=8.378, c2=-30.591 g=43.373\n",">Saved: generated_plot_0776.png and model_0776.h5\n",">777, c1=8.877, c2=-28.662 g=44.804\n",">778, c1=6.119, c2=-29.169 g=47.995\n",">779, c1=8.286, c2=-27.860 g=47.433\n",">780, c1=4.781, c2=-32.579 g=48.732\n",">781, c1=5.868, c2=-32.472 g=43.738\n",">782, c1=9.748, c2=-36.410 g=45.785\n",">783, c1=10.400, c2=-35.758 g=42.634\n",">784, c1=8.710, c2=-33.191 g=44.116\n",">785, c1=8.784, c2=-33.347 g=45.064\n",">786, c1=10.806, c2=-37.541 g=46.670\n",">787, c1=9.888, c2=-33.112 g=44.416\n",">788, c1=8.260, c2=-32.446 g=45.266\n",">789, c1=11.643, c2=-40.918 g=45.899\n",">790, c1=11.057, c2=-36.034 g=47.933\n",">791, c1=11.125, c2=-27.914 g=43.869\n",">792, c1=11.079, c2=-29.320 g=45.432\n",">793, c1=11.477, c2=-41.403 g=44.751\n",">794, c1=10.696, c2=-37.793 g=43.498\n",">795, c1=14.633, c2=-35.263 g=43.779\n",">796, c1=8.686, c2=-32.853 g=48.002\n",">797, c1=13.274, c2=-37.375 g=46.127\n",">798, c1=9.468, c2=-41.468 g=49.177\n",">799, c1=9.724, c2=-42.492 g=45.834\n",">800, c1=9.230, c2=-36.360 g=44.979\n",">801, c1=12.286, c2=-30.562 g=40.134\n",">802, c1=13.278, c2=-33.814 g=46.618\n",">803, c1=11.621, c2=-34.624 g=45.875\n",">804, c1=11.993, c2=-36.655 g=44.929\n",">805, c1=10.147, c2=-38.819 g=39.944\n",">806, c1=17.740, c2=-37.290 g=46.437\n",">807, c1=11.302, c2=-34.102 g=45.459\n",">808, c1=9.445, c2=-47.501 g=47.413\n",">809, c1=13.077, c2=-39.012 g=42.567\n",">810, c1=13.526, c2=-40.964 g=45.293\n",">811, c1=13.074, c2=-35.310 g=41.233\n",">812, c1=14.485, c2=-43.073 g=48.656\n",">813, c1=12.187, c2=-47.048 g=49.079\n",">814, c1=17.416, c2=-41.213 g=44.003\n",">815, c1=15.435, c2=-41.278 g=47.772\n",">816, c1=13.510, c2=-37.030 g=46.224\n",">817, c1=16.346, c2=-47.262 g=45.334\n",">818, c1=15.945, c2=-42.178 g=43.609\n",">819, c1=17.217, c2=-44.027 g=47.480\n",">820, c1=17.471, c2=-38.629 g=45.376\n",">821, c1=14.497, c2=-35.597 g=41.913\n",">822, c1=19.713, c2=-32.504 g=48.069\n",">823, c1=19.609, c2=-45.644 g=52.318\n",">824, c1=16.354, c2=-47.564 g=48.754\n",">825, c1=19.099, c2=-37.160 g=50.581\n",">826, c1=18.110, c2=-42.065 g=44.648\n",">827, c1=18.695, c2=-42.400 g=47.689\n",">828, c1=19.669, c2=-45.398 g=47.070\n",">829, c1=19.373, c2=-46.372 g=53.254\n",">830, c1=17.028, c2=-44.144 g=45.874\n",">831, c1=20.103, c2=-39.572 g=51.386\n",">832, c1=22.911, c2=-39.412 g=45.663\n",">833, c1=20.102, c2=-43.561 g=48.598\n",">834, c1=20.322, c2=-48.001 g=52.335\n",">835, c1=20.067, c2=-42.160 g=55.029\n",">836, c1=22.987, c2=-53.673 g=48.022\n",">837, c1=20.659, c2=-45.088 g=45.685\n",">838, c1=19.706, c2=-43.963 g=46.605\n",">839, c1=18.530, c2=-41.498 g=49.582\n",">840, c1=17.592, c2=-43.603 g=45.971\n",">841, c1=21.031, c2=-38.493 g=49.132\n",">842, c1=18.982, c2=-42.612 g=45.860\n",">843, c1=23.309, c2=-38.345 g=44.772\n",">844, c1=16.738, c2=-40.404 g=44.129\n",">845, c1=18.561, c2=-45.303 g=47.732\n",">846, c1=20.010, c2=-39.141 g=41.438\n",">847, c1=19.424, c2=-39.918 g=51.857\n",">848, c1=16.181, c2=-40.430 g=48.920\n",">849, c1=18.739, c2=-39.028 g=51.090\n",">850, c1=18.255, c2=-39.634 g=46.999\n",">851, c1=15.787, c2=-39.302 g=49.220\n",">852, c1=17.655, c2=-42.434 g=46.677\n",">853, c1=19.333, c2=-39.591 g=47.447\n",">854, c1=18.883, c2=-32.986 g=44.157\n",">855, c1=20.243, c2=-38.833 g=45.980\n",">856, c1=19.960, c2=-35.060 g=45.719\n",">857, c1=19.869, c2=-36.491 g=49.381\n",">858, c1=19.506, c2=-44.897 g=43.123\n",">859, c1=15.086, c2=-41.676 g=45.916\n",">860, c1=19.682, c2=-44.277 g=44.014\n",">861, c1=23.633, c2=-41.107 g=50.138\n",">862, c1=19.009, c2=-42.111 g=45.763\n",">863, c1=25.531, c2=-50.728 g=57.626\n",">864, c1=19.455, c2=-46.498 g=53.202\n",">865, c1=23.561, c2=-46.223 g=51.876\n",">866, c1=19.591, c2=-46.730 g=51.625\n",">867, c1=23.106, c2=-45.597 g=51.380\n",">868, c1=22.619, c2=-51.286 g=52.347\n",">869, c1=21.861, c2=-38.573 g=47.588\n",">870, c1=22.538, c2=-33.137 g=58.534\n",">871, c1=21.725, c2=-44.737 g=48.853\n",">872, c1=26.985, c2=-47.072 g=53.464\n",">873, c1=23.496, c2=-45.572 g=51.836\n",">Saved: generated_plot_0873.png and model_0873.h5\n",">874, c1=23.558, c2=-42.627 g=50.407\n",">875, c1=20.410, c2=-41.854 g=50.144\n",">876, c1=19.703, c2=-40.602 g=54.115\n",">877, c1=21.102, c2=-48.387 g=48.903\n",">878, c1=22.091, c2=-36.526 g=49.726\n",">879, c1=21.883, c2=-42.532 g=49.761\n",">880, c1=19.417, c2=-35.817 g=50.144\n",">881, c1=21.690, c2=-42.005 g=54.248\n",">882, c1=23.659, c2=-48.645 g=50.958\n",">883, c1=23.194, c2=-48.418 g=51.435\n",">884, c1=21.977, c2=-44.534 g=50.283\n",">885, c1=18.135, c2=-44.280 g=49.808\n",">886, c1=18.029, c2=-37.858 g=49.790\n",">887, c1=18.437, c2=-43.809 g=51.539\n",">888, c1=23.668, c2=-47.057 g=56.626\n",">889, c1=25.673, c2=-41.931 g=53.520\n",">890, c1=22.838, c2=-44.883 g=52.671\n",">891, c1=17.449, c2=-42.683 g=44.622\n",">892, c1=27.273, c2=-40.648 g=50.212\n",">893, c1=22.454, c2=-39.986 g=50.626\n",">894, c1=20.158, c2=-34.746 g=53.419\n",">895, c1=27.585, c2=-43.138 g=50.371\n",">896, c1=19.341, c2=-42.729 g=47.642\n",">897, c1=22.816, c2=-45.387 g=55.021\n",">898, c1=24.505, c2=-38.051 g=63.435\n",">899, c1=24.718, c2=-45.141 g=51.958\n",">900, c1=22.312, c2=-42.656 g=51.168\n",">901, c1=22.735, c2=-42.410 g=56.810\n",">902, c1=23.089, c2=-43.275 g=52.867\n",">903, c1=21.822, c2=-43.499 g=49.084\n",">904, c1=25.606, c2=-37.256 g=48.174\n",">905, c1=24.370, c2=-36.033 g=54.142\n",">906, c1=19.271, c2=-37.595 g=50.080\n",">907, c1=20.843, c2=-36.805 g=57.007\n",">908, c1=17.831, c2=-40.501 g=51.000\n",">909, c1=20.196, c2=-34.788 g=46.295\n",">910, c1=17.587, c2=-35.067 g=46.566\n",">911, c1=18.783, c2=-45.180 g=50.710\n",">912, c1=18.778, c2=-34.208 g=48.647\n",">913, c1=20.674, c2=-36.076 g=54.741\n",">914, c1=17.316, c2=-33.980 g=46.938\n",">915, c1=20.156, c2=-37.725 g=55.403\n",">916, c1=22.441, c2=-39.471 g=53.295\n",">917, c1=21.451, c2=-39.073 g=53.947\n",">918, c1=20.747, c2=-36.043 g=52.552\n",">919, c1=18.141, c2=-33.830 g=53.170\n",">920, c1=17.390, c2=-40.427 g=55.141\n",">921, c1=21.238, c2=-36.621 g=52.122\n",">922, c1=15.973, c2=-36.135 g=48.486\n",">923, c1=13.739, c2=-38.418 g=53.802\n",">924, c1=20.342, c2=-35.267 g=57.828\n",">925, c1=16.251, c2=-37.810 g=50.527\n",">926, c1=12.464, c2=-38.740 g=48.452\n",">927, c1=19.029, c2=-34.583 g=49.108\n",">928, c1=19.107, c2=-36.647 g=50.209\n",">929, c1=16.138, c2=-38.627 g=47.473\n",">930, c1=15.356, c2=-35.930 g=50.022\n",">931, c1=14.643, c2=-33.548 g=53.181\n",">932, c1=21.988, c2=-37.570 g=48.869\n",">933, c1=20.639, c2=-33.725 g=52.248\n",">934, c1=17.769, c2=-36.657 g=49.391\n",">935, c1=18.113, c2=-37.019 g=56.075\n",">936, c1=13.991, c2=-38.503 g=50.391\n",">937, c1=17.274, c2=-34.665 g=48.242\n",">938, c1=17.571, c2=-36.010 g=55.190\n",">939, c1=15.812, c2=-32.833 g=53.798\n",">940, c1=18.670, c2=-36.904 g=48.707\n",">941, c1=11.754, c2=-33.543 g=45.087\n",">942, c1=17.114, c2=-30.549 g=45.029\n",">943, c1=17.127, c2=-32.326 g=47.810\n",">944, c1=17.371, c2=-33.175 g=44.621\n",">945, c1=13.322, c2=-32.962 g=49.402\n",">946, c1=16.066, c2=-34.497 g=46.284\n",">947, c1=17.805, c2=-34.052 g=53.348\n",">948, c1=13.600, c2=-35.743 g=51.426\n",">949, c1=12.838, c2=-31.468 g=51.764\n",">950, c1=11.792, c2=-37.174 g=55.574\n",">951, c1=13.212, c2=-34.932 g=52.244\n",">952, c1=11.965, c2=-33.371 g=48.964\n",">953, c1=12.024, c2=-34.455 g=47.122\n",">954, c1=14.230, c2=-34.348 g=52.107\n",">955, c1=14.957, c2=-30.677 g=47.341\n",">956, c1=16.895, c2=-33.082 g=47.958\n",">957, c1=14.621, c2=-31.095 g=48.553\n",">958, c1=16.094, c2=-32.244 g=47.443\n",">959, c1=15.586, c2=-36.038 g=49.105\n",">960, c1=16.157, c2=-34.720 g=53.475\n",">961, c1=14.680, c2=-32.271 g=50.452\n",">962, c1=11.225, c2=-32.717 g=49.716\n",">963, c1=13.747, c2=-33.784 g=51.458\n",">964, c1=17.007, c2=-35.572 g=59.892\n",">965, c1=17.157, c2=-37.752 g=48.920\n",">966, c1=15.461, c2=-33.640 g=54.464\n",">967, c1=13.632, c2=-37.954 g=53.855\n",">968, c1=16.901, c2=-33.042 g=51.132\n",">969, c1=14.862, c2=-31.607 g=47.219\n",">970, c1=11.851, c2=-32.851 g=51.034\n",">Saved: generated_plot_0970.png and model_0970.h5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P2Z7ZqDuq3qD","colab_type":"text"},"source":["Your specific results will vary given the stochastic nature of the learning algorithm. Nevertheless, the general structure of training should be very similar.\n","\n","First, the loss of the critic and generator models is reported to the console each iteration of the training loop. Specifically, c1 is the loss of the critic on real examples, c2 is the loss of the critic in generated samples, and g is the loss of the generator trained via the critic.\n","\n","The c1 scores are inverted as part of the loss function; this means if they are reported as negative, then they are really positive, and if they are reported as positive, they are really negative. The sign of the c2 scores is unchanged.\n","\n","Recall that the Wasserstein loss seeks scores for real and fake that are more different during training. We can see this towards the end of the run, such as the final epoch where the c1 loss for real examples is 5.338 (really -5.338) and the c2 loss for fake examples is -14.260, and this separation of about 10 units is consistent at least for the prior few iterations.\n","\n","We can also see that in this case, the model is scoring the loss of the generator at around 20. Again, recall that we update the generator via the critic model and treat the generated examples as real with the target of -1, therefore the score can be interpreted as a value around -20, close to the loss for fake samples."]},{"cell_type":"markdown","metadata":{"id":"t273bgepz8ih","colab_type":"text"},"source":["Line plots for loss are created and saved at the end of the run.\n","\n","The plot shows the loss for the critic on real samples (blue), the loss for the critic on fake samples (orange), and the loss for the critic when updating the generator with fake samples (green).\n","\n","There is one important factor when reviewing learning curves for the WGAN and that is the trend.\n","\n","The benefit of the WGAN is that the loss correlates with generated image quality. Lower loss means better quality images, for a stable training process.\n","\n","In this case, lower loss specifically refers to lower Wasserstein loss for generated images as reported by the critic (orange line). This sign of this loss is not inverted by the target label (e.g. the target label is +1.0), therefore, a well-performing WGAN should show this line trending down as the image quality of the generated model is increased.\n","\n","![learn_curves](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Line-Plots-of-Loss-and-Accuracy-for-a-Wasserstein-Generative-Adversarial-Network.png)"]}]}