{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.1-introduction-to-convnets.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"fUcy81dlz_Lm","colab_type":"code","outputId":"0f34204c-9ed7-4b1b-aa29-5dda43851f6b","executionInfo":{"status":"ok","timestamp":1550497199470,"user_tz":-120,"elapsed":1820,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["import keras\n","keras.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'2.2.4'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"collapsed":true,"id":"WPM7G5cPz_Lr","colab_type":"text"},"cell_type":"markdown","source":["# 5.1 - Introduction to convnets\n","\n","This notebook contains the code sample found in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n","\n","----\n","\n","First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been \n","through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its \n","accuracy will still blow out of the water that of the densely-connected model from Chapter 2.\n","\n","The 6 lines of code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a \n","minute what they do concretely.\n","Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n","In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n","passing the argument `input_shape=(28, 28, 1)` to our first layer."]},{"metadata":{"id":"tEwQFo9Jz_Ls","colab_type":"code","outputId":"b77a99f4-4eaa-4ac4-ae21-57529401474a","executionInfo":{"status":"ok","timestamp":1550497199479,"user_tz":-120,"elapsed":1823,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["from keras import layers\n","from keras import models\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"nYzY6KjWz_Lv","colab_type":"text"},"cell_type":"markdown","source":["Let's display the architecture of our convnet so far:"]},{"metadata":{"id":"QymgEKDMz_Lw","colab_type":"code","outputId":"c2c4a1c4-f9e3-4a2b-f8c8-e2391c1971d6","executionInfo":{"status":"ok","timestamp":1550497199480,"user_tz":-120,"elapsed":1805,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"collapsed":true,"id":"dcGrFxNZz_L1","colab_type":"text"},"cell_type":"markdown","source":["You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n","and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n","the `Conv2D` layers (e.g. 32 or 64).\n","\n","The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n","already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n","So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"]},{"metadata":{"id":"_RipSLzdz_L1","colab_type":"code","colab":{}},"cell_type":"code","source":["model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vDWqS5pJz_L5","colab_type":"text"},"cell_type":"markdown","source":["We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n","looks like:"]},{"metadata":{"id":"XevNOC2oz_L7","colab_type":"code","outputId":"a35a9eb0-641f-4bdc-877d-db63cbde6814","executionInfo":{"status":"ok","timestamp":1550497199484,"user_tz":-120,"elapsed":1795,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":422}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 576)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                36928     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"ign1S9NXz_L_","colab_type":"text"},"cell_type":"markdown","source":["As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n","\n","Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter \n","2."]},{"metadata":{"id":"fjLulvYAz_MA","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import mnist\n","from keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","train_images = train_images.astype('float32') / 255\n","\n","test_images = test_images.reshape((10000, 28, 28, 1))\n","test_images = test_images.astype('float32') / 255\n","\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XgNWBJ7pz_MD","colab_type":"code","outputId":"0ee918f4-1d20-401f-fe05-ecdfa062415d","executionInfo":{"status":"ok","timestamp":1550497519043,"user_tz":-120,"elapsed":321334,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"cell_type":"code","source":["model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(train_images, train_labels, epochs=5, batch_size=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/5\n","60000/60000 [==============================] - 64s 1ms/step - loss: 0.1893 - acc: 0.9415\n","Epoch 2/5\n","60000/60000 [==============================] - 64s 1ms/step - loss: 0.0588 - acc: 0.9828\n","Epoch 3/5\n","60000/60000 [==============================] - 64s 1ms/step - loss: 0.0447 - acc: 0.9872\n","Epoch 4/5\n","60000/60000 [==============================] - 64s 1ms/step - loss: 0.0325 - acc: 0.9904\n","Epoch 5/5\n","60000/60000 [==============================] - 64s 1ms/step - loss: 0.0255 - acc: 0.9920\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f8d0ff6e860>"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"wob5b0yKz_MM","colab_type":"text"},"cell_type":"markdown","source":["Let's evaluate the model on the test data:"]},{"metadata":{"id":"SFukNK7Jz_MN","colab_type":"code","outputId":"da57ace2-21cb-4383-8d43-2b02e502141d","executionInfo":{"status":"ok","timestamp":1550497522708,"user_tz":-120,"elapsed":324979,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["test_loss, test_acc = model.evaluate(test_images, test_labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 4s 374us/step\n"],"name":"stdout"}]},{"metadata":{"id":"jD1KTD-Gz_MR","colab_type":"code","outputId":"1baf12b2-89ec-4f9e-87f9-63d2901bf0a8","executionInfo":{"status":"ok","timestamp":1550497522710,"user_tz":-120,"elapsed":324969,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["test_acc"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9897"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"nBlNyCVoz_MU","colab_type":"text"},"cell_type":"markdown","source":["While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we \n","decreased our error rate by 68% (relative). Not bad! \n","\n","___But why does this simple convnet work so well, compared to a densely connected\n","model?___\n","\n","___To answer this, let’s dive into what the Conv2D and MaxPooling2D layers do.___"]},{"metadata":{"id":"Mf1y2sdWz_MW","colab_type":"text"},"cell_type":"markdown","source":["# The convolution operation\n","\n","## Why?\n","__Dense is global while Conv is local__\n","![5.1_1_local_conv.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_1_local_conv.png?raw=true)\n","\n","___Key conv properties___:\n","- The patterns they learn are ___translation invariant___\n","\n","After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it ___anywhere___: \n","\n","for example, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location. \n","\n","This makes convnets ___data efficient___ when processing images (because the visual world is fundamentally translation invariant): \n","\n","they need _fewer training samples_ to learn representations that have generalization power.\n","\n"]},{"metadata":{"id":"4F25B9-Jz_MX","colab_type":"text"},"cell_type":"markdown","source":["- They can learn ___spatial hierarchies___ of patterns\n","the visual world is fundamentally spatially hierarchical\n","![5.1_2_hier_cat_example.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_2_hier_cat_example.png?raw=true)\n"]},{"metadata":{"id":"Y1nUwCooz_Ma","colab_type":"text"},"cell_type":"markdown","source":["# Basics of convolution from Signal Processing"]},{"metadata":{"id":"vvgl7P4oz_Mc","colab_type":"text"},"cell_type":"markdown","source":["__Output = Response --> Obtained by convolution__\n","![5.1_3_filter.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_3_filter.png?raw=true)"]},{"metadata":{"id":"QeULPdnVz_Md","colab_type":"text"},"cell_type":"markdown","source":["# Conv Filters perform template matching = Search for a pattern\n","\n","__1D__\n","![5.1_4_1D_pattern_search.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_4_1D_pattern_search.png?raw=true)"]},{"metadata":{"id":"CjWKVMgxz_Md","colab_type":"text"},"cell_type":"markdown","source":["__2D__\n","\n","![5.1_5_2D_pattern_search.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_5_2D_pattern_search.png?raw=true)"]},{"metadata":{"id":"bPU_NW-Rz_Me","colab_type":"text"},"cell_type":"markdown","source":["![5.1_6_2D_correlation_map.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_6_2D_correlation_map.png?raw=true)"]},{"metadata":{"id":"1P9VJjNGz_Mf","colab_type":"text"},"cell_type":"markdown","source":["# Terminology\n","\n","- Feature = Kernel = Filter = template --> parametrized by fitler coefficients. \n","- Feature Extraction = matching = conv operation\n","- Feature map = result of conv = filter response\n","\n","\n","Example of features are edge detectors\n","A Kernel is a feature detector – Local features\n","![5.1_7_edge_detection.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_7_edge_detection.png?raw=true)"]},{"metadata":{"id":"71tmzEb9z_Mg","colab_type":"text"},"cell_type":"markdown","source":["# Convolution and Deep Learning\n","__Instead of designing the filter coefficients, lets learn them!__\n","\n","_Now filter coeff. = weights_\n","\n","Convolutions operate over 3D tensors, called __feature maps__, with two spatial axes (_height_ and _width_) as well as a _depth_ axis (also called the ___channels axis___). \n","\n","For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. \n","\n","For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray). \n","\n","\n"]},{"metadata":{"id":"5vLTk4Z7z_Mg","colab_type":"text"},"cell_type":"markdown","source":["The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an ___output feature map___. \n","\n","__Filter vs. Feature map__\n","\n","This __output feature map is still a 3D tensor__: it has a width and a height. \n","\n","___Its depth can be arbitrary___, because the output depth is a _parameter of the layer_, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for __filters__. \n","\n","_Feature map = 3D tensor_\n","\n","_Filter = 2D kernel ==> each channel feature map = response map_\n","\n","Filters encode specific aspects of the input data: at a high level, a single filter could encode the concept “presence of a face in the input,”for instance. \n","\n"]},{"metadata":{"id":"0EocFjj3z_Mh","colab_type":"text"},"cell_type":"markdown","source":["In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contains a 26 × 26 grid of values, which is a response map of the filter over the input, indicating the response of that filter pattern at different locations in the input. \n","\n","___That is what the term feature map means___: every dimension in the depth axis is a feature (or filter), and the 2D tensor output[:, :, n] is the 2D spatial map of the response of this filter over the input."]},{"metadata":{"id":"k9YR8g7qz_Mi","colab_type":"text"},"cell_type":"markdown","source":["![5.1_8_response_map.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_8_response_map.png?raw=true)"]},{"metadata":{"id":"XtA_EjWiz_Mj","colab_type":"text"},"cell_type":"markdown","source":["# Convolutions are defined by two key parameters:\n","\n","- ___Kernel size___ Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the\n","example, they were 3 × 3, which is a common choice.\n","-  ___Output channels___ Depth of the output feature map—The number of filters computed by the convolution.\n","The example started with a depth of 32 and ended with a depth of 64.\n","\n","# How convolution occurs\n","- We have n_in_channels x L x W x n_out_channels kernel tensors as follows:\n","    - For each input channel, we have LxW kernel --> in_channel_block\n","    - We have n_out_channel of these in_channel_block\n","    - We perform a do prodcut of each in_channel_block with the input tensor feature map, which gives us out_feature_map for each output channel. Note that, the dot product sums over the in_channels, so this dimension disappear.\n","    - We have an output = n_out_channels x L_out x W_out. L_out and W_out are determined according to the border effect and stride (see below)."]},{"metadata":{"id":"AVMl3wv_z_Mk","colab_type":"text"},"cell_type":"markdown","source":["![5.1_9_conv_example.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_9_conv_example.png?raw=true)"]},{"metadata":{"id":"g9xEXauxz_Mk","colab_type":"text"},"cell_type":"markdown","source":["__Note that the output width and height may differ from the input width and height.__\n","\n","They may differ for two reasons:\n","- __Border effects__, which can be countered by padding the input feature map\n","- The use of __strides__, which I’ll define in a second\n","Let’s take a deeper look at these notions."]},{"metadata":{"id":"GEb2a12Sz_Ml","colab_type":"text"},"cell_type":"markdown","source":["# UNDERSTANDING BORDER EFFECTS AND PADDING\n","Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around which you\n","can center a 3 × 3 window, forming a 3 × 3 grid. Hence, the output feature\n","map will be 3 × 3. It shrinks a little: by exactly two tiles alongside each dimension,\n","in this case. You can see this border effect in action in the earlier example: you start\n","with 28 × 28 inputs, which become 26 × 26 after the first convolution layer.\n","\n","\n","\n","__VALID conv__\n","- Kernel = MxM\n","- Input = NxN\n","- Output = N-M+1 x N-M+1\n","\n","PAD = No padding"]},{"metadata":{"id":"yjbSBWbcz_Mo","colab_type":"text"},"cell_type":"markdown","source":["![5.1_10_border_effect.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_10_border_effect.png?raw=true)"]},{"metadata":{"id":"HF7kqK9Sz_Mp","colab_type":"text"},"cell_type":"markdown","source":["If you want to get an output feature map with the same spatial dimensions as the input, you can use padding. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center\n","convolution windows around every input tile. \n","\n","For a 3 × 3 window, you add one column on the right, one column on the left, one row at the top, and one row at the\n","bottom. For a 5 × 5 window, you add two rows:\n","\n","Padding is (M-1)/2 in general.\n","\n","__SAME conv__\n","- Kernel = MxM\n","- Input = NxN\n","- Output = NxN \n","\n","PAD = (M-1)/2, so out=(N + 2 * (M-1)/2) - M + 1 = M"]},{"metadata":{"id":"iJe34LNBz_Mr","colab_type":"text"},"cell_type":"markdown","source":["![5.1_11_SAME_conv_pad.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_11_SAME_conv_pad.png?raw=true)"]},{"metadata":{"id":"GJC2SF7tz_Ms","colab_type":"text"},"cell_type":"markdown","source":["In Conv2D layers, padding is configurable via the padding argument, which takes two\n","values: \"valid\", which means no padding (only valid window locations will be used);\n","and \"same\", which means “pad in such a way as to have an output with the same width\n","and height as the input.” The padding argument defaults to \"valid\".\n","\n","Note that, the normal convolution produces bigger size, but this is not in Keras.\n","\n","In this type (default from signal processing), we pad with the size of the kernel (M) on each side.\n","\n","__NORM conv__\n","- Kernel = MxM\n","- Input = NxN\n","- Output = N+M-1xN+M-1\n","\n","PAD = M, so out = (N+2*M) - M + 1 = N + M - 1 "]},{"metadata":{"id":"wJrbDdt5z_Mv","colab_type":"text"},"cell_type":"markdown","source":["___UNDERSTANDING CONVOLUTION STRIDES___\n","\n","The other factor that can influence output size is the notion of strides. \n","\n","The description of convolution so far has assumed that the center tiles of the convolution windows are all contiguous. \n","\n","But the __distance between two successive windows__ is a parameter of the convolution, called its stride, which defaults to 1. \n","\n","It’s possible to have strided convolutions: convolutions with a stride higher than 1. \n","\n","You can see the patches extracted by a 3 × 3 convolution with stride 2 over a 5 × 5 input (without padding)."]},{"metadata":{"id":"EbDgiiN9z_Mx","colab_type":"text"},"cell_type":"markdown","source":["![5.1_12_stride.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/5.1_12_stride.png?raw=true)"]},{"metadata":{"id":"ZfkdmXeVz_Mz","colab_type":"text"},"cell_type":"markdown","source":["__Padding equations for 3 conv types in case of stride S__\n","\n","VALID --> PAD = 0, Out--> (N-M+1)/S\n","\n","SAME --> Invalid in case of strides. Meaning, you cannot get SAME output size as input in case of stride.\n","\n","NORM--> Output size = [ (N-M+2P)/S ]  + 1, with S and P such that the PAD is integer. 2P because we pad right and left. stride is the same before and after (same step in all directions).\n","\n","Check the above equation with S = 1, P = (M-1)/2, you get output size = N\n","\n"]},{"metadata":{"id":"PQdObRS0z_M0","colab_type":"text"},"cell_type":"markdown","source":["# Why strides?\n","___Downsampling___\n","\n","__Stride 2 conv:\n","Using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects).\n","So in SAME conv (the most widely used type, we halve the size by stride 2 conv__\n","\n","_Strided convolutions are rarely used in practice_, although they can come in handy for some types of models; it’s good to be familiar with the concept.\n","\n","To downsample feature maps, instead of strides, we tend to use the __max-pooling__ operation, which you saw in action in the first convnet example. Let’s look at it in more depth."]},{"metadata":{"id":"Hx1Zaz7xz_M1","colab_type":"text"},"cell_type":"markdown","source":["# The max-pooling operation\n","\n","_Downsampleing_: In the convnet example, you may have noticed that the size of the feature maps is halved after every MaxPooling2D layer. \n","\n","For instance, before the first MaxPooling2D layers, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.\n","\n","_That’s the role of max pooling: to aggressively downsample feature maps, much like strided convolutions._\n","\n","Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel.\n","\n","_It’s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation._\n","\n","_A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 × 3 windows and no stride (stride 1)._\n","\n","## Why downsample feature maps this way? \n","Why not remove the max-pooling layers and keep fairly large feature maps all the way up? Let’s look at this option. The convolutional base of the model would then look like this:"]},{"metadata":{"id":"441_t6tNz_M1","colab_type":"code","colab":{}},"cell_type":"code","source":["model_no_max_pool = models.Sequential()\n","model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ov8psBUEz_M3","colab_type":"code","outputId":"c08587ea-fb72-40a6-bdcd-47ba013e512c","executionInfo":{"status":"ok","timestamp":1550497523033,"user_tz":-120,"elapsed":325287,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":246}},"cell_type":"code","source":["model_no_max_pool.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"-R7C7lEMz_M6","colab_type":"text"},"cell_type":"markdown","source":["# What’s wrong with this setup? Two things:\n","\n","- It isn’t ___conducive to learning a spatial hierarchy of features___. \n","The 3 × 3 windows in the third layer will only contain information coming from 7 × 7 windows in\n","the initial input. The high-level patterns learned by the convnet will still be very small with regard to the initial input, which may not be enough to learn to classify digits (try recognizing a digit by only looking at it through windows that are\n","7 × 7 pixels!). \n","\n","_We need the features from the last convolution layer to contain information about the totality of the input._\n","\n","- The final feature map has 22 × 22 × 64 = 30,976 total coefficients per sample.\n","___This is huge___. If you were to flatten it to stick a Dense layer of size 512 on top,\n","that layer would have 15.8 million parameters. \n","\n","_This is far too large for such a small model and would result in intense overfitting._\n","\n","In short, the reason to use __downsampling__ is to ___reduce the number of feature-map\n","coefficients to process__, as well as to ___induce spatial-filter hierarchies by making successive\n","convolution layers look at increasingly large windows__ (in terms of the fraction of\n","the original input they cover).\n","\n","Note that max pooling isn’t the only way you can achieve such downsampling. \n","As you already know, you can also use strides in the prior convolution layer. \n","\n","And you can use __average pooling__ instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch, rather than the max. \n","But max pooling tends to work better than these alternative solutions. \n","\n","In a nutshell, the reason is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map (hence, the term feature map), and __it’s more informative to look at the maximal presence of different features than at their average presence__. \n","\n","So the most reasonable subsampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of\n","the inputs (via strided convolutions) or averaging input patches, which could cause you to miss or dilute feature-presence information.\n"]},{"metadata":{"id":"BfbI_A7uz_M6","colab_type":"text"},"cell_type":"markdown","source":["# Wrapping up\n","At this point, you should understand the basics of convnets—feature maps, convolution,\n","and max pooling—and you know how to build a small convnet to solve a toy problem such as MNIST digits classification. Now let’s move on to more useful, practical applications."]}]}