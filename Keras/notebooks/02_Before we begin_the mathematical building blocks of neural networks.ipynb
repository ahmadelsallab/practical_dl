{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_Before we begin_the mathematical building blocks of neural networks.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"1O_NCUIHh7Ok"},"cell_type":"markdown","source":["# Chapter 2: Before we begin: the mathematical building blocks of neural networks"]},{"metadata":{"colab_type":"text","id":"KtZy7BTth7Or"},"cell_type":"markdown","source":["- Hello World!\n","    - Basic NN keras program blocks\n","- Data representation for DL:\n","    - Tensors: Rank vs. Dimension        \n","    - Tensors ranks: Scalar (0D), Vector (1D), Matrix (2D), TimeSeries (3D), Image (4D), Video (5D)\n","    - Data notations: Batches, Vector, Image, Time, Video\n","- Basic tensor operations\n","    - Digression: Tensor operations, Broadcasting, Reshape vs. Transpose\n","- Gradient-based optimization\n","    - Training loop\n","    - Stochastic Gradient Descent (SGD)\n","    - Backpropagation\n","- NN Anatomy:\n","    - Data\n","    - Model\n","    - Loss\n","    - Optimizer\n","    "]},{"metadata":{"colab_type":"text","id":"y1ESRvSQh7Ow"},"cell_type":"markdown","source":["## A first look at a neural network\n","\n","We will now take a look at a first concrete example of a neural network, which makes use of the Python library Keras to learn to classify \n","hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this \n","first example right away. You probably haven't even installed Keras yet. Don't worry, that is perfectly fine. In the next chapter, we will \n","review each element in our example and explain them in detail. So don't worry if some steps seem arbitrary or look like magic to you! \n","We've got to start somewhere.\n","\n","### Data\n","\n","The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 \n","categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been \n","around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test \n","images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST \n","as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine \n","learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."]},{"metadata":{"colab_type":"text","id":"RoZe9cfjh7Oz"},"cell_type":"markdown","source":["The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"]},{"metadata":{"colab_type":"code","id":"V5EEGdqxh7O3","colab":{}},"cell_type":"code","source":["from keras.datasets import mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I8Dl4ge4zn9N","colab_type":"code","colab":{}},"cell_type":"code","source":["train_labels[100] = 10"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"50avirtah7PD"},"cell_type":"markdown","source":["`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \n","\"test set\", `test_images` and `test_labels`. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging \n","from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n","\n","Let's have a look at the training data:"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552811693035,"user_tz":-120,"elapsed":841,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"_tyOs8p0h7PG","outputId":"41d5b028-54bf-4d8a-9be8-34f2569af4ab","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["train_images.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":65}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805887843,"user_tz":-120,"elapsed":5949,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"3g1d9QEvh7PQ","outputId":"300a8940-c135-41b1-aeae-1b5163f4aaa5","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["len(train_labels)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60000"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805887844,"user_tz":-120,"elapsed":5918,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"T6gBgjKKh7Pa","outputId":"1db8dd8d-4730-436c-a027-6d21a328cea0","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["train_labels"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805887845,"user_tz":-120,"elapsed":5890,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"LItbqeJYh7Pl","outputId":"cb982ad1-ee54-4843-ca24-e055e4f8fefa","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["test_images.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805887846,"user_tz":-120,"elapsed":5861,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"ZCTI-Z8Uh7Pu","outputId":"82494f36-2f2a-4aa3-da39-56c0a2f72703","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["len(test_labels)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805887847,"user_tz":-120,"elapsed":5839,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"ldV8buLNh7P5","outputId":"ce56f6dd-24e6-49f9-ecb4-9f77a530af38","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["test_labels"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"colab_type":"text","id":"3cqgZvq6h7QH"},"cell_type":"markdown","source":["Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in \n","the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \n","values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."]},{"metadata":{"colab_type":"code","id":"6HYeUCjsh7QM","colab":{}},"cell_type":"code","source":["train_images = train_images.reshape((60000, 28 * 28))\n","train_images = train_images.astype('float32') / 255\n","\n","test_images = test_images.reshape((10000, 28 * 28))\n","test_images = test_images.astype('float32') / 255"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pn5xxCUW4qBs","colab_type":"code","outputId":"8b40f925-7b56-4477-9bbd-62d958b0fd4f","executionInfo":{"status":"ok","timestamp":1552813310175,"user_tz":-120,"elapsed":928,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["train_images.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 784)"]},"metadata":{"tags":[]},"execution_count":78}]},{"metadata":{"colab_type":"text","id":"fqHyVnYEh7QT"},"cell_type":"markdown","source":["We also need to categorically encode the labels, a step which we explain in chapter 3:"]},{"metadata":{"colab_type":"code","id":"5Z5KYaJPh7QW","colab":{}},"cell_type":"code","source":["from keras.utils import to_categorical\n","\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nKL-ruixzKxS","colab_type":"code","outputId":"ee655000-a085-4a27-ec00-202b354ed6a9","executionInfo":{"status":"ok","timestamp":1552812086752,"user_tz":-120,"elapsed":435,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["train_labels.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 10)"]},"metadata":{"tags":[]},"execution_count":76}]},{"metadata":{"colab_type":"text","id":"t9DOvj6wh7Qd"},"cell_type":"markdown","source":["Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The \n","network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we \n","will verify if these predictions match the labels from `test_labels`.\n","\n","Let's build our network -- again, remember that you aren't supposed to understand everything about this example just yet."]},{"metadata":{"colab_type":"text","id":"1T6GoQVjh7Qf"},"cell_type":"markdown","source":["### Model"]},{"metadata":{"colab_type":"code","id":"iJswxjLFh7Qi","colab":{}},"cell_type":"code","source":["from keras import models\n","from keras import layers\n","\n","network = models.Sequential()\n","network.add(layers.Dense(512, activation='relu', name='Layer_1', input_shape=(28 * 28,)))\n","network.add(layers.Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"POER9O4Q7rra","colab_type":"code","outputId":"a5523ba5-6569-4664-9a59-1ac3655aaf92","executionInfo":{"status":"ok","timestamp":1552814133451,"user_tz":-120,"elapsed":859,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"cell_type":"code","source":["network.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","Layer_1 (Dense)              (None, 512)               401920    \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 407,050\n","Trainable params: 407,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"bdzFWF4eh7Qs"},"cell_type":"markdown","source":["\n","The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some \n","data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully \n","representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers \n","which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a \n","succession of increasingly refined data filters -- the \"layers\".\n","\n","Here our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers. \n","The second (and last) layer is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). Each \n","score will be the probability that the current digit image belongs to one of our 10 digit classes.\n","\n","### Compilation: loss + optimizer\n","\n","To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n","\n","* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be \n","able to steer itself in the right direction.\n","* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n","* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n","classified).\n","\n","The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters."]},{"metadata":{"colab_type":"code","id":"-cfRLMyxh7Qv","colab":{}},"cell_type":"code","source":["network.compile(optimizer='rmsprop',\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"5gxG_b9Gh7RD"},"cell_type":"markdown","source":["### Learning"]},{"metadata":{"colab_type":"text","id":"ApnI_EVFh7RJ"},"cell_type":"markdown","source":["We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n","we \"fit\" the model to its training data."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912136,"user_tz":-120,"elapsed":30104,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"3A0LkzQnh7RL","outputId":"46bc96de-598e-4aa3-d060-fcb7721dc536","colab":{"base_uri":"https://localhost:8080/","height":292}},"cell_type":"code","source":["network.fit(train_images, train_labels, epochs=5, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/5\n","60000/60000 [==============================] - 6s 92us/step - loss: 0.2563 - acc: 0.9257\n","Epoch 2/5\n","60000/60000 [==============================] - 5s 77us/step - loss: 0.1030 - acc: 0.9695\n","Epoch 3/5\n","60000/60000 [==============================] - 5s 76us/step - loss: 0.0698 - acc: 0.9785\n","Epoch 4/5\n","60000/60000 [==============================] - 5s 77us/step - loss: 0.0496 - acc: 0.9851\n","Epoch 5/5\n","60000/60000 [==============================] - 5s 76us/step - loss: 0.0374 - acc: 0.9891\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4e98a53668>"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"colab_type":"text","id":"m4zRGeKgh7RT"},"cell_type":"markdown","source":["Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over \n","the training data.\n","\n","We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912425,"user_tz":-120,"elapsed":30376,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"3rRATWsLh7RV","outputId":"2e64e3ef-1457-444b-ebb9-9f1faef68f65","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["test_loss, test_acc = network.evaluate(test_images, test_labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 0s 47us/step\n"],"name":"stdout"}]},{"metadata":{"id":"6EQSbwUjB166","colab_type":"code","outputId":"bb689631-aad5-493d-efdb-405caf2ed5df","executionInfo":{"status":"ok","timestamp":1552815775503,"user_tz":-120,"elapsed":1245,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","img = test_images[0]\n","plt.imshow(np.reshape(img, (28,28)))\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f4e90e529e8>"]},"metadata":{"tags":[]},"execution_count":85},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMOhSAv8Hz9SiSEO4MFYAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"K0ThmHYzCOSH","colab_type":"code","outputId":"d1321f08-2e89-478d-e519-a79d7d459e2a","executionInfo":{"status":"ok","timestamp":1552815813091,"user_tz":-120,"elapsed":882,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["img.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784,)"]},"metadata":{"tags":[]},"execution_count":87}]},{"metadata":{"id":"LNG-Y_hYC10f","colab_type":"code","outputId":"5dc2cc12-bfd6-498f-ea56-6381314fbb2b","executionInfo":{"status":"ok","timestamp":1552815973950,"user_tz":-120,"elapsed":854,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["network.predict(np.reshape(img, (1,784)))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.07699326, 0.07023557, 0.09060565, 0.1736434 , 0.07890654,\n","        0.09350489, 0.08068074, 0.09926819, 0.09897252, 0.13718927]],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":93}]},{"metadata":{"id":"_tm1zCgdCHlS","colab_type":"code","outputId":"5a652542-68de-42c3-cbea-158f8e6bfe07","executionInfo":{"status":"ok","timestamp":1552815930875,"user_tz":-120,"elapsed":725,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["np.argmax(network.predict(np.reshape(img, (1,784))))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":92}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912426,"user_tz":-120,"elapsed":30358,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"9YBsyCJ7h7Rc","outputId":"e42ec1f7-bf1a-402b-bcab-e5ffd6fd914a","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["print('test_acc:', test_acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["test_acc: 0.9794\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"iyo3d2oUh7Rk"},"cell_type":"markdown","source":["\n","Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy. \n","This gap between training accuracy and test accuracy is an example of \"overfitting\", \n","the fact that machine learning models tend to perform worse on new data than on their training data. \n","Overfitting will be a central topic in chapter 3.\n","\n","This concludes our very first example -- you just saw how we could build and a train a neural network to classify handwritten digits, in \n","less than 20 lines of Python code. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really \n","going on behind the scenes. You will learn about \"tensors\", the data-storing objects going into the network, about tensor operations, which \n","layers are made of, and about gradient descent, which allows our network to learn from its training examples."]},{"metadata":{"colab_type":"text","id":"C7NqC4Ash7Rm"},"cell_type":"markdown","source":["# 2.2 Data representations for neural networks\n","In the previous example, we started from data stored in multidimensional Numpy\n","arrays, also called tensors. In general, all current machine-learning systems use tensors\n","as their basic data structure.\n","## What is a tensor?\n","At its core, a tensor is a container for data—almost always numerical data. So, it’s a\n","container for numbers. You may be already familiar with matrices, which are 2D tensors:\n","tensors are a generalization of matrices to an arbitrary number of dimensions\n","(note that in the context of tensors, a dimension is often called an axis).\n","\n","## Tensor dimension vs. rank\n","Dimension = number of entries in an axis\n","Rank = number of dimensions/axes\n","\n"]},{"metadata":{"colab_type":"text","id":"DJ0MXWXGh7Ro"},"cell_type":"markdown","source":["## Key attributes of tensors:\n","- Rank (.ndim)\n","- Shape (.shape)\n","- Data type (.dtype)"]},{"metadata":{"colab_type":"text","id":"Ej_u9gMYh7Rr"},"cell_type":"markdown","source":["## Scalar (0D tensor)"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912767,"user_tz":-120,"elapsed":30679,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"bxQXXJlCh7Rs","outputId":"64d98152-7545-4bf6-ecc5-e1abae288954","colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["import numpy as np\n","x = np.array(12)\n","print(x.ndim)\n","print(x.shape)\n","print(x.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","()\n","int64\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"QDMpP_wMh7R0"},"cell_type":"markdown","source":["\n","## Vector (1D tensor)\n"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912769,"user_tz":-120,"elapsed":30653,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"dsLWZOxWh7R1","outputId":"b8b840de-7e3f-40c5-abc8-14f43bc8c5da","colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["x = np.array([12, 3, 6, 14])\n","print(x.ndim)\n","print(x.shape)\n","print(x.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1\n","(4,)\n","int64\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"MgLTP_C6h7R8"},"cell_type":"markdown","source":["## Matrix (2D tensor)\n"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912772,"user_tz":-120,"elapsed":30636,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"tlRkgcrjh7R-","outputId":"efc9ede0-894f-4f12-ae2c-d2f9f35c638a","colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["x = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])\n","print(x.ndim)\n","print(x.shape)\n","print(x.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2\n","(3, 5)\n","int64\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"VjOiJ8LYh7SD"},"cell_type":"markdown","source":["### 3D tensors and higher-dimensional tensors"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805912775,"user_tz":-120,"elapsed":30623,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"_NCiQSamh7SE","outputId":"8f9ed293-3f6f-4421-ecb3-151b4c153b31","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["x = np.array([[[5, 78, 2, 34, 0],\n","[6, 79, 3, 35, 1],\n","[7, 80, 4, 36, 2]],\n","[[5, 78, 2, 34, 0],\n","[6, 79, 3, 35, 1],\n","[7, 80, 4, 36, 2]],\n","[[5, 78, 2, 34, 0],\n","[6, 79, 3, 35, 1],\n","[7, 80, 4, 36, 2]]])\n","\n","print(x)\n","print(x.ndim)\n","print(x.shape)\n","print(x.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[[ 5 78  2 34  0]\n","  [ 6 79  3 35  1]\n","  [ 7 80  4 36  2]]\n","\n"," [[ 5 78  2 34  0]\n","  [ 6 79  3 35  1]\n","  [ 7 80  4 36  2]]\n","\n"," [[ 5 78  2 34  0]\n","  [ 6 79  3 35  1]\n","  [ 7 80  4 36  2]]]\n","3\n","(3, 3, 5)\n","int64\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"R9WR7Yb2h7SL"},"cell_type":"markdown","source":["### Back to MNIST"]},{"metadata":{"colab_type":"code","id":"e1j6yQWnh7SM","colab":{}},"cell_type":"code","source":["from keras.datasets import mnist\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913094,"user_tz":-120,"elapsed":30926,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"OJxa14JBh7SR","outputId":"1fac9a6a-7a6e-47d4-e7ea-5d1ac371f97c","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["print(train_images.ndim)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913096,"user_tz":-120,"elapsed":30908,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"OXgzP6HHh7SZ","outputId":"1bd56bb4-887d-4c69-b525-72dd35913f3d","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["print(train_images.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913098,"user_tz":-120,"elapsed":30886,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"ws4NdSAph7Sd","outputId":"8232ed70-fd68-43ae-fee5-6b0679d61b16","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["print(train_images.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["uint8\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"4yLg6Qtyh7Sl"},"cell_type":"markdown","source":["So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of\n","60,000 matrices of 28 × 8 integers. Each such matrix is a grayscale image, with coefficients\n","between 0 and 255."]},{"metadata":{"colab_type":"text","id":"aubP76t-h7Sm"},"cell_type":"markdown","source":["Let’s display the fourth digit in this 3D tensor, using the library Matplotlib\n"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913566,"user_tz":-120,"elapsed":31334,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"FHEpEuOzh7So","outputId":"e9c9447b-573a-400b-acf9-b22810adb69d","colab":{"base_uri":"https://localhost:8080/","height":265}},"cell_type":"code","source":["digit = train_images[4]\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.imshow(digit, cmap=plt.cm.binary)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"colab_type":"text","id":"k3cWi6u5h7Su"},"cell_type":"markdown","source":["### Tensors slicing\n","The following example selects digits #10 to #100 (#100 isn’t included) and puts\n","them in an array of shape (90, 28, 28):"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913568,"user_tz":-120,"elapsed":31320,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"bKtqWO1dh7Sv","outputId":"91e97720-fdec-40ff-fd86-48db8bb7ad33","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["my_slice = train_images[10:100]\n","print(my_slice.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(90, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"7Fa67AR6h7Sz"},"cell_type":"markdown","source":["It’s equivalent to this more detailed notation, which specifies a start index and stop\n","index for the slice along each tensor axis. Note that : is equivalent to selecting the\n","entire axis:"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805913569,"user_tz":-120,"elapsed":31302,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"Hy2YALtPh7S0","outputId":"bd0b0c42-30d1-4bb6-c568-6648fc035a19","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["my_slice = train_images[10:100, :, :]\n","print(my_slice.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(90, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"a31sn4jWh7S4"},"cell_type":"markdown","source":["In general, you may select between any two indices along each tensor axis. For\n","instance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you\n","do this:"]},{"metadata":{"colab_type":"code","id":"ceNg0Chph7S6","colab":{}},"cell_type":"code","source":["my_slice = train_images[:, 14:, 14:]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"LdyQCdbIh7S9"},"cell_type":"markdown","source":["It’s also possible to use negative indices. Much like negative indices in Python lists,\n","they indicate a position relative to the end of the current axis. In order to crop the\n","images to patches of 14 × 14 pixels centered in the middle, you do this:"]},{"metadata":{"colab_type":"code","id":"pYaX9o_Zh7S-","colab":{}},"cell_type":"code","source":["my_slice = train_images[:, 7:-7, 7:-7]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"AJPLayvRh7TB"},"cell_type":"markdown","source":["## Batches and tensor notations in Keras/TensorFlow"]},{"metadata":{"colab_type":"text","id":"mscNqL8dh7TD"},"cell_type":"markdown","source":["### Dimension 0 = samples axis\n","In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll\n","come across in deep learning will be the samples axis (sometimes called the samples\n","dimension). In the MNIST example, samples are images of digits."]},{"metadata":{"colab_type":"text","id":"9WS5M97jh7TD"},"cell_type":"markdown","source":["### Batches \n","In addition, deep-learning models don’t process an entire dataset at once; rather,\n","they break the data into small batches. \n","\n","When considering such a batch tensor, the first axis (axis 0) is called the batch axis or\n","batch dimension. This is a term you’ll frequently encounter when using Keras and other\n","deep-learning libraries.\n","\n","Concretely, here’s one batch of our MNIST digits,\n","with batch size of 128:"]},{"metadata":{"colab_type":"code","id":"adq7rvrGh7TE","colab":{}},"cell_type":"code","source":["batch = train_images[:128]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"FOXDSOdMh7TI"},"cell_type":"markdown","source":["And here’s the next batch:"]},{"metadata":{"colab_type":"code","id":"H2h05436h7TJ","colab":{}},"cell_type":"code","source":["batch = train_images[128:256]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"re41Zjkmh7TN"},"cell_type":"markdown","source":["And the nth batch:"]},{"metadata":{"colab_type":"code","id":"AehA0Ereh7TP","colab":{}},"cell_type":"code","source":["n = 10\n","batch = train_images[128 * n:128 * (n + 1)]\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"21szrcN4h7TU"},"cell_type":"markdown","source":["## Real-world examples of data tensors"]},{"metadata":{"colab_type":"text","id":"P_vA0_Nih7TW"},"cell_type":"markdown","source":["### Vector Data\n","\n","2D tensors of shape (samples, features)\n","samples could be the batch (size=batch_size)\n","\n","\n","Previously we called this Matrix (2D), which is still true.\n","But remember, we talk here about each entry of data, where each row is a vector.\n","We have array of Vector data, so it's a Matrix, each row is Vector data.\n","\n","This is the most common case. In such a dataset, each single data point can be encoded\n","as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of\n","vectors), where the first axis is the samples axis and the second axis is the features axis.\n","Let’s take a look at two examples:\n","\n","-  A dataset of people, where we consider each person’s age, ZIP code,\n","and income. Each person can be characterized as a vector of 3 values, and thus\n","an entire dataset of 100,000 people can be stored in a 2D tensor of shape\n","(100000, 3).\n","-  A dataset of text documents, where we represent each document by the counts\n","of how many times each word appears in it (out of a dictionary of 20,000 common\n","words). Each document can be encoded as a vector of 20,000 values (one\n","count per word in the dictionary), and thus an entire dataset of 500 documents\n","can be stored in a tensor of shape (500, 20000)."]},{"metadata":{"colab_type":"text","id":"Owby39LPh7TW"},"cell_type":"markdown","source":["### Timeseries data or sequence data\n","3D tensors of shape (samples, timesteps,\n","features)\n","samples could be the batch (size=batch_size)\n","\n","Whenever time matters in your data (or the notion of sequence order), it makes sense\n","to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\n","sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\n","tensor"]},{"metadata":{"colab_type":"text","id":"mnTsG7Jfh7TY"},"cell_type":"markdown","source":["![02_3D tensors.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_3D%20tensors.png?raw=true)"]},{"metadata":{"colab_type":"text","id":"60j51U0Lh7TZ"},"cell_type":"markdown","source":["# The time axis is always the second axis (axis of index 1), by convention.\n","\n","Let’s look at a\n","few examples:\n","-  A dataset of stock prices. Every minute, we store the current price of the stock,\n","the highest price in the past minute, and the lowest price in the past minute.\n","Thus every minute is encoded as a 3D vector, an entire day of trading is\n","encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n","day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n","390, 3). Here, each sample would be one day’s worth of data.\n","- A dataset of tweets, where we encode each tweet as a sequence of 280 characters\n","out of an alphabet of 128 unique characters. In this setting, each character can\n","be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\n","at the index corresponding to the character). Then each tweet can be encoded\n","as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be\n","stored in a tensor of shape (1000000, 280, 128)."]},{"metadata":{"colab_type":"text","id":"MkOqA2g1h7Tb"},"cell_type":"markdown","source":["### Image data\n","4D tensors of shape (samples, height, width, channels) or (samples,\n","channels, height, width)\n","samples could be the batch (size=batch_size)\n","\n","Images typically have three dimensions: height, width, and color depth. Although\n","grayscale images (like our MNIST digits) have only a single color channel and could\n","thus be stored in 2D tensors, by convention image tensors are always 3D, with a onedimensional\n","color channel for grayscale images. A batch of 128 grayscale images of\n","size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a\n","batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)"]},{"metadata":{"colab_type":"text","id":"--m1f4cRh7Te"},"cell_type":"markdown","source":["![02_4D tensors.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_4D%20tensors.png?raw=true)"]},{"metadata":{"colab_type":"text","id":"u_SizOWnh7Th"},"cell_type":"markdown","source":["There are two conventions for shapes of images tensors: the channels-last convention\n","(used by TensorFlow) and the channels-first convention (used by Theano). The Tensor-\n","Flow machine-learning framework, from Google, places the color-depth axis at the\n","end: (samples, height, width, color_depth). Meanwhile, Theano places the color\n","depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256)\n","and (128, 3, 256, 256). The Keras framework provides support for both formats."]},{"metadata":{"colab_type":"text","id":"77ivf6cZh7Ti"},"cell_type":"markdown","source":["### Video data\n","5D tensors of shape (samples, frames, height, width, channels) or\n","(samples, frames, channels, height, width)\n","samples could be the batch (size=batch_size)\n","\n","Video data is one of the few types of real-world data for which you’ll need 5D tensors.\n","A video can be understood as a sequence of frames, each frame being a color image.\n","Because each frame can be stored in a 3D tensor (height, width, color_depth), a\n","sequence of frames can be stored in a 4D tensor (frames, height, width, color_\n","depth), and thus a batch of different videos can be stored in a 5D tensor of shape\n","(samples, frames, height, width, color_depth)."]},{"metadata":{"colab_type":"text","id":"FeTiF6-3h7Tk"},"cell_type":"markdown","source":["For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n","second would have 240 frames. A batch of four such video clips would be stored in a\n","tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the\n","dtype of the tensor was float32, then each value would be stored in 32 bits, so the\n","tensor would represent 405 MB. Heavy! Videos you encounter in real life are much\n","lighter, because they aren’t stored in float32, and they’re typically compressed by a\n","large factor (such as in the MPEG format)."]},{"metadata":{"colab_type":"text","id":"1D4ZF_yCh7Tl"},"cell_type":"markdown","source":["## Tensors operations\n","Neural networks perform transformations to input data.\n","all transformations learned\n","by deep neural networks can be reduced to a handful of tensor operations applied to\n","tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors,\n","and so on.\n","\n","### A transformation is a Layer:\n","In our initial example, we were building our network by stacking Dense layers on\n","top of each other. A Keras layer instance looks like this:"]},{"metadata":{"colab_type":"code","id":"0W0SujJCh7Tm","colab":{}},"cell_type":"code","source":["from keras.layers import Dense\n","fully_connected_layer = Dense(512, activation='relu')\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"BDHnfjnrh7Ts"},"cell_type":"markdown","source":["This layer can be interpreted as a function, which takes as input a 2D tensor and\n","returns another 2D tensor—a new representation for the input tensor. Specifically, the\n","function is as follows (where W is a 2D tensor and b is a vector, both attributes of the\n","layer):"]},{"metadata":{"colab_type":"text","id":"pmST1nED4ObS"},"cell_type":"markdown","source":["```\n","output = relu(dot(W, input) + b)\n","```"]},{"metadata":{"colab_type":"text","id":"NyM-rh7_h7Ty"},"cell_type":"markdown","source":["Let’s unpack this. We have three tensor operations here: a dot product (dot) between\n","the input tensor and a tensor named W; an addition (+) between the resulting 2D tensor\n","and a vector b; and, finally, a relu operation. relu(x) is max(x, 0)."]},{"metadata":{"colab_type":"text","id":"kZq4sPA-h7T0"},"cell_type":"markdown","source":["### Element-wise operations\n","The relu operation and addition are element-wise operations: operations that are\n","applied independently to each entry in the tensors being considered."]},{"metadata":{"colab_type":"code","id":"FoCqt3tXh7T2","colab":{}},"cell_type":"code","source":["def naive_relu(x):\n","    assert len(x.shape) == 2\n","    x = x.copy()\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] = max(x[i, j], 0)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"aE7_5GOZh7T6","colab":{}},"cell_type":"code","source":["def naive_add(x, y):\n","    assert len(x.shape) == 2\n","    assert x.shape == y.shape\n","    x = x.copy()\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] += y[i, j]\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vvFOcJu3h7T-"},"cell_type":"markdown","source":["### But in reality it's actually parallel:\n","This means\n","these operations are highly amenable to massively parallel implementations (vectorized\n","implementations, a term that comes from the vector processor supercomputer architecture\n","from the 1970–1990 period)."]},{"metadata":{"colab_type":"text","id":"m_vGHYa0h7T_"},"cell_type":"markdown","source":["## Try it yourself: measure the time for very long tensors (naive vs. GPU)\n"]},{"metadata":{"colab_type":"text","id":"T16xqzX2h7UC"},"cell_type":"markdown","source":["### BLAS\n","In practice, when dealing with Numpy arrays, these operations are available as welloptimized\n","built-in Numpy functions, which themselves delegate the heavy lifting to a\n","Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed\n","(which you should). BLAS are low-level, highly parallel, efficient tensor-manipulation\n","routines that are typically implemented in Fortran or C."]},{"metadata":{"colab_type":"text","id":"cmmNCeJ8h7UD"},"cell_type":"markdown","source":["# Tensor Dot\n","Not same as element-wise\n","Product is a scalar, for 2 vectors\n"]},{"metadata":{"colab_type":"code","id":"rg6_0uBFh7UE","colab":{}},"cell_type":"code","source":["def naive_vector_dot(x, y):\n","    assert len(x.shape) == 1\n","    assert len(y.shape) == 1\n","    assert x.shape[0] == y.shape[0]\n","    z = 0.\n","    for i in range(x.shape[0]):\n","        z += x[i] * y[i]\n","    return z"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"oB82rmDHh7UO"},"cell_type":"markdown","source":["For matrix and vector, dot produces a vector, with same number of elements = number of rows of the matrix."]},{"metadata":{"colab_type":"text","id":"oCpZruJuh7UP"},"cell_type":"markdown","source":["For matrix and matrix, is a another matrix, with same number of rows as the first, and number of cols as the 2nd.\n","#cols (1st) = #rows (2nd)\n","\n"]},{"metadata":{"colab_type":"text","id":"bqGY-HcQh7UQ"},"cell_type":"markdown","source":["![02_tensors dot.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_tensors%20dot.png?raw=true)"]},{"metadata":{"colab_type":"text","id":"pTxukTACh7UR"},"cell_type":"markdown","source":["# Tensor reshaping"]},{"metadata":{"colab_type":"text","id":"gXWb8_7Rh7US"},"cell_type":"markdown","source":["Reshaping a tensor means rearranging its rows and columns to match a target shape.\n","Naturally, the reshaped tensor has the same total number of coefficients as the initial\n","tensor."]},{"metadata":{"colab_type":"text","id":"uDZ_azFmh7UT"},"cell_type":"markdown","source":["# Why?\n","Image you have as the example an input like:"]},{"metadata":{"colab_type":"code","id":"mdgVNZzph7UV","outputId":"c0d7558b-5e60-4752-8d18-ef30116a392f","executionInfo":{"status":"ok","timestamp":1552805913630,"user_tz":-120,"elapsed":31321,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["train_images.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":44}]},{"metadata":{"colab_type":"text","id":"9m3mXWzMh7UZ"},"cell_type":"markdown","source":["Since the input layer is not conv, it cannot accept 2D input (Remember LEGO where inputs should match outputs shapes for layers compatibility)"]},{"metadata":{"colab_type":"code","id":"XBPFMgSZh7Ua","colab":{}},"cell_type":"code","source":["train_images = train_images.reshape((60000, 28 * 28))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ea36oJCWh7Uf"},"cell_type":"markdown","source":["# When to reshape and when to transpose?\n","Reshape is usually used to re-order the elements from a shape to another. However, elements per axis is not necessarily preserved.\n","\n","Imagine that you have a vector data. As mentioned above, this type of data should be 2D: (samples, features).\n","Suppose that your input is mis-ordered as (featrues, samples).\n","Then you want to re-oreder the axes to match Keras convention.\n"]},{"metadata":{"colab_type":"text","id":"jJccKAvch7Uf"},"cell_type":"markdown","source":["Using reshape:\n","let n_samples = 3, n_features = 2"]},{"metadata":{"colab_type":"code","id":"C8X8ZbOsh7Uh","colab":{}},"cell_type":"code","source":["import numpy as np\n","sample_1 = [0., 1.]\n","sample_2 = [2., 3.]\n","sample_3 = [4., 5.]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"mEjFHwPFh7Uk"},"cell_type":"markdown","source":["Say the samples are ordered column wise"]},{"metadata":{"colab_type":"code","id":"FiKE4-Okh7Ul","outputId":"1441a71d-693a-45ea-b8c1-486ee01f6418","executionInfo":{"status":"ok","timestamp":1552805913653,"user_tz":-120,"elapsed":31306,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["x = np.array([[0., 2., 4.], [1., 3., 5.]])\n","x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3)"]},"metadata":{"tags":[]},"execution_count":47}]},{"metadata":{"colab_type":"text","id":"cralNgiqh7Uv"},"cell_type":"markdown","source":["Let's try to fix with reshape"]},{"metadata":{"colab_type":"code","id":"664nCTvch7Uw","colab":{}},"cell_type":"code","source":["x = x.reshape(3,2)\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"3zf_Lwjfh7Uz","outputId":"1e53c47c-91f6-4ab7-d85a-4a8685605a26","executionInfo":{"status":"ok","timestamp":1552805913669,"user_tz":-120,"elapsed":31281,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 2)"]},"metadata":{"tags":[]},"execution_count":49}]},{"metadata":{"colab_type":"text","id":"S24eyuwrh7U3"},"cell_type":"markdown","source":["#### Is the original samples restored?\n","No, why?"]},{"metadata":{"colab_type":"code","id":"Q2QI6NAKh7U4","outputId":"4b5b14c4-1bff-43ef-a73e-bb11c9593831","executionInfo":{"status":"ok","timestamp":1552805913671,"user_tz":-120,"elapsed":31266,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["x"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 2.],\n","       [4., 1.],\n","       [3., 5.]])"]},"metadata":{"tags":[]},"execution_count":50}]},{"metadata":{"colab_type":"text","id":"nduwF-gWh7U9"},"cell_type":"markdown","source":["#### Because reshape works row-wise, but doesnot preserve the axes."]},{"metadata":{"colab_type":"text","id":"MtjrRyB7h7U-"},"cell_type":"markdown","source":["### Using transpose\n","np.transpose will re-order the axes, keeping the element in each axis the same.\n","So it's a special case of re-shape.\n","We usually use it for axes re-ordering.\n","\n","it takes the new axes positions as input"]},{"metadata":{"colab_type":"code","id":"TT817uYMh7U_","outputId":"8560eb50-6b52-4610-e628-ca385209cb99","executionInfo":{"status":"ok","timestamp":1552805914398,"user_tz":-120,"elapsed":31977,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["x = np.array([[0., 2., 4.], [1., 3., 5.]])\n","x = np.transpose(x)\n","x"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1.],\n","       [2., 3.],\n","       [4., 5.]])"]},"metadata":{"tags":[]},"execution_count":51}]},{"metadata":{"colab_type":"text","id":"RWkb1z1Bh7VG"},"cell_type":"markdown","source":["As you can see, the original samples are resotred"]},{"metadata":{"colab_type":"text","id":"w0bUNBvch7VH"},"cell_type":"markdown","source":["#### Time series example\n","Imagine that you have a time series data. As mentioned above, this type of data should be 3D: (samples, time, features).\n","Suppose that your input is mis-ordered as (time, samples, features).\n","Then you want to re-oreder the axes to match Keras convention.\n","Here you use transpose as follows:"]},{"metadata":{"colab_type":"code","id":"eHVxaxa8h7VJ","outputId":"31a27cf9-dc85-4e55-e47c-a0807912d3cb","executionInfo":{"status":"ok","timestamp":1552805914400,"user_tz":-120,"elapsed":31949,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["n_samples = 10\n","n_timesteps = 5\n","n_features = 3\n","x = np.zeros([n_samples, n_features, n_timesteps])\n","print(x.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10, 3, 5)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"TEavvwYdh7VN","outputId":"526290db-b098-4dd1-9689-262362fca1f9","executionInfo":{"status":"ok","timestamp":1552805914401,"user_tz":-120,"elapsed":31929,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["x = np.transpose(x, [0,2,1]) # also works x = x.transpose([0,2,1])\n","x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 5, 3)"]},"metadata":{"tags":[]},"execution_count":53}]},{"metadata":{"colab_type":"text","id":"yUSTi64eh7VU"},"cell_type":"markdown","source":["### Notes:\n","1. In 2D, np.transpose holds the same mathmatical meaning as matrix transpose, so no need to feed the axes order as they will normally be flipped\n","2. Using reshape with the above 3D example will give the same shape, and most dangerously, the whole program will not give an error \n","## But it will give wrong output and it's very hard to debug"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805914404,"user_tz":-120,"elapsed":31920,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"lOXMT9Nuh7VW","outputId":"e56ef5c6-c3bc-4f8e-b48a-f1f74e72004f","colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["x = np.zeros([n_samples, n_features, n_timesteps])\n","print(x.shape)\n","x = x.reshape([n_samples, n_timesteps, n_features])\n","x.shape"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10, 3, 5)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(10, 5, 3)"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"colab_type":"text","id":"lpoMNPCjh7Va"},"cell_type":"markdown","source":["# Gradient based optimization\n","What we want to have at the end from a NN is a `Model`, represents an input-output relation\n","![02_gd.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_gd.png?raw=true)"]},{"metadata":{"colab_type":"text","id":"3FjamPhI5LsK"},"cell_type":"markdown","source":["output = relu(dot(W, input) + b)"]},{"metadata":{"colab_type":"text","id":"sa4zolWlh7Vh"},"cell_type":"markdown","source":["The model has `parameters` in the above case those are `W` and `b`.\n","They’re called the weights or trainable parameters of the layer (the kernel and bias attributes, respectively).\n","These weights contain the information learned by the network from exposure to training data.\n","\n","Initially, these weight matrices are filled with small random values (a step called random\n","initialization). Of course, there’s no reason to expect that relu(dot(W, input) + b),\n","when W and b are random, will yield any useful representations. The resulting representations\n","are meaningless—but they’re a starting point. \n","### Learning:\n","What comes next is to gradually\n","adjust these weights, based on a feedback signal. This gradual adjustment, also called\n","training, is basically the learning that machine learning is all about."]},{"metadata":{"colab_type":"text","id":"vL2rU4Nth7Vi"},"cell_type":"markdown","source":["## Training loop\n","\n","This happens within what’s called a training loop, which works as follows. Repeat\n","these steps in a loop, as long as necessary:\n","\n","1. Draw a batch of training samples x and corresponding targets y.\n","2. Run the network on x (a step called the forward pass) to obtain predictions y_pred.\n","3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n","4. Update all weights of the network in a way that slightly reduces the loss on this batch.\n","\n","You’ll eventually end up with a network that has a very low loss on its training data: a\n","low mismatch between predictions y_pred and expected targets y. The network has\n","“learned” to map its inputs to correct targets. From afar, it may look like magic, but\n","when you reduce it to elementary steps, it turns out to be simple.\n","\n","Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the application\n","of a handful of tensor operations, so you could implement these steps purely\n","from what you learned in the previous section. The difficult part is step 4: updating\n","the network’s weights. \n","### Given an individual weight coefficient in the network, how can you compute whether the coefficient should be increased or decreased, and by how much?\n","One naive solution would be to freeze all weights in the network except the one\n","scalar coefficient being considered, and try different values for this coefficient. Let’s\n","say the initial value of the coefficient is 0.3. After the forward pass on a batch of data,\n","the loss of the network on the batch is 0.5. If you change the coefficient’s value to 0.35\n","and rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to\n","0.25, the loss falls to 0.4. In this case, it seems that updating the coefficient by -0.05 would contribute to minimizing the loss. This would have to be repeated for all coefficients\n","in the network. But such an approach would be horribly inefficient, because you’d need to compute\n","two forward passes (which are expensive) for every individual coefficient (of\n","which there are many, usually thousands and sometimes up to millions).\n","\n","\n","## Follow the opposite gradient down: Gradient Descent\n"," A much better\n","approach is to take advantage of the fact that all operations used in the network\n","are differentiable, and compute the gradient of the loss with regard to the network’s\n","coefficients. You can then move the coefficients in the opposite direction from the\n","gradient, thus decreasing the loss.\n","![image.png](attachment:image.png)\n","\n","```\n","y_pred = dot(W, x)\n","loss_value = loss(y_pred, y) # loss_value = f(W)\n","# Let’s say the current value of W is W0.\n","# Update weights opposite to the gradient of the loss at the W=W0\n","W1 = W0 - step * gradient(f)(W0)# gradient(f) w.r.t. W is easily calculated in DL frameworks thanks to sybmolic auto diff.\n","# Repeat for W2, W3,....W100\n","```"]},{"metadata":{"colab_type":"text","id":"Y6U3yPLih7Vp"},"cell_type":"markdown","source":["## Stochastic Gradient Descent\n","### GD (batch SGD)\n","The above GD algorithm updates the gradient with every new sample of x.\n","If X is of 1000 samples, then the delta_W[t+1] = step * gradient(f)(W[t]) is calculated and accumulated every sample, but the application of weight update is done once after all 1000 samples are fed.\n","\n","- Update once at the end\n","- High accumulation of error, Risk of saturation\n","- Slow feedback, slow convergence, but more stable\n","- Take advantage of parallelism in matrix operations (comp. arch.)\n","\n","### SGD\n","There are other options of updating:\n","1. SGD: Each sample (batch_size=1)\n","\n","    - Fast feedback, fast convergence (corrects itself a lot) \n","    - Could oscillate, unstable (tend to corrupt what it learnt)\n","    - Not taking advantage of parallelism in matrix operations (comp. arch.)\n","    - Not saturating\n","    - Stochastic: every update is a representing sample of the true gradient \n","2. Minibatch SGD: Every group of samples (batch_size=N)\n","Accumulate M gradients, update every M\n","    - More stable than 1 sample\n","    - Faster than GD\n","### Steps\n","    1. Draw a batch of training samples x and corresponding targets y.\n","    2. Run the network on x to obtain predictions y_pred.\n","    3. Compute the loss of the network on the batch, a measure of the mismatch\n","    between y_pred and y.\n","    4. Compute the gradient of the loss with regard to the network’s parameters (a\n","    backward pass).\n","    5. Move the parameters a little in the opposite direction from the gradient—for\n","    example W -= step * gradient—thus reducing the loss on the batch a bit."]},{"metadata":{"colab_type":"text","id":"KAnaf5hHh7Vq"},"cell_type":"markdown","source":["### Learning rate\n","As you can see, intuitively it’s important to pick a reasonable value for the step factor.\n","If it’s too small, the descent down the curve will take many iterations, and it could get\n","stuck in a local minimum. If step is too large, your updates may end up taking you to\n","completely random locations on the curve."]},{"metadata":{"colab_type":"text","id":"JiN55Y-kh7Vr"},"cell_type":"markdown","source":["### Momentum\n","Additionally, there exist multiple variants of SGD that differ by taking into account\n","previous weight updates when computing the next weight update, rather than just\n","looking at the current value of the gradients. There is, for instance, SGD with momentum,\n","as well as Adagrad, RMSProp, and several others. Such variants are known as optimization\n","methods or optimizers. In particular, the concept of momentum, which is used in\n","many of these variants, deserves your attention. Momentum addresses two issues with\n","SGD: convergence speed and local minima.\n","    \n","Momentum draws inspiration from physics.\n","A useful mental image here is to think of the optimization process as a small ball\n","rolling down the loss curve. If it has enough momentum, the ball won’t get stuck in a\n","ravine and will end up at the global minimum. Momentum is implemented by moving\n","the ball at each step based not only on the current slope value (current acceleration)\n","but also on the current velocity (resulting from past acceleration). In practice, this\n","means updating the parameter w based not only on the current gradient value but also\n","on the previous parameter update, such as in this naive implementation:"]},{"metadata":{"colab_type":"text","id":"GMgqz0Ks56uP"},"cell_type":"markdown","source":["```\n","past_velocity = 0.\n","momentum = 0.1\n","while loss > 0.01:\n","    w, loss, gradient = get_current_parameters()\n","    velocity = past_velocity * momentum + learning_rate * gradient\n","    w = w + momentum * velocity - learning_rate * gradient\n","    past_velocity = velocity\n","    update_parameter(w)\n","```    "]},{"metadata":{"colab_type":"text","id":"_7M7gH2Xh7Vu"},"cell_type":"markdown","source":["### Momentum to avoid local min\n","\n","\n","![02_momentum.png](https://github.com/ahmadelsallab/practical_dl/blob/master/Keras/notebooks/imgs/02_momentum.png?raw=true)\n","\n","You can avoid such issues by using momentum, "]},{"metadata":{"colab_type":"text","id":"wfBVIBWLh7Vw"},"cell_type":"markdown","source":["## Backprop\n","In the previous algorithm, we casually assumed that because a function is differentiable,\n","we can explicitly compute its derivative. In practice, a neural network function\n","consists of many tensor operations chained together, each of which has a simple,\n","known derivative. For instance, this is a network f composed of three tensor operations,\n","a, b, and c, with weight matrices W1, W2, and W3:\n","\n","`f(W1, W2, W3) = a(W1, b(W2, c(W3)))`\n","Calculus tells us that such a chain of functions can be derived using the following identity,\n","called the chain rule: `f(g(x)) = f'(g(x)) * g'(x)`. Applying the chain rule to the\n","computation of the gradient values of a neural network gives rise to an algorithm called Backpropagation (also sometimes called reverse-mode differentiation). \n","\n","Backpropagation starts with the final loss value and works backward from the top layers to the bottom\n","layers, applying the chain rule to compute the contribution that each parameter\n","had in the loss value.\n","![image.png](attachment:image.png)\n","\n","### Symbolic differentiation\n","Nowadays, and for years to come, people will implement networks in modern\n","frameworks that are capable of symbolic differentiation, such as TensorFlow. This means\n","that, given a chain of operations with a known derivative, they can compute a gradient\n","function for the chain (by applying the chain rule) that maps network parameter values\n","to gradient values. When you have access to such a function, the backward pass is\n","reduced to a call to this gradient function. Thanks to symbolic differentiation, you’ll\n","never have to implement the Backpropagation algorithm by hand. For this reason, we\n","won’t waste your time and your focus on deriving the exact formulation of the Backpropagation\n","algorithm in these pages. All you need is a good understanding of how\n","gradient-based optimization works."]},{"metadata":{"colab_type":"text","id":"osZxwWyQh7Vx"},"cell_type":"markdown","source":["# NN Anatomy (Revision of 1st example)\n","## 1. Data"]},{"metadata":{"colab_type":"code","id":"WB4LLHbLh7V1","colab":{}},"cell_type":"code","source":["(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","train_images = train_images.reshape((60000, 28 * 28))\n","train_images = train_images.astype('float32') / 255\n","test_images = test_images.reshape((10000, 28 * 28))\n","test_images = test_images.astype('float32') / 255"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"CkiFyZXKh7V3"},"cell_type":"markdown","source":["## 2. Model"]},{"metadata":{"colab_type":"code","id":"kmKzoBaph7V4","colab":{}},"cell_type":"code","source":["network = models.Sequential()\n","network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n","network.add(layers.Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1552805914754,"user_tz":-120,"elapsed":32257,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"ctEtM5Ej6ERz","outputId":"4f40a64f-7b51-4482-f699-19ca174d7053","colab":{"base_uri":"https://localhost:8080/","height":201}},"cell_type":"code","source":["network.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 512)               401920    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 407,050\n","Trainable params: 407,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"Vz4KU5IEh7V-"},"cell_type":"markdown","source":["## 3. Loss\n","## 4. Optimizer\n","Both under one step in Keras called compile"]},{"metadata":{"colab_type":"code","id":"rv3PBD4Qh7V_","colab":{}},"cell_type":"code","source":["network.compile(optimizer='rmsprop',\n","loss='categorical_crossentropy',\n","metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1552805914780,"user_tz":-120,"elapsed":32274,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"id":"v72rYjuhh7WF","outputId":"7cba3df9-c129-4056-9b65-632a7d9135bd","colab":{"base_uri":"https://localhost:8080/","height":533}},"cell_type":"code","source":["network.fit(train_images, train_labels, epochs=5, batch_size=128)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-8b04f355273b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have shape (10,) but got array with shape (1,)"]}]},{"metadata":{"colab_type":"text","id":"r1uOaaHB6Lgy"},"cell_type":"markdown","source":["# Why the above error?\n","Let's have a look on the labels"]},{"metadata":{"colab_type":"code","id":"WZTLObRH6KWd","colab":{}},"cell_type":"code","source":["train_labels.shape"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"c9TcyrWR6V2z","colab":{}},"cell_type":"code","source":["train_labels[0]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"kacCLA9s6YKG"},"cell_type":"markdown","source":["Our final activation is 'softmax', and our loss is 'categorial'.\n","\n","This means that, our output must be consistent with softmax output.\n","\n","That format is called __One-Hot Encoding__"]},{"metadata":{"colab_type":"code","id":"LdYjZvjz6oON","colab":{}},"cell_type":"code","source":["from keras.utils import to_categorical\n","train_labels = to_categorical(train_labels)\n","print(train_labels.shape)\n","print(train_labels[0])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"9wU1y3a866b0","colab":{}},"cell_type":"code","source":["network.fit(train_images, train_labels, epochs=5, batch_size=128)"],"execution_count":0,"outputs":[]}]}